Implement TypeScript library "PatchTSTRegression": a PatchTST (Patch-based Time Series Transformer) neural network for multivariate regression with incremental online learning, Adam optimizer, and Welford z-score normalization.

SCOPE AND GOALS
Build a single self-contained TypeScript library (no heavy runtime deps) that trains online (one sample at a time) and predicts multiple steps ahead.
Primary use-case is tight CPU and memory environments, so the implementation must be deterministic, allocation-free in hot paths, and numerically stable.

PERFORMANCE PRINCIPLES (HARD REQUIREMENTS)
Fixed-size memory: preallocate all parameter, gradient, optimizer-moment, and scratch buffers once at initialization; never grow during training.
Bounded attention memory: do not allocate O(seqLen^2) attention score/weight matrices; implement streaming or tiled attention using fixed reusable blocks. (For PatchTST, attention length is numberOfPatches, but the same constraint applies.)
Typed-array only core: all tensors live in contiguous Float64Array slabs in row-major layout; expose zero-copy views via (data, offset, shape, strides).
No hot-path allocations: forward(), backward(), fitOnline() must allocate 0 arrays/objects per call; all temporaries come from TensorArena and BufferPool.
Strict reuse policy: rent scratch buffers by size class, return immediately; no new Float64Array inside inner loops.
In-place and fused kernels: prefer fused ops (matmul+bias+activation, softmax+mask, residual+dropout) and in-place transforms to avoid intermediates.
Deterministic caps: enforce maxSequenceLength and maxBatch=1; if inputs exceed limits, truncate/stride rather than allocating more memory.
Minimal training tape: store only what backprop strictly needs (offsets and compact stats); recompute cheap intermediates instead of caching large activations.
Tight loops only: classic for-loops with manual indexing; avoid map/reduce/forEach, closures, and polymorphic dispatch in inner loops.
Cache once, reuse forever: positional encodings, masks, index maps, and shape metadata computed once and reused; lazy init allowed, but never re-init.
GC guardrails: pool small objects (TensorView, ForwardContext shells, ADWIN nodes); keep debug and verbose metrics off by default.

DESIGN PRINCIPLES (HARD REQUIREMENTS)
Full numerical stability throughout (stable softmax, epsilon guards, finite checks, variance floors).
Object-oriented design with interfaces for public contracts; private field encapsulation for internal state.
JSDoc on public methods and key classes (@param, @returns, @example).
Inline math formula notes where it clarifies behavior (normalization, loss, Adam, softmax, layer norm).
Full backpropagation across all layers (no partial training shortcuts).

PUBLIC API (MUST MATCH)
fitOnline({ xCoordinates: number[][], yCoordinates: number[][] }): FitResult
Behavior: incremental Adam update, Welford z-score, L2 regularization, outlier downweighting, ADWIN drift detection.
predict(futureSteps: number): PredictionResult
Behavior: forward-only inference with uncertainty bounds from ResidualStatsTracker.
getModelSummary(): ModelSummary
getWeights(): WeightInfo
getNormalizationStats(): NormalizationStats
reset(): void
save(): string
Behavior: JSON.stringify of all state data.
load(w: string): void
Behavior: restore all state data from JSON.

CONFIG DEFAULTS (MUST MATCH)
numBlocks: 3
embeddingDim: 64
numHeads: 8
ffnMultiplier: 4
attentionDropout: 0.0
learningRate: 0.001
warmupSteps: 100
totalSteps: 10000
beta1: 0.9
beta2: 0.999
epsilon: 1e-8
regularizationStrength: 1e-4
convergenceThreshold: 1e-6
outlierThreshold: 3.0
adwinDelta: 0.002
temporalScales: [1, 2, 4]
temporalKernelSize: 3
maxSequenceLength: 512
fusionDropout: 0.0

ARCHITECTURE OVERVIEW
PatchTSTRegression is the public facade and owns all state.
PatchTST core idea: convert each input channel into a sequence of patch tokens (each token represents a contiguous window of the series), add positional and (optional) scale embeddings, then run a channel-independent Transformer encoder (shared weights across channels) over patch tokens. Produce an aggregated representation per channel, then map to outputs.
Multi-scale patching uses temporalScales as patchScale multipliers applied to a base patch length derived from temporalKernelSize (and/or a deterministic rule described in code). Different scales produce different patch streams that are aligned/fused using a gated fusion module (fusionDropout applies here).
Training step pipeline: validate shapes and caps, update normalizers, normalize input/target, forward pass, compute robust weighted loss, backward pass, clip gradients, schedule LR, Adam step, update residual stats, ADWIN drift update, optional drift strategy actions.
Inference pipeline: forward pass on last normalized sequence, denormalize outputs, compute standard error and bounds, optional autoregressive rolling of the stored normalized sequence.

CLASSES (MUST IMPLEMENT THESE EXACT NAMES AND ROLES)

PatchTSTRegression (PUBLIC FACADE)
Owns all state and exposes fitOnline(), predict(), getModelSummary(), getWeights(), getNormalizationStats(), reset(), save(), load().
Delegates to normalization, model graph, loss/backprop, optimizer, drift, metrics, serialization.
Enforces shape contracts, deterministic caps, and typed-array-only hot paths.

ModelCore (NETWORK ORCHESTRATOR)
forward(seqXNorm: Float64Array, seqLen: number, inputDim: number): ForwardContext
backward(ctx: ForwardContext, dLoss_dyPredNorm: Float64Array): void
Composed of PatchMaker, PatchEmbeddingBank, PositionalEncodingCache, PatchScaleFusion, ChannelIndependentTransformer, AttentionPooling, OutputHead.
Never allocates in hot paths; all temporary tensors come from TensorArena and BufferPool.

ParameterStore (WEIGHTS, GRADS, MOMENTS REGISTRY)
Central registry of all trainable tensors and their optimizer moments.
Holds W (params), G (grads), M and V (Adam moments) as contiguous Float64Array buffers.
Provides deterministic parameter iteration order for optimizer and serialization.
Exposes zero-copy TensorView slices (offset + shape metadata).

TensorView (ZERO-COPY TENSOR ACCESSOR)
Lightweight object describing a slice: data, offset, shape, strides.
Used to pass around tensor regions without copying.
Instances are pooled via ObjectPool<TensorView>.

TensorArena / BufferPool / ObjectPool<T> (MEMORY MANAGEMENT)
TensorArena: preallocates large Float64Array slabs for activations and fixed scratch regions (by purpose).
BufferPool: rents/recycles scratch Float64Array by size class, returns immediately after use.
ObjectPool<T>: pools small objects (TensorView, ForwardContext shells, ADWIN bucket nodes, etc.).

MatrixOps (IN-PLACE NUMERICS KERNELS)
Static in-place ops specialized for Float64Array: matmul, gemv, add, scale, axpy, dot, softmaxStable, layerNorm, gelu, sigmoid, clamp, globalNorm.
Strict loops only; provide fused kernels (matmul+bias+activation, softmax+mask, residual+dropout).
Numerical stability primitives (max-sub softmax, epsilon guards, finite masking).

Initializer (STABLE PARAM INIT)
xavierUniform(), heUniform(), smallUniform(), zeros(), ones().
Deterministic seeding via DeterministicRNG.
Applies init through ParameterStore without reflection or dynamic dispatch.

DeterministicRNG (REPRODUCIBLE INIT AND DROPOUT)
nextU32(), nextFloat01().
Used by Initializer and DropoutMaskGenerator.
Optional stateless hashing (seed + step + tensorId) to avoid storing masks when feasible.

OnlineNormalizer (WELFORD Z-SCORE FOR INPUTS AND OUTPUTS)
Maintains mean, m2, std, count per feature as Float64Array.
updateX(seqXRaw: number[][]): void
normalizeX(seqXRaw: number[][], out: Float64Array, seqLen: number, inputDim: number): void
updateY(yRaw: number[]): void
normalizeY(yRaw: number[], out: Float64Array, outputDim: number): void
Denormalize helpers used by predict().
Apply std floors consistently to avoid explosions.

ResidualStatsTracker (UNCERTAINTY AND ACCURACY STREAM)
Welford residual variance per output in normalized space.
updateResidual(rNormVec: Float64Array): void
Tracks running loss (mean or EMA) and derives bounded accuracy = 1 / (1 + avgLoss).
Computes standard error and prediction intervals for predict().

OutlierWeighter (ROBUST ONLINE WEIGHTING)
computeWeight(residualNorm: number, outlierThreshold: number): { isOutlier: boolean, weight: number }
Weight scales only the data-loss gradient (MSE path), not L2, unless explicitly configured.

LossComputer (MSE AND REGULARIZATION)
mse(yTargetNorm: Float64Array, yPredNorm: Float64Array): number
l2(params: ParameterStore, regularizationStrength: number): number
dMse_dyPredNorm(yTargetNorm: Float64Array, yPredNorm: Float64Array, outGrad: Float64Array): void
Support exactly one global policy: either L2-in-loss or decoupled AdamW style weight decay, consistently.

GradientClipper (GLOBAL NORM CLIP)
clip(params: ParameterStore, clipThreshold: number): number
Computes stable global norm and scales grads in-place if needed.

LRScheduler (WARMUP AND COSINE DECAY)
getLR(step: number): number
Pure function, no allocations, encapsulates warmupSteps, totalSteps, baseLR.

AdamOptimizer (INCREMENTAL ADAM UPDATER)
step(params: ParameterStore, lr: number, beta1: number, beta2: number, epsilon: number, weightDecayPolicy: number): void
In-place updates, maintains updateCount and bias correction.
Supports moment reset hooks used by drift policy.

AdwinDetector (DRIFT DETECTION)
update(errorValue: number): boolean
Bucketed adaptive window with split checks; uses pooled bucket nodes.
On drift: shrinks window, increments driftCount, triggers DriftStrategy actions via facade.

DriftStrategy (DRIFT RESPONSE POLICY)
onDrift(model: ModelCore, optimizer: AdamOptimizer, normalizer: OnlineNormalizer, scheduler: LRScheduler): void
Default actions: reset optimizer moments, temporary LR reduction, optional normalization adjustments.

ForwardContext (MINIMAL TRAINING TAPE)
Holds only references and offsets into TensorArena needed for backprop.
Includes (as offsets/views): normalized input view, per-channel patch buffers, per-scale patch token embeddings, aligned/fused patch token sequence, per-block LN stats, Q/K/V tile buffers, attention tile buffers, FFN activations, pooling weights, agg vector(s), yPredNorm.
No deep object graphs; allocated once and reused as a mutable container.

Layer Interfaces (INTERNAL CONTRACTS, LIGHTWEIGHT)
ILayer: forward(in: TensorView, ctx: ForwardContext): TensorView; backward(dOut: TensorView, ctx: ForwardContext): TensorView
ITrainableLayer: exposes parameter ids/slices in ParameterStore
Avoid virtual calls in inner loops when possible (prefer direct method calls inside ModelCore).

PatchMaker
Deterministically converts a (seqLen,inputDim) sequence into per-channel patch matrices.
Computes patchCount = floor((seqLen - patchLen)/stride) + 1 with deterministic handling when seqLen < patchLen (pad/truncate policy must be deterministic and allocation-free).
Supports multi-scale patching: temporalScales defines scale multipliers for patchLen and/or stride; patchLenBase derived from temporalKernelSize (or a fixed deterministic function of it).
Produces patchRaw views that are reused buffers in TensorArena.

PatchEmbeddingBank
For each scale and channel, projects each raw patch (length patchLen) into embeddingDim with a linear layer (W_patch, b_patch).
Optionally applies a lightweight pre-embedding temporal filter using temporalKernelSize (implemented as in-place convolution over the raw channel sequence into a scratch buffer, then patched), but must be optional and must not allocate.
Applies GELU in-place after projection.
Adds scale embeddings (one embedding per scale) and positional encodings (per patch position) from PositionalEncodingCache.

PositionalEncodingCache
Precomputes positional encodings once per (patchCount,length) up to limits derived from maxSequenceLength and minimum stride; stored in cache.
Provides zero-copy views, no recomputation.

PatchScaleFusion
Fuses multi-scale patch token streams into a single patch token stream per channel using learned gates (sigmoid or softmax across scales).
Uses fusionDropout (if nonzero) deterministically.
Stores gate logits/weights and any required fusion intermediates in ForwardContext for backprop.

ChannelIndependentTransformer
Implements PatchTST encoder: runs the same TransformerStack weights across channels independently (treat channels as an outer loop, not a batch dimension that allocates new tensors).
For each channel, runs numBlocks TransformerBlock forward passes over that channel’s patch token sequence.
Must reuse attention tiling buffers and arena scratch across channels; never allocate per channel.

TransformerStack
Holds numBlocks TransformerBlock instances.
Runs pre-LN residual MHA + FFN for each block, reusing arena scratch and fixed attention tiling buffers.

TransformerBlock
LayerNorm1 -> MultiHeadSelfAttention -> Residual -> LayerNorm2 -> FeedForward -> Residual (optional dropout).
Caches minimal LN stats and attention/FFN intermediates required for backward.

LayerNorm
In-place LN forward/backward with gamma/beta parameters and stable epsilon.

MultiHeadSelfAttention
Projects Q/K/V, applies optional causal mask (configurable policy: PatchTST can be causal or non-causal; choose one deterministic default and document it), stable softmax, computes head outputs and final projection.
Supports RelPosBias in score computation.
Must use streaming/tiled attention with fixed buffers (tileQ x tileK) allocated once; no full score/weight matrices.

RelPosBias
Stores learnable relative position bias table or computed bias; fast lookup by (i-j) index in patch-token coordinates.

FeedForward
Two linear layers with GELU, hiddenDim = embeddingDim * ffnMultiplier.
Uses fused kernels and minimal caching for backward.

DropoutMaskGenerator (OPTIONAL, DETERMINISTIC)
Generates dropout masks into preallocated buffers using DeterministicRNG and step index.
If dropout is 0.0, it must be fully bypassed with near-zero overhead.

AttentionPooling
Computes pooling logits and weights over patch tokens, produces agg vector per channel (or a single fused agg, depending on output head design).
Stores pooling weights/logits in ForwardContext for backward.

OutputHead
Maps aggregated representation(s) to yPredNorm using W_out and b_out.
Must support outputDim possibly different from inputDim (multivariate regression targets).
Predict path denormalizes using OnlineNormalizer.

ModelMetrics (SUMMARY AND READBACK)
Builds ModelSummary (initialized, dims, param count, sampleCount, accuracy, converged, effectiveLR, driftCount).
Provides getWeights() and getNormalizationStats() views without copying when safe; otherwise copies only on request.

ModelSerializer (SAVE AND LOAD)
save(state): string serializes config, tensors, normalizer stats, metrics, optimizer step, drift state.
load(json): void validates shapes and restores into existing buffers when possible; if mismatch, rebuilds deterministically.
Stable schema versioning and compatibility hooks.

TYPES (MUST MATCH)
FitResult { loss, gradientNorm, effectiveLearningRate, isOutlier, converged, sampleIndex, driftDetected }
PredictionResult { predictions: SinglePrediction[], accuracy, sampleCount, isModelReady }
SinglePrediction { predicted, lowerBound, upperBound, standardError: number[] }
WeightInfo { temporalConvWeights, scaleEmbeddings, positionalEncoding, fusionWeights, attentionWeights, ffnWeights, layerNormParams, outputWeights, firstMoment, secondMoment: number[][][], updateCount }
NormalizationStats { inputMean, inputStd, outputMean, outputStd: number[], count }
ModelSummary { isInitialized, inputDimension, outputDimension, numBlocks, embeddingDim, numHeads, temporalScales, totalParameters, sampleCount, accuracy, converged, effectiveLearningRate, driftCount }

PSEUDOCODE (IMPLEMENTATION PLAN)

Initialization strategy
Constructor stores merged config and creates subsystem objects without allocating large slabs.
ensureInitialized(seqLen, inputDim, outputDim) runs once:
Allocate ParameterStore.W/G/M/V as contiguous Float64Array buffers with deterministic sizes derived from config and dims, including PatchMaker/PatchEmbeddingBank weights, fusion weights, Transformer weights, pooling weights, and output head weights.
Create and pool TensorView slices for every tensor region.
Apply Initializer with DeterministicRNG.
Derive patching hyperparameters deterministically from config:

* basePatchLen: a deterministic function of temporalKernelSize (e.g., basePatchLen = max(2, temporalKernelSize * 4) or another fixed rule documented in code)
* per-scale patchLen[s] = basePatchLen * temporalScales[s]
* per-scale stride[s] = max(1, patchLen[s] / 2) or another fixed rule documented in code
  Compute maximum patchCount supported under maxSequenceLength and min stride; allocate TensorArena slabs accordingly:
* raw patch buffers (per channel, per scale) reused
* patch token embeddings (per channel, per scale) reused
* fused patch token buffer (per channel) reused
* Transformer activations and attention tiling buffers sized to maxPatchCount (not seqLen)
  Build ModelCore graph and bind layers to ParameterStore views and arena buffers.
  Allocate facade scratch vectors (outputDim): yTargetNorm, dLoss_dyPredNorm, residualNorm.

fitOnline({ xCoordinates, yCoordinates }) flow
Validate xCoordinates is [seqLen][inputDim].
Select target row from yCoordinates: if length==1 use row 0 else use last row; outputDim = yRow.length.
Apply deterministic caps: if seqLen > maxSequenceLength, truncate or stride to maxSequenceLength.
ensureInitialized(seqLen, inputDim, outputDim).
Update OnlineNormalizer with raw x and y.
Normalize x into arena input buffer (Float64Array length seqLen*inputDim).
Normalize y into scratch_yTargetNorm.
Cache last normalized sequence for predict() using a single reusable buffer; copy with tight loops only.
Forward pass: ctx = model.forward(seqXNorm, seqLen, inputDim); yPredNorm is a view stored in ctx.
Compute residuals in normalized space; compute residual RMSE scalar; OutlierWeighter returns (isOutlier, wData).
Compute dataLoss (MSE) and regLoss according to the chosen global policy; totalLoss = wData*dataLoss + regLoss.
Seed gradient: dMse/dyPredNorm into scratch_dLoss_dyPredNorm, then scale by wData.
Zero gradients in ParameterStore.G.
Backward pass: model.backward(ctx, scratch_dLoss_dyPredNorm) fills grads in-place.
Clip gradients by global norm via GradientClipper.
Compute lr via LRScheduler.getLR(stepIndex).
Apply AdamOptimizer.step in-place (with consistent weight decay policy).
Update ResidualStatsTracker with residual vector and running loss; derive accuracy = 1/(1+avgLoss).
Convergence detection uses convergenceThreshold on stable loss delta and/or gradNorm with hysteresis if needed.
Drift detection: errorSignal = residualRMSE or totalLoss (choose one), driftDetected = AdwinDetector.update(errorSignal).
If driftDetected, DriftStrategy.onDrift(...) applies configured responses (moment reset, LR reduction, optional normalizer actions).
Increment stepIndex and sampleCount.
Return FitResult with loss, gradientNorm, effectiveLearningRate, isOutlier, converged, sampleIndex, driftDetected.

ModelCore.forward(seqXNorm, seqLen, inputDim) outline
PatchMaker produces per-channel raw patch matrices for each scale using deterministic patchLen/stride rules; writes into arena buffers.
PatchEmbeddingBank projects patches into embeddingDim tokens; adds scale embeddings and positional encodings; GELU in-place; stores per-scale token streams in ctx.
PatchScaleFusion computes gates across scales and produces a single fused patch-token sequence per channel; records gate stats in ctx.
ChannelIndependentTransformer loops channels:

* runs TransformerStack over that channel’s fused patch-token sequence
* MultiHeadSelfAttention must compute attention via fixed tiling over patchCount: iterate query blocks and key blocks, accumulate stable softmax in streaming form, store only minimal per-tile info needed for backward in ctx
  AttentionPooling produces agg vector(s) and stores pooling weights/logits.
  OutputHead produces yPredNorm view in arena and stores it in ctx.
  Return ctx.

ModelCore.backward(ctx, dLoss_dyPredNorm) outline
Backprop through OutputHead to dAgg and parameter grads.
Backprop through AttentionPooling to dH and pooling parameter grads.
Backprop through ChannelIndependentTransformer:

* for each channel, backprop through TransformerStack blocks in reverse order, accumulating grads in ParameterStore
  Backprop through PatchScaleFusion to per-scale token grads and fusion parameter grads.
  Backprop through PatchEmbeddingBank to patch projection weights and to raw patch grads (if needed for optional pre-embedding filter grads).
  Backprop through PatchMaker only if required by any learnable pre-embedding filter; otherwise stop at normalized input.
  Do not allocate new buffers; reuse arena scratch and ctx offsets.

predict(futureSteps) flow
If not initialized or no cached last sequence, return empty predictions with isModelReady false.
Determine isModelReady from sampleCount and normalizer validity.
For each step in 0..futureSteps-1:
Forward pass on lastSeqXNorm to get yPredNorm.
Denormalize to predicted[].
Compute standardError and bounds using ResidualStatsTracker variance and sampleCount, then scale to raw space using normalizer output std.
Append SinglePrediction to results (allocations allowed only for returned objects).
Optionally roll lastSeqXNorm in-place (shift left by one row); keep exogenous features constant unless config provides a mapping to inject predicted targets into input features.
Return PredictionResult with predictions, accuracy, sampleCount, isModelReady.

getModelSummary(), getWeights(), getNormalizationStats()
Return stable snapshots; avoid copying unless explicitly needed (slow path allowed only here).

reset()
Reinitialize or reset weights deterministically, reset optimizer moments, normalizer stats, residual stats, drift detector, counters, and cached sequence.

save() and load(w)
save serializes config, dims, counters, normalizer state, residual stats, drift state, optimizer state, and ParameterStore buffers deterministically into JSON.
load parses JSON, validates schema, ensures initialized with correct dims, restores into existing buffers when sizes match; if mismatch, rebuild ParameterStore and ModelCore deterministically, then restore; restore cached last sequence into a reusable buffer.
