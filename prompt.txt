Implement TypeScript library "PatchTSTRegression": a PatchTST (Patch-based Time Series Transformer) neural network for multivariate regression with incremental online learning, Adam optimizer, and Welford z-score normalization.

SCOPE AND GOALS
Build a single self-contained TypeScript library (no heavy runtime deps) that trains online (one sample at a time) and predicts multiple steps ahead.
Primary use-case is tight CPU and memory environments, so the implementation must be deterministic, allocation-free in hot paths, and numerically stable.

PERFORMANCE PRINCIPLES (HARD REQUIREMENTS)
Fixed-size memory: preallocate all parameter, gradient, optimizer-moment, and scratch buffers once at initialization; never grow during training.
Bounded attention memory: do not allocate O(seqLen^2) attention score/weight matrices; implement streaming or tiled attention using fixed reusable blocks. (For PatchTST, attention length is numberOfPatches, but the same constraint applies.)
Typed-array only core: all tensors live in contiguous Float64Array slabs in row-major layout; expose zero-copy views via (data, offset, shape, strides).
No hot-path allocations: forward(), backward(), fitOnline() must allocate 0 arrays/objects per call; all temporaries come from TensorArena and BufferPool.
Strict reuse policy: rent scratch buffers by size class, return immediately; no new Float64Array inside inner loops.
In-place and fused kernels: prefer fused ops (matmul+bias+activation, softmax+mask, residual+dropout) and in-place transforms to avoid intermediates.
Deterministic caps: enforce maxSequenceLength and maxBatch=1; if inputs exceed limits, truncate/stride rather than allocating more memory.
Minimal training tape: store only what backprop strictly needs (offsets and compact stats); recompute cheap intermediates instead of caching large activations.
Tight loops only: classic for-loops with manual indexing; avoid map/reduce/forEach, closures, and polymorphic dispatch in inner loops.
Cache once, reuse forever: positional encodings, masks, index maps, and shape metadata computed once and reused; lazy init allowed, but never re-init.
GC guardrails: pool small objects (TensorView, ForwardContext shells, ADWIN nodes); keep debug and verbose metrics off by default.

DESIGN PRINCIPLES (HARD REQUIREMENTS)
Full numerical stability throughout (stable softmax, epsilon guards, finite checks, variance floors).
Object-oriented design with interfaces for public contracts; private field encapsulation for internal state.
JSDoc on public methods and key classes (@param, @returns, @example).
Inline math formula notes where it clarifies behavior (normalization, loss, Adam, softmax, layer norm).
Full backpropagation across all layers (no partial training shortcuts).

PUBLIC API (MUST MATCH)
fitOnline({ xCoordinates: number[][], yCoordinates: number[][] }): FitResult
Behavior: incremental Adam update, Welford z-score, L2 regularization, outlier downweighting, ADWIN drift detection.
predict(futureSteps: number): PredictionResult
Behavior: forward-only inference with uncertainty bounds from ResidualStatsTracker.
getModelSummary(): ModelSummary
getWeights(): WeightInfo
getNormalizationStats(): NormalizationStats
reset(): void
save(): string
Behavior: JSON.stringify of all state data.
load(w: string): void
Behavior: restore all state data from JSON.


