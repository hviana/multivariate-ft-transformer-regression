Write a single self-contained TypeScript module that exports class FusionTemporalTransformerRegression implementing a Fusion Temporal Transformer for multivariate regression with incremental online learning (online Adam), online z-score normalization (Welford), L2 regularization, outlier downweighting, and ADWIN drift detection.

OUTPUT RULES
Return ONLY the TypeScript code.
No external dependencies. No Node/Deno-specific APIs (pure TS).

HARD PERFORMANCE CONSTRAINTS
Use Float64Array for all numerical tensors and parameters.
Avoid allocations in hot paths: reuse preallocated buffers, caches, and scratch space.
Avoid creating intermediate arrays in forward/backward; use in-place ops where safe.
Use for-loops only in compute-heavy sections (no forEach/map/reduce).
Store matrices as contiguous 1D Float64Array in row-major with explicit shape/stride helpers.
Preallocate attention score buffers, Q/K/V buffers, FFN buffers, conv outputs per scale, fusion buffers, gradients, and optimizer moments.
Lazy-initialize buffers on first call when input/output dimensions become known.
No recursion; iterative only.

HARD DESIGN CONSTRAINTS
Full numerical stability; OOP with interfaces for public contracts
Private field encapsulation; JSDoc (@param, @returns, @example)
Inline math formula docs; normalize inputs; track running average loss for accuracy

PUBLIC API (EXACT)
class FusionTemporalTransformerRegression
constructor(config?: Partial<Config>)
fitOnline(data: { xCoordinates: number[][]; yCoordinates: number[][] }): FitResult
predict(futureSteps: number): PredictionResult
getModelSummary(): ModelSummary
getWeights(): WeightInfo
getNormalizationStats(): NormalizationStats
reset(): void
save(): string
load(w: string): void

TYPES (EXACT SHAPES, KEEP FIELD NAMES)
type FitResult = {
loss: number
gradientNorm: number
effectiveLearningRate: number
isOutlier: boolean
converged: boolean
sampleIndex: number
driftDetected: boolean
}

type SinglePrediction = {
predicted: number[]
lowerBound: number[]
upperBound: number[]
standardError: number[]
}

type PredictionResult = {
predictions: SinglePrediction[]
accuracy: number
sampleCount: number
isModelReady: boolean
}

type WeightInfo = {
temporalConvWeights: number[][][]
scaleEmbeddings: number[][]
positionalEncoding: number[][]
fusionWeights: number[][]
attentionWeights: number[][][]
ffnWeights: number[][][]
layerNormParams: number[][]
outputWeights: number[][]
firstMoment: number[][][]
secondMoment: number[][][]
updateCount: number
}

type NormalizationStats = {
inputMean: number[]
inputStd: number[]
outputMean: number[]
outputStd: number[]
count: number
}

type ModelSummary = {
isInitialized: boolean
inputDimension: number
outputDimension: number
numBlocks: number
embeddingDim: number
numHeads: number
temporalScales: number[]
totalParameters: number
sampleCount: number
accuracy: number
converged: boolean
effectiveLearningRate: number
driftCount: number
}

CONFIG DEFAULTS (EXACT DEFAULT VALUES)
numBlocks=3
embeddingDim=64
numHeads=8
ffnMultiplier=4
attentionDropout=0.0
fusionDropout=0.0
learningRate=0.001
warmupSteps=100
totalSteps=10000
beta1=0.9
beta2=0.999
epsilon=1e-8
regularizationStrength=1e-4
convergenceThreshold=1e-6
outlierThreshold=3.0
adwinDelta=0.002
temporalScales=[1,2,4]
temporalKernelSize=3
maxSequenceLength=512

DATA CONTRACT (REMOVE AMBIGUITY)
fitOnline receives one training sample as a sequence window:
xCoordinates is seqLen x inputDim (seqLen = xCoordinates.length).
yCoordinates is seqLen x outputDim (seqLen = yCoordinates.length).
Use target y = yCoordinates[seqLen-1] (last timestep) and treat the model output ŷ as outputDim (single vector after temporal pooling).
Also cache the latest xCoordinates window internally (clipped/padded to maxSequenceLength) for predict().

predict(futureSteps):
Compute one-step prediction from the last cached window.
For futureSteps>1, repeat autoregressively WITHOUT inventing future x features: repeat the same one-step mean prediction, and increase uncertainty with standardError *= sqrt(stepIndex+1).
Return predictions length = futureSteps.
isModelReady is true only after initialization and at least 2 samples for normalization variance.

ARCHITECTURE (MUST IMPLEMENT)

1. Auto-detect dims on first fitOnline: inputDim = xCoordinates[0].length, outputDim = yCoordinates[0].length, seqLen = min(xCoordinates.length, maxSequenceLength).
2. Online normalization:
   Maintain Welford stats for each input feature and each output dimension.
   Normalize x and y using current mean/std (std = sqrt(var), var from Welford M2; clamp std to >= 1e-12).
3. Temporal positional encoding:
   PE(pos,2i)=sin(pos/10000^(2i/d)), PE(pos,2i+1)=cos(pos/10000^(2i/d)).
   Compute up to maxSequenceLength and cache in Float64Array; expose positionalEncoding in getWeights as number[][] for the cached length.
4. Multi-scale temporal convolution:
   For each scale s in temporalScales:
   Stride = s. Output length Ls = ceil(seqLen / s).
   Conv1D over time with kernel temporalKernelSize, using zero-padding on the left for causality (only t-k where t-k>=0).
   Compute F_s[t,e] = GELU( sum_{k=0..K-1} sum_{f=0..inputDim-1} X[(t*s - k),f] * W_s[k,f,e] + b_s[e] ).
5. Scale-specific embedding:
   E_s = F_s + PE_s + ScaleEmb_s, where ScaleEmb_s is a learnable vector length embeddingDim added to every timestep.
   PE_s uses positional encoding at the scale’s timestep index (t) with same embeddingDim.
6. Cross-scale fusion (make lengths consistent deterministically):
   Define fine scale as s=1 length L1=seqLen.
   Upsample each scale s to length L1 by repetition: expanded index te = floor(t / s) clamped to [0, Ls-1].
   Build Concat(E_1_up, E_2_up, ... E_n_up) per timestep (dimension nScales*embeddingDim).
   Gates: G = sigmoid( Concat * Wg + bg ) where Wg maps (nScales*embeddingDim) -> (nScales*embeddingDim).
   Split G into per-scale gate vectors length embeddingDim.
   Fused[t,e] = sum_s ( G_s[t,e] * E_s_up[t,e] ).
   Apply fusionDropout to fused activations (in training only) using deterministic RNG seeded from updateCount (no Math.random in hot paths; implement a small xorshift32).
7. Transformer blocks repeated numBlocks:
   Each block: LayerNorm -> MultiHead Self-Attention -> residual -> LayerNorm -> FFN -> residual.
   LayerNorm per timestep over embeddingDim with learnable gamma/beta per block (store in layerNormParams).
   FFN hiddenDim = embeddingDim*ffnMultiplier, activation GELU.
8. Temporal aggregation (attention-weighted pooling):
   score[t] = dot(H[t], Wpool) + bpool (Wpool length embeddingDim).
   alpha = softmax(scores) over t.
   out = sum_t alpha[t] * H[t].
9. Output head:
   ŷ = out * Wout + bout, Wout shape embeddingDim x outputDim.

ATTENTION DETAILS (MUST IMPLEMENT)
d_k = embeddingDim/numHeads (require divisible; if not, throw error on init).
Project Q,K,V with separate weights per block:
Q = X * Wq, K = X * Wk, V = X * Wv, each shape (L1 x embeddingDim).
Split into heads (numHeads) with headDim=d_k.
Causal mask: disallow attending to future positions (j>i). Implement by setting score=-1e9.
Scaled dot-product: score = (Qi·Kj)/sqrt(d_k).
Softmax must be numerically stable (subtract row max).
Apply attentionDropout on attention probabilities during training (deterministic RNG).
Head outputs concatenated then projected with Wo to embeddingDim.

TRAINING / LOSS / OPTIMIZER (MUST IMPLEMENT)
Loss: MSE on normalized outputs:
L = (1/2) * mean_d ( (y_norm[d] - yhat_norm[d])^2 ) + (lambda/2)*sum(W^2) for all trainable weights (exclude normalization stats).
Outlier handling:
Compute residual r[d] = (y_norm[d] - yhat_norm[d]).
If any |r[d]| > outlierThreshold, set sampleWeight=0.1 else 1.0. Multiply loss and all output residual grads by sampleWeight.
Accuracy metric:
Maintain running average loss Lbar via exponential-free running mean: Lbar = (Lbar*(n-1)+loss)/n.
accuracy = 1/(1+Lbar).
Learning rate schedule:
warmup then cosine decay:
if step < warmupSteps: lr = baseLR * (step/warmupSteps)
else: progress = (step-warmupSteps)/max(1,totalSteps-warmupSteps), lr = baseLR * 0.5*(1+cos(pi*min(1,progress))).
Adam update with bias correction:
m = beta1*m + (1-beta1)*g
v = beta2*v + (1-beta2)*g*g
mhat = m/(1-beta1^t)
vhat = v/(1-beta2^t)
W -= lr * mhat/(sqrt(vhat)+epsilon)
Implement L2 regularization by adding lambda*W to gradient g before Adam.
Track gradientNorm = sqrt(sum(g^2)) over all parameters for this step.
Convergence:
converged = gradientNorm < convergenceThreshold.

BACKPROP REQUIREMENT (MUST IMPLEMENT)
Implement a fully-expanded backward pass through:
Output head, pooling softmax, transformer blocks (LN, MHA, FFN, residuals), fusion gating, scale embeddings, positional encoding addition (no grads for fixed PE), temporal conv per scale, and normalization (do not backprop into Welford stats; normalization uses current mean/std treated as constants).
Cache all necessary forward activations per layer for backprop; reuse buffers and overwrite where safe.
Implement stable LayerNorm backward (with epsilon).
Implement softmax backward for pooling and attention.
Implement GELU forward and backward (approx ok but must be stable).
Include optional global gradient clipping by norm at 5.0 (apply if norm > 5; scale all grads) to prevent explosions.

ADWIN DRIFT DETECTION (MUST IMPLEMENT, LIGHTWEIGHT)
Maintain an error window of recent losses (normalized, after outlier weight) up to a fixed cap (e.g., 256) in a ring buffer.
On each step after a minimum size (e.g., 32), test a split point that minimizes |mean_left-mean_right| significance using a simplified epsilon cut:
eps = sqrt( (2*log(2/delta)) * (1/n_left + 1/n_right) )
If |mean_left-mean_right| > eps, driftDetected=true, increment driftCount, and reset the window (but DO NOT wipe model weights; only reset window and optionally reset running loss Lbar to current loss).
Use adwinDelta as delta.

NUMERICAL STABILITY (MUST)
No NaN/Infinity propagation: clamp denominators, add epsilons, guard sqrt, guard exp overflow.
Softmax: subtract max; if all masked, fall back to uniform over allowed positions.
LayerNorm variance clamp to >= 1e-12.
Standard deviation clamp to >= 1e-12.
Use f32-safe constants only if needed; computations in Float64.

STATE / SERIALIZATION (MUST)
save(): JSON.stringify of a plain object containing:
config, dims, sampleCount, updateCount, driftCount, runningLoss, and ALL weights, moments, and normalization stats.
Represent Float64Array as regular number[] in JSON.
load(w): restore everything; validate shapes; rehydrate as Float64Array; set isInitialized accordingly.
reset(): reinitialize all trainable weights with stable init (Xavier/He appropriate), zero moments, reset counters and drift window; keep config.

WEIGHT INITIALIZATION (MUST)
Conv and linear weights: Xavier uniform (limit = sqrt(6/(fanIn+fanOut))) or He for GELU; choose one consistently and document in a short comment.
Biases start at 0.
LayerNorm gamma start 1, beta 0.
Scale embeddings small normal/uniform (e.g., ±0.02).

INTERNAL DESIGN (MUST)
Use private fields for all state.
Expose only requested methods.
Implement small internal helpers:
pack2D/ unpack2D for returning WeightInfo as number arrays (only in getters/save; not in hot path).
matMul, addBias, layerNorm, softmax, gelu, conv1D, attentionForward/backward, ffnForward/backward.
Implement a deterministic RNG (xorshift32) for dropout with a seed derived from updateCount.

FINAL CHECKLIST
All required types and methods exist and match names.
fitOnline updates normalization stats, forward, backward, Adam step, metrics, drift detection.
predict uses cached last window; returns bounds via ±1.96*standardError; standardError derived from running output residual variance per output dim (maintain online residual M2).
getWeights/getNormalizationStats/getModelSummary return current state in requested shapes.
No heavy allocations inside fitOnline forward/backward loops.