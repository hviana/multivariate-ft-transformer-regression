Implement TypeScript library "FusionTemporalTransformerRegression": Fusion Temporal Transformer neural network for multivariate regression with incremental online learning, Adam optimizer, z-score normalization.

PERFORMANCE:
- Minimize memory allocations by reusing arrays and objects wherever possible
- Use typed arrays (Float64Array) for all numerical computations
- Avoid creating intermediate arrays in hot paths
- Implement in-place matrix operations
- Use object pooling for frequently created objects
- Minimize garbage collection pressure
- Cache computed values that are reused
- Use efficient loop structures (avoid forEach, map, reduce in critical paths)
- Preallocate buffers for attention scores and layer activations
- Implement lazy initialization where appropriate
- Use iterative methods to prevent memory overallocation
- Write code that is highly CPU-optimized as well

DESIGN:
- Full numerical stability; OOP with interfaces for public contracts
- Private field encapsulation; JSDoc (@param, @returns, @example)
- Inline math formula docs; normalize inputs; track running average loss for accuracy
- Full backpropagation over all layers

CONFIG (defaults):
numBlocks: 3, embeddingDim: 64, numHeads: 8, ffnMultiplier: 4, attentionDropout: 0.0, learningRate: 0.001, warmupSteps: 100, totalSteps: 10000, beta1: 0.9, beta2: 0.999, epsilon: 1e-8, regularizationStrength: 1e-4, convergenceThreshold: 1e-6, outlierThreshold: 3.0, adwinDelta: 0.002, temporalScales: [1, 2, 4], temporalKernelSize: 3, maxSequenceLength: 512, fusionDropout: 0.0

API:
fitOnline({ xCoordinates: number[][], yCoordinates: number[][] }): FitResult → Incremental Adam, Welford's z-score, L2 reg, outlier downweighting, ADWIN drift detection
predict(futureSteps: number): PredictionResult → Predictions
getModelSummary(): ModelSummary | getWeights(): WeightInfo | getNormalizationStats(): NormalizationStats | reset(): void
save(): string //JSON.stringify in all state data
load(w: string) //JSON string of all state data

TYPES:
FitResult { loss, gradientNorm, effectiveLearningRate, isOutlier, converged, sampleIndex, driftDetected }
PredictionResult { predictions: SinglePrediction[], accuracy, sampleCount, isModelReady }
SinglePrediction { predicted, lowerBound, upperBound, standardError: number[] }
WeightInfo { temporalConvWeights, scaleEmbeddings, positionalEncoding, fusionWeights, attentionWeights, ffnWeights, layerNormParams, outputWeights, firstMoment, secondMoment: number[][][], updateCount }
NormalizationStats { inputMean, inputStd, outputMean, outputStd: number[], count }
ModelSummary { isInitialized, inputDimension, outputDimension, numBlocks, embeddingDim, numHeads, temporalScales, totalParameters, sampleCount, accuracy, converged, effectiveLearningRate, driftCount }

ALGORITHMS:

Fusion Temporal Transformer Architecture:

inputDim = xCoordinates[0].length
outputDim = yCoordinates[0].length
seqLen = xCoordinates.length

for pos from 0 to seqLen:
  for i from 0 to embeddingDim/2:
    angle = pos / power(10000, 2*i / embeddingDim)
    positionalEncoding[pos][2*i] = sin(angle)
    positionalEncoding[pos][2*i + 1] = cos(angle)

for each scale s in temporalScales:
  convOutput = conv1d(X, temporalKernelSize, stride=s)
  F_s = gelu(convOutput)  // shape: (seqLen/s, embeddingDim)
  E_s = F_s + positionalEncoding_s + scaleEmbedding[s]

fusedInput = concatenate(E_1, E_2, ..., E_s) along feature dim
gates = sigmoid(fusedInput * W_gate)
fused = sum over scales(gates[s] * E_s)

for block from 0 to numBlocks:
  normalized = layerNorm(fused)
  attention = temporalMultiHeadAttention(normalized)
  fused = fused + attention
  normalized = layerNorm(fused)
  ffnOut = feedForward(normalized)
  fused = fused + ffnOut

poolWeights = softmax(fused * W_pool)
aggregated = sum(poolWeights[i] * fused[i] for i in seqLen)
output = dense(aggregated)  // shape: outputDim


Temporal Multi-Head Self-Attention:

headDim = embeddingDim / numHeads

for h from 0 to numHeads:
  Q[h] = X * W_query[h]
  K[h] = X * W_key[h]
  V[h] = X * W_value[h]
  
  scores = (Q[h] * transpose(K[h])) / sqrt(headDim)
  scores = scores + temporalBias
  if useCausalMask:
    for i from 0 to seqLen:
      for j from i+1 to seqLen:
        scores[i][j] = -infinity
  attention[h] = softmax(scores) * V[h]

multiHead = concatenate(attention[0], ..., attention[numHeads-1])
output = multiHead * W_output + bias


Cross-Scale Attention Fusion:

Q = E_finest * W_query
allScales = concatenate(E_1, E_2, ..., E_s) along sequence dim
K = allScales * W_key
V = allScales * W_value

scores = (Q * transpose(K)) / sqrt(embeddingDim)
weights = softmax(scores)
fused = weights * V


Feed-Forward Network:

hiddenDim = embeddingDim * ffnMultiplier
hidden = X * W1 + b1
activated = gelu(hidden)
output = activated * W2 + b2


GELU Activation:

gelu(x) = 0.5 * x * (1 + tanh(sqrt(2/pi) * (x + 0.044715 * x^3)))


Layer Normalization:

mean = sum(x) / length
variance = sum((x[i] - mean)^2) / length
normalized = (x - mean) / sqrt(variance + epsilon)
output = gamma * normalized + beta


Adam Optimizer with Cosine Warmup:

normalize inputs:
  xNorm = (x - inputMean) / (inputStd + epsilon)

forward pass:
  cache all activations for backprop
  predicted = forwardThroughNetwork(xNorm)

compute loss:
  mseLoss = sum((y - predicted)^2) / (2 * n)
  l2Reg = (regularizationStrength / 2) * sum(W^2 for all weights)
  totalLoss = mseLoss + l2Reg

backward pass:
  compute gradients via chain rule through:
    output layer -> attention pooling -> transformer blocks -> fusion -> temporal conv

learning rate schedule:
  if step < warmupSteps:
    lr = learningRate * (step / warmupSteps)
  else:
    progress = (step - warmupSteps) / (totalSteps - warmupSteps)
    lr = learningRate * 0.5 * (1 + cos(pi * progress))

adam update for each weight W:
  m = beta1 * m + (1 - beta1) * gradient
  v = beta2 * v + (1 - beta2) * gradient^2
  mCorrected = m / (1 - beta1^step)
  vCorrected = v / (1 - beta2^step)
  W = W - lr * mCorrected / (sqrt(vCorrected) + epsilon)


Welford Online Normalization:

on new sample x:
  count = count + 1
  delta = x - mean
  mean = mean + delta / count
  delta2 = x - mean
  m2 = m2 + delta * delta2
  if count > 1:
    variance = m2 / (count - 1)
    std = sqrt(variance)


Running Accuracy:

totalLoss = totalLoss + currentLoss
sampleCount = sampleCount + 1
averageLoss = totalLoss / sampleCount
accuracy = 1 / (1 + averageLoss)


ADWIN Drift Detection:

add error to window
while window can be split:
  for each split point:
    leftMean = mean of left subwindow
    rightMean = mean of right subwindow
    n0 = left size, n1 = right size
    n = n0 + n1
    harmonicMean = (1/n0 + 1/n1)
    epsilonCut = sqrt(harmonicMean * ln(4/delta) / 2)
    if abs(leftMean - rightMean) >= epsilonCut:
      driftDetected = true
      remove older portion of window
      trigger stats reset
      break


Outlier Detection and Downweighting:

residual = (y - predicted) / outputStd
residualNorm = sqrt(sum(residual^2))
if residualNorm > outlierThreshold:
  isOutlier = true
  sampleWeight = 0.1
else:
  isOutlier = false
  sampleWeight = 1.0
weightedLoss = loss * sampleWeight


Prediction with Uncertainty:

predicted = forward(normalizedInput)
denormalized = predicted * outputStd + outputMean

standardError = outputStd / sqrt(sampleCount)
confidenceMultiplier = 1.96  // 95% confidence
lowerBound = denormalized - confidenceMultiplier * standardError
upperBound = denormalized + confidenceMultiplier * standardError


Convergence Check:

if abs(previousLoss - currentLoss) < convergenceThreshold:
  converged = true


Export: FusionTemporalTransformerRegression