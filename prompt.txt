Implement TypeScript library "FusionTemporalTransformerRegression": Fusion Temporal Transformer neural network for multivariate regression with incremental online learning, Adam optimizer, z-score normalization.

PERFORMANCE:
- Minimize memory allocations by reusing arrays and objects wherever possible
- Use typed arrays (Float64Array) for all numerical computations
- Avoid creating intermediate arrays in hot paths
- Implement in-place matrix operations
- Use object pooling for frequently created objects
- Minimize garbage collection pressure
- Cache computed values that are reused
- Use efficient loop structures (avoid forEach, map, reduce in critical paths)
- Preallocate buffers for attention scores and layer activations
- Implement lazy initialization where appropriate
- Use iterative methods to prevent memory overallocation
- Write code that is highly CPU-optimized as well

DESIGN:
- Full numerical stability; OOP with interfaces for public contracts
- Private field encapsulation; JSDoc (@param, @returns, @example)
- Inline math formula docs; normalize inputs; track running average loss for accuracy
- Full backpropagation over all layers

CONFIG (defaults):
numBlocks: 3, embeddingDim: 64, numHeads: 8, ffnMultiplier: 4, attentionDropout: 0.0, learningRate: 0.001, warmupSteps: 100, totalSteps: 10000, beta1: 0.9, beta2: 0.999, epsilon: 1e-8, regularizationStrength: 1e-4, convergenceThreshold: 1e-6, outlierThreshold: 3.0, adwinDelta: 0.002, temporalScales: [1, 2, 4], temporalKernelSize: 3, maxSequenceLength: 512, fusionDropout: 0.0

API:
fitOnline({ xCoordinates: number[][], yCoordinates: number[][] }): FitResult → Incremental Adam, Welford's z-score, L2 reg, outlier downweighting, ADWIN drift detection
predict(futureSteps: number): PredictionResult → Predictions
getModelSummary(): ModelSummary | getWeights(): WeightInfo | getNormalizationStats(): NormalizationStats | reset(): void
save(): string //JSON.stringify in all state data
load(w: string) //JSON string of all state data

TYPES:
FitResult { loss, gradientNorm, effectiveLearningRate, isOutlier, converged, sampleIndex, driftDetected }
PredictionResult { predictions: SinglePrediction[], accuracy, sampleCount, isModelReady }
SinglePrediction { predicted, lowerBound, upperBound, standardError: number[] }
WeightInfo { temporalConvWeights, scaleEmbeddings, positionalEncoding, fusionWeights, attentionWeights, ffnWeights, layerNormParams, outputWeights, firstMoment, secondMoment: number[][][], updateCount }
NormalizationStats { inputMean, inputStd, outputMean, outputStd: number[], count }
ModelSummary { isInitialized, inputDimension, outputDimension, numBlocks, embeddingDim, numHeads, temporalScales, totalParameters, sampleCount, accuracy, converged, effectiveLearningRate, driftCount }

ALGORITHMS (DETAILED):

1. DATA CONTRACT + SHAPES (per fitOnline call)

* X = xCoordinates, shape: (seqLen, inputDim)
* Y = yCoordinates, shape: (ySeqLen, outputDim)
* Target vector yTarget:

  * if ySeqLen == 1: yTarget = Y[0]
  * else: yTarget = Y[ySeqLen - 1]  // last step target by default
* seqLen = X.length, inputDim = X[0].length, outputDim = yTarget.length

2. ONLINE NORMALIZATION (Welford, per feature, inputs + outputs)

* Maintain per-input-feature: meanX[inputDim], m2X[inputDim], stdX[inputDim], countX
* Maintain per-output-feature: meanY[outputDim], m2Y[outputDim], stdY[outputDim], countY
* Update stats on each fitOnline before forward (or update after forward using raw x/y; implementation must be consistent):
  count += 1
  delta = v - mean
  mean += delta / count
  delta2 = v - mean
  m2 += delta * delta2
  variance = (count > 1) ? (m2 / (count - 1)) : 0
  std = sqrt(max(variance, 0))
* Normalize with epsilon guard + optional floor to avoid explosion:
  xNorm[i] = (xRaw[i] - meanX[i]) / (stdX[i] + epsilon)
  yNorm[k] = (yRaw[k] - meanY[k]) / (stdY[k] + epsilon)

3. PARAMETER INITIALIZATION (numerically stable defaults)

* Linear/conv/attention weights: Xavier/Glorot uniform for dot-product layers
* FFN weights (with GELU): He/Kaiming uniform is acceptable; pick one consistently
* Biases: 0
* LayerNorm: gamma = 1, beta = 0
* Scale embeddings: small normal/uniform (e.g., ~1e-3 magnitude) to avoid early dominance

4. INPUT PROJECTION TO EMBEDDING SPACE

* Project each timestep to embeddingDim:
  E0[t, d] = sum_{c=0..inputDim-1} Xnorm[t, c] * W_in[d, c] + b_in[d]
* Shape: E0 is (seqLen, embeddingDim)

5. MULTI-SCALE TEMPORAL CONV (per scale s in temporalScales)

* Stride = s, kernelSize = temporalKernelSize, padding = causal or same-padding (choose and implement consistently)
* For each output timestep u in [0..L_s-1], where L_s = ceil(seqLen / s):
  center = u * s
  Conv_s[u, d] = b_conv[s, d] + sum_{k=0..kernelSize-1} sum_{d2=0..embeddingDim-1} E0[idx(center, k), d2] * W_conv[s, d, d2, k]
  where idx(center,k) maps to a valid time index with padding (e.g., center + k - pad)
* Activation:
  F_s[u, d] = gelu(Conv_s[u, d])
* Positional encoding for this scale (length L_s):
  PE_s[u, 2i]   = sin(u / 10000^(2i/embeddingDim))
  PE_s[u, 2i+1] = cos(u / 10000^(2i/embeddingDim))
* Add scale embedding (learned vector per scale):
  E_s[u, d] = F_s[u, d] + PE_s[u, d] + ScaleEmb[s, d]

6. CROSS-SCALE FUSION (gated, per timestep u aligned to finest scale)

* Align all scales to a common timeline (typically finest scale length seqLen):

  * For each fine timestep t: sample/nearest/interpolate E_s at corresponding u = floor(t / s)
* Compute per-scale gates (per timestep, per scale; scalar gate or per-dimension gate)
  gateLogit_s[t] = dot(E_s_aligned[t], W_gate[s]) + b_gate[s]
  gate_s[t] = sigmoid(gateLogit_s[t])
* Fuse:
  fused[t, d] = sum_{s} gate_s[t] * E_s_aligned[t, d]
* Optional (more stable): normalize gates across scales
  gate_s[t] = exp(gateLogit_s[t]) / sum_r exp(gateLogit_r[t])

7. TRANSFORMER BLOCKS (numBlocks, pre-LN residual stack)
   For block = 0..numBlocks-1:
   a) LayerNorm (pre-norm)
   LN(x): mean = avg(x), var = avg((x-mean)^2)
   xhat = (x-mean) / sqrt(var + epsilon)
   out = gamma * xhat + beta
   b) Temporal Multi-Head Self-Attention (MHA)
   headDim = embeddingDim / numHeads
   For each head h:
   Q[t] = fusedLN[t] * Wq[block,h]
   K[t] = fusedLN[t] * Wk[block,h]
   V[t] = fusedLN[t] * Wv[block,h]
   score[i,j] = (dot(Q[i], K[j]) / sqrt(headDim)) + RelPosBias[i-j]
   If causal mask: for j > i => score[i,j] = -1e9 (or -Infinity in math, finite large negative in code)
   Softmax (stable): w[i,j] = exp(score[i,j] - max_j score[i,j]) / sum_j exp(score[i,j] - max_j score[i,j])
   headOut[i] = sum_j w[i,j] * V[j]
   Concat heads -> H[i] (embeddingDim)
   attnOut[i] = H[i] * Wo[block] + bo[block]
   c) Residual
   fused = fused + attnOut
   d) LayerNorm (pre-FFN)
   e) Feed-Forward Network (FFN)
   hiddenDim = embeddingDim * ffnMultiplier
   h = fusedLN2 * W1 + b1
   h = gelu(h)
   ffnOut = h * W2 + b2
   f) Residual
   fused = fused + ffnOut
   g) Dropout (if enabled by config; attentionDropout/fusionDropout; use deterministic RNG state for reproducibility)

8. ATTENTION POOLING TO FIXED VECTOR

* Compute pooling logits per timestep:
  pLogit[t] = dot(fused[t], W_pool) + b_pool
* Stable softmax over t:
  pW[t] = exp(pLogit[t] - max_t pLogit[t]) / sum_t exp(pLogit[t] - max_t pLogit[t])
* Aggregate:
  agg[d] = sum_{t=0..seqLen-1} pW[t] * fused[t, d]

9. OUTPUT HEAD (multivariate regression)

* Predicted normalized output:
  yPredNorm[k] = sum_{d=0..embeddingDim-1} agg[d] * W_out[k, d] + b_out[k]
* Denormalize for predict():
  yPred[k] = yPredNorm[k] * (stdY[k] + epsilon) + meanY[k]

10. LOSS (weighted MSE + L2 regularization)

* Compute residual in normalized space:
  r[k] = yTargetNorm[k] - yPredNorm[k]
* MSE:
  mse = (1 / outputDim) * sum_k (r[k]^2)
* L2 (exclude biases and LayerNorm beta optionally; gamma can be included or excluded consistently):
  l2 = 0.5 * regularizationStrength * sum_{w in weights} (w^2)
* Total (before outlier weighting):
  loss = mse + l2

11. OUTLIER DETECTION + DOWNWEIGHTING (robust online training)

* Residual norm in normalized output space:
  rNorm = sqrt(sum_k r[k]^2)
* If rNorm > outlierThreshold:
  isOutlier = true
  sampleWeight = 0.1  // or clamp(outlierThreshold / rNorm, minWeight, 1.0)
* Else:
  isOutlier = false
  sampleWeight = 1.0
* Weighted loss:
  lossWeighted = sampleWeight * mse + l2
* Backprop scales gradients from mse part by sampleWeight (do NOT scale L2 unless you intend to)

12. BACKPROPAGATION (full-network gradients, cached activations)

* Cache minimal tensors needed for gradients:

  * E0, per-scale conv inputs/outputs, fused, per-block LN stats, Q/K/V per head, attention weights, FFN activations, pooling weights
* Compute gradients via chain rule in reverse:
  output head -> pooling -> transformer blocks (FFN + MHA) -> fusion gates -> temporal conv -> input projection
* Gradient clipping (recommended for stability in online mode):
  globalNorm = sqrt(sum_all_params grad^2)
  if globalNorm > clipThreshold: scale all grads by clipThreshold / (globalNorm + 1e-12)

13. ADAM OPTIMIZER (with warmup + cosine decay schedule)

* Learning rate schedule:
  if step < warmupSteps:
  lr = learningRate * (step / warmupSteps)
  else:
  progress = (step - warmupSteps) / max(1, totalSteps - warmupSteps)
  lr = learningRate * 0.5 * (1 + cos(pi * progress))
* Adam per parameter W:
  m = beta1 * m + (1 - beta1) * g
  v = beta2 * v + (1 - beta2) * (g*g)
  mHat = m / (1 - beta1^(step+1))
  vHat = v / (1 - beta2^(step+1))
  W = W - lr * mHat / (sqrt(vHat) + epsilon)
* Weight decay option (recommended for online stability; pick ONE approach):
  a) L2-in-loss (as above): g += regularizationStrength * W
  b) Decoupled AdamW: W *= (1 - lr * regularizationStrength) and do NOT add L2 into g

14. RUNNING LOSS/ACCURACY (online metric)

* Maintain running average loss (EMA or mean):
  avgLoss = avgLoss + (lossWeighted - avgLoss) / sampleCount
* Map to bounded “accuracy” score (simple monotonic proxy):
  accuracy = 1 / (1 + avgLoss)

15. ADWIN DRIFT DETECTION (on prediction error stream)

* Input to ADWIN: e = mse (or rNorm) per sample
* Maintain adaptive window with bucketed counts/sums; periodically test all split points:
  For each split (W0, W1):
  n0, n1 = sizes; mu0, mu1 = means
  epsCut = sqrt((1/n0 + 1/n1) * ln(4/delta) / 2)
  If |mu0 - mu1| >= epsCut:
  driftDetected = true
  drop oldest part of window (shrink)
  driftCount += 1
  Optional actions (choose and implement):
  - reset optimizer moments (m,v) to 0
  - partially reset normalization stats (or freeze stats for N steps)
  - temporarily reduce lr (e.g., lr *= 0.5 for some steps)

16. PREDICTION UNCERTAINTY (approximate, online)

* Track running residual variance per output (normalized space) using Welford on r[k]
* Estimated standard error per output:
  seNorm[k] = sqrt(varResidualNorm[k]) / sqrt(max(1, sampleCount))
* Convert to original scale:
  se[k] = seNorm[k] * (stdY[k] + epsilon)
* 95% interval (normal approximation):
  lower[k] = yPred[k] - 1.96 * se[k]
  upper[k] = yPred[k] + 1.96 * se[k]

17. CONVERGENCE CHECK (online)

* Use absolute improvement threshold on running loss:
  converged = abs(prevAvgLoss - avgLoss) < convergenceThreshold
* Update prevAvgLoss each step (or every N steps for noise resistance)

Export: FusionTemporalTransformerRegression
