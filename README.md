Model: # ğŸ”® Multivariate Fusion Temporal Transformer Regression

<div align="center">

**A high-performance transformer architecture with multi-scale temporal
processing for online time series regression**

[ğŸ“¦ JSR Package](https://jsr.io/@hviana/multivariate-ft-transformer-regression)
â€¢ [ğŸ™ GitHub](https://github.com/hviana/multivariate-ft-transformer-regression)
â€¢ [ğŸ“– Documentation](#-api-reference)

</div>

---

## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [âš™ï¸ Configuration Parameters](#ï¸-configuration-parameters)
- [ğŸ“– API Reference](#-api-reference)
- [ğŸ¯ Use Case Examples](#-use-case-examples)
- [ğŸ”§ Optimization Guide](#-optimization-guide)
- [ğŸ’¾ Serialization](#-serialization)
- [ğŸ“Š Performance Tips](#-performance-tips)
- [ğŸ“œ License](#-license)

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ§  Advanced Architecture

- **Multi-scale Temporal Convolutions** - Extract features at different time
  resolutions
- **Cross-scale Gated Attention Fusion** - Intelligent feature combination
- **Transformer Blocks** - Self-attention with temporal bias
- **Attention-weighted Pooling** - Adaptive temporal aggregation

</td>
<td width="50%">

### âš¡ High Performance

- **Online Learning** - Incremental training with Adam optimizer
- **Buffer Pooling** - Memory-efficient computation
- **Cache-friendly Operations** - Optimized matrix operations
- **Float64 Precision** - High numerical accuracy

</td>
</tr>
<tr>
<td width="50%">

### ğŸ›¡ï¸ Robust Training

- **Z-score Normalization** - Welford's algorithm for running statistics
- **ADWIN Drift Detection** - Automatic concept drift handling
- **Outlier Detection** - Residual-based anomaly filtering
- **L2 Regularization** - Prevents overfitting

</td>
<td width="50%">

### ğŸ”§ Developer Experience

- **TypeScript Native** - Full type safety
- **Zero Dependencies** - Pure implementation
- **Serialization** - Save/load model state
- **Comprehensive API** - Detailed statistics and weights access

</td>
</tr>
</table>

---

## ğŸš€ Quick Start

### Installation

```typescript
import { FusionTemporalTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";
```

### Basic Usage

```typescript
// 1. Create model instance
const model = new FusionTemporalTransformerRegression({
  numBlocks: 3,
  embeddingDim: 64,
  numHeads: 8,
  learningRate: 0.001,
});

// 2. Train incrementally (online learning)
for (const batch of dataStream) {
  const result = model.fitOnline({
    xCoordinates: batch.inputs, // [[x1, x2, ...], [x1, x2, ...], ...]
    yCoordinates: batch.outputs, // [[y1, y2, ...], [y1, y2, ...], ...]
  });

  console.log(`ğŸ“‰ Loss: ${result.loss.toFixed(6)}`);
  console.log(`âœ… Converged: ${result.converged}`);
}

// 3. Make predictions
const predictions = model.predict(5); // Predict 5 future steps
predictions.predictions.forEach((pred, i) => {
  console.log(`Step ${i + 1}: ${pred.predicted} Â± ${pred.standardError}`);
});
```

---

## ğŸ—ï¸ Architecture

The Fusion Temporal Transformer combines multiple advanced techniques for time
series processing:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FUSION TEMPORAL TRANSFORMER                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                        INPUT SEQUENCE                                â”‚   â”‚
â”‚  â”‚                    [seq_len Ã— input_dim]                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                               â”‚
â”‚                             â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              STAGE 1: MULTI-SCALE TEMPORAL CONVOLUTION              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚   â”‚
â”‚  â”‚  â”‚  Scale 1    â”‚  â”‚  Scale 2    â”‚  â”‚  Scale 4    â”‚  ...            â”‚   â”‚
â”‚  â”‚  â”‚ (stride=1)  â”‚  â”‚ (stride=2)  â”‚  â”‚ (stride=4)  â”‚                 â”‚   â”‚
â”‚  â”‚  â”‚   Conv1D    â”‚  â”‚   Conv1D    â”‚  â”‚   Conv1D    â”‚                 â”‚   â”‚
â”‚  â”‚  â”‚   + GELU    â”‚  â”‚   + GELU    â”‚  â”‚   + GELU    â”‚                 â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚            â”‚                â”‚                â”‚                              â”‚
â”‚            â–¼                â–¼                â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚        STAGE 2: POSITIONAL ENCODING + SCALE EMBEDDINGS              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    F_s = Conv_s(X) + PE(pos) + ScaleEmb_s                           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    PE(pos, 2i)   = sin(pos / 10000^(2i/d))                          â”‚   â”‚
â”‚  â”‚    PE(pos, 2i+1) = cos(pos / 10000^(2i/d))                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                               â”‚
â”‚                             â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              STAGE 3: CROSS-SCALE GATED ATTENTION FUSION            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚  â”‚    â”‚  G = Ïƒ(Concat(Eâ‚,...,Eâ‚›) Ã— Wg + bg)         â”‚                  â”‚   â”‚
â”‚  â”‚    â”‚  Fused = Î£(Gâ‚› âŠ™ Eâ‚›)                         â”‚                  â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                               â”‚
â”‚                             â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              STAGE 4: TRANSFORMER BLOCKS (Ã— numBlocks)              â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚                                                               â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   â”‚LayerNorm â”‚â”€â”€â–¶â”‚  Multi-Head Attention   â”‚â”€â”€â–¶â”‚ Residual â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   â”‚    1     â”‚   â”‚  + Temporal Bias        â”‚   â”‚    +     â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                                                      â”‚        â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   â”‚LayerNorm â”‚â”€â”€â–¶â”‚   Feed-Forward Network  â”‚â”€â”€â–¶â”‚ Residual â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   â”‚    2     â”‚   â”‚   GELU(xWâ‚+bâ‚)Wâ‚‚+bâ‚‚    â”‚   â”‚    +     â”‚  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                                                               â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                               â”‚
â”‚                             â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              STAGE 5: ATTENTION-WEIGHTED TEMPORAL POOLING           â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    Î± = softmax(H Ã— W_pool)                                          â”‚   â”‚
â”‚  â”‚    out = Î£(Î±áµ¢ Ã— háµ¢)                                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                               â”‚
â”‚                             â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              STAGE 6: OUTPUT PROJECTION                             â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚    Å· = pooled Ã— W_out + b_out                                       â”‚   â”‚
â”‚  â”‚    [output_dim]                                                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Head Self-Attention Detail

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MULTI-HEAD SELF-ATTENTION                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚    Input: X [seq_len Ã— emb_dim]                                 â”‚
â”‚                                                                  â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚    â”‚   Wq    â”‚   â”‚   Wk    â”‚   â”‚   Wv    â”‚                      â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                      â”‚
â”‚         â”‚             â”‚             â”‚                            â”‚
â”‚         â–¼             â–¼             â–¼                            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚    â”‚    Q    â”‚   â”‚    K    â”‚   â”‚    V    â”‚                      â”‚
â”‚    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                      â”‚
â”‚         â”‚             â”‚             â”‚                            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚                            â”‚
â”‚                â”‚                    â”‚                            â”‚
â”‚                â–¼                    â”‚                            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                            â”‚
â”‚    â”‚   QKáµ€ / âˆšd_k        â”‚         â”‚                            â”‚
â”‚    â”‚   + Temporal Bias   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€ Learnable position bias  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                            â”‚
â”‚               â”‚                    â”‚                            â”‚
â”‚               â–¼                    â”‚                            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚                            â”‚
â”‚    â”‚     Softmax         â”‚         â”‚                            â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚                            â”‚
â”‚               â”‚                    â”‚                            â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                          â”‚                                       â”‚
â”‚                          â–¼                                       â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚               â”‚   Attention Ã— V     â”‚                           â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                          â”‚                                       â”‚
â”‚                          â–¼                                       â”‚
â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚               â”‚   Output Projection â”‚                           â”‚
â”‚               â”‚        Wo           â”‚                           â”‚
â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                  â”‚
â”‚    Formula: Attention(Q,K,V) = softmax(QKáµ€/âˆšd_k + bias) Ã— V    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Training Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ONLINE TRAINING PIPELINE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚   Input Data      â”‚ xCoordinates: [[xâ‚, xâ‚‚], [xâ‚ƒ, xâ‚„], ...]         â”‚
â”‚  â”‚   (Raw)           â”‚ yCoordinates: [[yâ‚], [yâ‚‚], ...]                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Welford Update   â”‚â”€â”€â–¶â”‚ Î¼ += (x - Î¼) / n                        â”‚    â”‚
â”‚  â”‚  (Running Stats)  â”‚   â”‚ Mâ‚‚ += (x - Î¼_old)(x - Î¼_new)            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Z-Score Norm     â”‚â”€â”€â–¶â”‚ x_norm = (x - Î¼) / Ïƒ                    â”‚    â”‚
â”‚  â”‚                   â”‚   â”‚ Ïƒ = âˆš(Mâ‚‚ / (n-1))                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                   â”‚
â”‚  â”‚  Forward Pass     â”‚ Cache activations for backpropagation            â”‚
â”‚  â”‚  (with caching)   â”‚                                                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Outlier Check    â”‚â”€â”€â–¶â”‚ r = |y - Å·| / Ïƒ                         â”‚    â”‚
â”‚  â”‚                   â”‚   â”‚ isOutlier = r > threshold               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Backward Pass    â”‚â”€â”€â–¶â”‚ âˆ‚L/âˆ‚W for all weights                   â”‚    â”‚
â”‚  â”‚  (Backprop)       â”‚   â”‚ + L2 regularization gradient            â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Adam Optimizer   â”‚â”€â”€â–¶â”‚ m = Î²â‚m + (1-Î²â‚)g                       â”‚    â”‚
â”‚  â”‚                   â”‚   â”‚ v = Î²â‚‚v + (1-Î²â‚‚)gÂ²                      â”‚    â”‚
â”‚  â”‚                   â”‚   â”‚ W -= Î· Ã— mÌ‚ / (âˆšvÌ‚ + Îµ)                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  ADWIN Drift      â”‚â”€â”€â–¶â”‚ |Î¼â‚€ - Î¼â‚| â‰¥ Îµ_cut â†’ drift detected     â”‚    â”‚
â”‚  â”‚  Detection        â”‚   â”‚ Reset statistics on drift               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration Parameters

### ğŸ“ Architecture Parameters

| Parameter            | Type       | Default     | Description                                                               |
| -------------------- | ---------- | ----------- | ------------------------------------------------------------------------- |
| `numBlocks`          | `number`   | `3`         | Number of transformer encoder blocks                                      |
| `embeddingDim`       | `number`   | `64`        | Internal embedding dimension (must be divisible by `numHeads`)            |
| `numHeads`           | `number`   | `8`         | Number of attention heads                                                 |
| `ffnMultiplier`      | `number`   | `4`         | Multiplier for FFN hidden layer (`ffnDim = embeddingDim Ã— ffnMultiplier`) |
| `temporalScales`     | `number[]` | `[1, 2, 4]` | Stride values for multi-scale temporal convolution                        |
| `temporalKernelSize` | `number`   | `3`         | Kernel size for temporal convolutions                                     |
| `maxSequenceLength`  | `number`   | `512`       | Maximum input sequence length                                             |

<details>
<summary>ğŸ’¡ <b>Architecture Optimization Tips</b></summary>

#### `numBlocks` - Transformer Depth

```typescript
// ğŸ”¹ Simple patterns (e.g., linear trends, basic seasonality)
{
  numBlocks: 2;
}

// ğŸ”¹ Moderate complexity (e.g., multiple seasonalities, non-linear trends)
{
  numBlocks: 3;
} // DEFAULT

// ğŸ”¹ Complex patterns (e.g., long-range dependencies, hierarchical patterns)
{
  numBlocks: 4 - 6;
}

// âš ï¸ More blocks = more parameters = longer training time
// âš ï¸ Diminishing returns after 4-5 blocks for most tasks
```

#### `embeddingDim` - Representation Capacity

```typescript
// ğŸ”¹ Low-dimensional data (< 10 features)
{ embeddingDim: 32, numHeads: 4 }

// ğŸ”¹ Medium-dimensional data (10-50 features)
{ embeddingDim: 64, numHeads: 8 }  // DEFAULT

// ğŸ”¹ High-dimensional data (50+ features)
{ embeddingDim: 128, numHeads: 8 }

// ğŸ”¹ Very complex relationships
{ embeddingDim: 256, numHeads: 16 }

// âš ï¸ embeddingDim must be divisible by numHeads
// âš ï¸ headDim = embeddingDim / numHeads (ideally >= 8)
```

#### `temporalScales` - Multi-resolution Processing

```typescript
// ğŸ”¹ Fine-grained patterns only
{
  temporalScales: [1];
}

// ğŸ”¹ Short to medium patterns
{
  temporalScales: [1, 2, 4];
} // DEFAULT

// ğŸ”¹ Multi-scale with longer patterns
{
  temporalScales: [1, 2, 4, 8, 16];
}

// ğŸ”¹ Focus on longer-term patterns
{
  temporalScales: [2, 4, 8, 16];
}

// Example: Hourly data with daily/weekly patterns
{
  temporalScales: [1, 4, 24, 168];
} // hour, 4-hour, day, week
```

</details>

---

### ğŸ“ˆ Learning Parameters

| Parameter      | Type     | Default | Description                           |
| -------------- | -------- | ------- | ------------------------------------- |
| `learningRate` | `number` | `0.001` | Base learning rate for Adam optimizer |
| `warmupSteps`  | `number` | `100`   | Steps for linear learning rate warmup |
| `totalSteps`   | `number` | `10000` | Total steps for cosine decay schedule |
| `beta1`        | `number` | `0.9`   | Adam first moment decay rate          |
| `beta2`        | `number` | `0.999` | Adam second moment decay rate         |
| `epsilon`      | `number` | `1e-8`  | Numerical stability constant          |

<details>
<summary>ğŸ’¡ <b>Learning Rate Optimization Tips</b></summary>

#### Learning Rate Schedule

```
Learning Rate
     â”‚
  lr â”‚    â•±â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â€¾â•²
     â”‚   â•±               â•²
     â”‚  â•±                 â•²
     â”‚ â•±                   â•²
     â”‚â•±                     â•²
   0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Steps
     â”‚â† warmup â†’â”‚â† cosine decay â†’â”‚
        100          10000
```

```typescript
// ğŸ”¹ Stable, slower convergence
{ learningRate: 0.0001, warmupSteps: 200 }

// ğŸ”¹ Balanced (DEFAULT)
{ learningRate: 0.001, warmupSteps: 100, totalSteps: 10000 }

// ğŸ”¹ Fast initial learning, quick adaptation
{ learningRate: 0.01, warmupSteps: 50 }

// ğŸ”¹ Long training, fine convergence
{ learningRate: 0.001, warmupSteps: 500, totalSteps: 50000 }
```

#### Adam Parameters

```typescript
// ğŸ”¹ Standard (most cases)
{ beta1: 0.9, beta2: 0.999 }  // DEFAULT

// ğŸ”¹ Faster adaptation to new data (streaming)
{ beta1: 0.8, beta2: 0.99 }

// ğŸ”¹ More stable updates (noisy data)
{ beta1: 0.95, beta2: 0.9999 }
```

</details>

---

### ğŸ›¡ï¸ Regularization & Robustness

| Parameter                | Type     | Default | Description                                     |
| ------------------------ | -------- | ------- | ----------------------------------------------- |
| `regularizationStrength` | `number` | `1e-4`  | L2 weight decay coefficient                     |
| `convergenceThreshold`   | `number` | `1e-6`  | Loss change threshold for convergence detection |
| `outlierThreshold`       | `number` | `3.0`   | Z-score threshold for outlier detection         |
| `adwinDelta`             | `number` | `0.002` | ADWIN drift detection sensitivity               |
| `attentionDropout`       | `number` | `0.0`   | Dropout rate in attention layers                |
| `fusionDropout`          | `number` | `0.0`   | Dropout rate in scale fusion                    |

<details>
<summary>ğŸ’¡ <b>Robustness Optimization Tips</b></summary>

#### Regularization Strength

```typescript
// ğŸ”¹ Minimal regularization (clean, large datasets)
{
  regularizationStrength: 1e-6;
}

// ğŸ”¹ Light regularization (DEFAULT)
{
  regularizationStrength: 1e-4;
}

// ğŸ”¹ Strong regularization (small datasets, overfitting prevention)
{
  regularizationStrength: 1e-3;
}

// ğŸ”¹ Very strong (highly noisy data)
{
  regularizationStrength: 1e-2;
}
```

#### Outlier Handling

```typescript
// ğŸ”¹ Strict outlier detection (clean data)
{
  outlierThreshold: 2.0;
}

// ğŸ”¹ Standard (DEFAULT)
{
  outlierThreshold: 3.0;
} // 99.7% normal distribution

// ğŸ”¹ Permissive (heavy-tailed distributions)
{
  outlierThreshold: 4.0;
}

// ğŸ”¹ Very permissive (accept most samples)
{
  outlierThreshold: 5.0;
}
```

#### Drift Detection

```typescript
// ğŸ”¹ Very sensitive to drift
{
  adwinDelta: 0.01;
}

// ğŸ”¹ Standard sensitivity (DEFAULT)
{
  adwinDelta: 0.002;
}

// ğŸ”¹ Less sensitive (stable environments)
{
  adwinDelta: 0.0001;
}
```

</details>

---

## ğŸ“– API Reference

### Constructor

```typescript
const model = new FusionTemporalTransformerRegression(config?: FusionTemporalConfig);
```

### Methods

#### `fitOnline(data)` â†’ `FitResult`

Performs a single online learning step.

```typescript
interface FitResult {
  loss: number; // Current training loss
  gradientNorm: number; // L2 norm of gradients
  effectiveLearningRate: number; // Current learning rate (after warmup/decay)
  isOutlier: boolean; // Whether sample was detected as outlier
  converged: boolean; // Whether model has converged
  sampleIndex: number; // Total samples seen
  driftDetected: boolean; // Whether concept drift was detected
}
```

**Example:**

```typescript
const result = model.fitOnline({
  xCoordinates: [
    [1.0, 2.0, 3.0], // timestep 1: 3 features
    [1.1, 2.1, 3.1], // timestep 2: 3 features
    [1.2, 2.2, 3.2], // timestep 3: 3 features
  ],
  yCoordinates: [
    [10.0], // target for timestep 1
    [10.5], // target for timestep 2
    [11.0], // target for timestep 3 (used for training)
  ],
});
```

---

#### `predict(futureSteps)` â†’ `PredictionResult`

Generates predictions with uncertainty estimates.

```typescript
interface PredictionResult {
  predictions: SinglePrediction[]; // Array of predictions
  accuracy: number; // Model accuracy estimate (0-1)
  sampleCount: number; // Training samples seen
  isModelReady: boolean; // Whether model is trained
}

interface SinglePrediction {
  predicted: number[]; // Point predictions
  lowerBound: number[]; // 95% CI lower bound
  upperBound: number[]; // 95% CI upper bound
  standardError: number[]; // Standard error estimates
}
```

**Example:**

```typescript
const predictions = model.predict(3);

predictions.predictions.forEach((pred, step) => {
  console.log(`Step ${step + 1}:`);
  console.log(`  Value: ${pred.predicted[0].toFixed(2)}`);
  console.log(
    `  95% CI: [${pred.lowerBound[0].toFixed(2)}, ${
      pred.upperBound[0].toFixed(2)
    }]`,
  );
});
```

---

#### `getModelSummary()` â†’ `ModelSummary`

Returns comprehensive model information.

```typescript
interface ModelSummary {
  isInitialized: boolean;
  inputDimension: number;
  outputDimension: number;
  numBlocks: number;
  embeddingDim: number;
  numHeads: number;
  temporalScales: number[];
  totalParameters: number;
  sampleCount: number;
  accuracy: number;
  converged: boolean;
  effectiveLearningRate: number;
  driftCount: number;
}
```

---

#### `getNormalizationStats()` â†’ `NormalizationStats`

Returns running normalization statistics.

```typescript
interface NormalizationStats {
  inputMean: number[]; // Running mean for inputs
  inputStd: number[]; // Running std for inputs
  outputMean: number[]; // Running mean for outputs
  outputStd: number[]; // Running std for outputs
  count: number; // Sample count for statistics
}
```

---

#### `getWeights()` â†’ `WeightInfo`

Returns all model weights and optimizer state.

---

#### `save()` â†’ `string`

Serializes model to JSON string.

---

#### `load(json: string)` â†’ `void`

Restores model from JSON string.

---

#### `reset()` â†’ `void`

Resets model to initial state.

---

## ğŸ¯ Use Case Examples

### ğŸ“Š Stock Price Prediction

```typescript
const stockModel = new FusionTemporalTransformerRegression({
  numBlocks: 4,
  embeddingDim: 128,
  numHeads: 8,
  temporalScales: [1, 5, 20, 60], // minute, 5-min, 20-min, hour
  learningRate: 0.0005,
  outlierThreshold: 4.0, // Financial data has fat tails
  adwinDelta: 0.005, // Quick regime change detection
});

// Features: [open, high, low, close, volume]
// Target: [next_close]
async function trainOnMarketData(stream: AsyncIterable<MarketData>) {
  for await (const data of stream) {
    const result = stockModel.fitOnline({
      xCoordinates: data.features, // Last N candles
      yCoordinates: data.targets,
    });

    if (result.driftDetected) {
      console.log("âš ï¸ Market regime change detected!");
    }
  }
}
```

---

### ğŸŒ¡ï¸ Sensor Data Forecasting

```typescript
const sensorModel = new FusionTemporalTransformerRegression({
  numBlocks: 2,
  embeddingDim: 32,
  numHeads: 4,
  temporalScales: [1, 6, 24, 168], // hour, 6-hour, day, week
  learningRate: 0.001,
  regularizationStrength: 1e-3, // Prevent overfitting on periodic data
  outlierThreshold: 3.5, // Handle sensor glitches
});

// Multiple sensor readings
const sensorData = {
  xCoordinates: [
    [temp1, humidity1, pressure1],
    [temp2, humidity2, pressure2],
    // ... last 24 hours of readings
  ],
  yCoordinates: [
    [temp_target1],
    [temp_target2],
    // ... corresponding targets
  ],
};

const result = sensorModel.fitOnline(sensorData);
const forecast = sensorModel.predict(24); // Forecast next 24 hours
```

---

### ğŸ­ Industrial Process Control

```typescript
const processModel = new FusionTemporalTransformerRegression({
  numBlocks: 3,
  embeddingDim: 64,
  numHeads: 8,
  temporalScales: [1, 2, 4, 8],
  learningRate: 0.0001, // Conservative for stability
  warmupSteps: 500,
  convergenceThreshold: 1e-7, // Tight convergence
  regularizationStrength: 1e-4,
});

// Control loop with online adaptation
function controlLoop(measurement: number[]) {
  // Train on new measurement
  processModel.fitOnline({
    xCoordinates: measurementHistory,
    yCoordinates: targetHistory,
  });

  // Predict next setpoint
  const prediction = processModel.predict(1);
  return prediction.predictions[0].predicted;
}
```

---

### ğŸ“ˆ Multi-target Regression

```typescript
// Predict multiple outputs simultaneously
const multiModel = new FusionTemporalTransformerRegression({
  numBlocks: 3,
  embeddingDim: 96,
  numHeads: 8,
});

// Input: 5 features, Output: 3 targets
multiModel.fitOnline({
  xCoordinates: [
    [f1, f2, f3, f4, f5],
    [f1, f2, f3, f4, f5],
    [f1, f2, f3, f4, f5],
  ],
  yCoordinates: [
    [y1, y2, y3],
    [y1, y2, y3],
    [y1, y2, y3],
  ],
});

const predictions = multiModel.predict(1);
console.log("Predicted targets:", predictions.predictions[0].predicted);
// Output: [predicted_y1, predicted_y2, predicted_y3]
```

---

## ğŸ”§ Optimization Guide

### Configuration Presets

```typescript
// ğŸƒ FAST: Quick training, lower accuracy
const fastConfig = {
  numBlocks: 2,
  embeddingDim: 32,
  numHeads: 4,
  ffnMultiplier: 2,
  learningRate: 0.01,
  warmupSteps: 50,
  temporalScales: [1, 2],
};

// âš–ï¸ BALANCED: Good tradeoff (DEFAULT-like)
const balancedConfig = {
  numBlocks: 3,
  embeddingDim: 64,
  numHeads: 8,
  ffnMultiplier: 4,
  learningRate: 0.001,
  warmupSteps: 100,
  temporalScales: [1, 2, 4],
};

// ğŸ¯ ACCURATE: Higher accuracy, slower training
const accurateConfig = {
  numBlocks: 5,
  embeddingDim: 128,
  numHeads: 8,
  ffnMultiplier: 4,
  learningRate: 0.0005,
  warmupSteps: 200,
  totalSteps: 50000,
  temporalScales: [1, 2, 4, 8, 16],
};

// ğŸŒŠ STREAMING: Optimized for online/streaming data
const streamingConfig = {
  numBlocks: 2,
  embeddingDim: 48,
  numHeads: 6,
  learningRate: 0.005,
  warmupSteps: 20,
  beta1: 0.8,
  beta2: 0.99,
  adwinDelta: 0.01,
  outlierThreshold: 3.0,
};

// ğŸ“‰ NOISY DATA: Robust to noise and outliers
const robustConfig = {
  numBlocks: 3,
  embeddingDim: 64,
  numHeads: 8,
  regularizationStrength: 1e-3,
  outlierThreshold: 2.5,
  attentionDropout: 0.1,
  fusionDropout: 0.1,
};
```

### Memory Estimation

```
Total Parameters â‰ˆ 
  numScales Ã— (K Ã— inputDim Ã— D + D)           // Temporal conv
  + numScales Ã— D                               // Scale embeddings
  + numScales Ã— D Ã— numScales + numScales      // Fusion
  + 4 Ã— D Ã— D                                   // Cross-scale attention
  + numBlocks Ã— (
      4 Ã— D Ã— D + D                             // Self-attention
      + seqLenÂ²                                 // Temporal bias
      + D Ã— ffnDim + ffnDim + ffnDim Ã— D + D   // FFN
      + 4 Ã— D                                   // LayerNorms
    )
  + D                                           // Pool weights
  + D Ã— outputDim + outputDim                  // Output projection

Example (default config, inputDim=10, outputDim=1, seqLen=100):
â‰ˆ 3Ã—(3Ã—10Ã—64+64) + 3Ã—64 + ... â‰ˆ 150,000 parameters
â‰ˆ 1.2 MB in Float64
```

---

## ğŸ’¾ Serialization

### Save and Load Model

```typescript
// Save model state
const modelState = model.save();

// Store in localStorage (browser)
localStorage.setItem("myModel", modelState);

// Or write to file (Deno/Node)
await Deno.writeTextFile("model.json", modelState);

// Later, restore the model
const newModel = new FusionTemporalTransformerRegression({
  // Must match original config!
  numBlocks: 3,
  embeddingDim: 64,
  numHeads: 8,
});

const savedState = localStorage.getItem("myModel");
// Or: const savedState = await Deno.readTextFile('model.json');

newModel.load(savedState!);

// Continue training or make predictions
const predictions = newModel.predict(5);
```

### What's Saved

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           SERIALIZED STATE              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… All model weights                    â”‚
â”‚ âœ… Adam optimizer state (moments)       â”‚
â”‚ âœ… Normalization statistics             â”‚
â”‚ âœ… Training progress (loss, count)      â”‚
â”‚ âœ… Convergence state                    â”‚
â”‚ âœ… ADWIN window                         â”‚
â”‚ âœ… Input history                        â”‚
â”‚ âœ… Configuration                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Performance Tips

### âœ… Do's

```typescript
// âœ… Reuse model instance for online learning
const model = new FusionTemporalTransformerRegression();
for (const data of stream) {
  model.fitOnline(data); // Incremental updates
}

// âœ… Use appropriate sequence lengths
{
  maxSequenceLength: Math.ceil(yourTypicalSequence * 1.2);
}

// âœ… Monitor drift detection
if (result.driftDetected) {
  console.log("Consider adjusting strategy");
}

// âœ… Check convergence
if (result.converged && result.loss < threshold) {
  // Model is stable, reduce learning rate or stop training
}

// âœ… Use uncertainty estimates
const pred = model.predict(1);
if (pred.predictions[0].standardError[0] > threshold) {
  console.log("High uncertainty - be cautious");
}
```

### âŒ Don'ts

```typescript
// âŒ Don't create new model for each sample
// BAD:
for (const data of stream) {
  const model = new FusionTemporalTransformerRegression();
  model.fitOnline(data); // Loses all learning!
}

// âŒ Don't use very long sequences without need
// BAD:
{
  maxSequenceLength: 10000;
} // Memory intensive

// âŒ Don't ignore dimension mismatches
// Model will throw error if dimensions change after init

// âŒ Don't use very high learning rates
// BAD:
{
  learningRate: 0.1;
} // May cause instability
```

---

## ğŸ“ Mathematical Foundations

<details>
<summary><b>Click to expand mathematical details</b></summary>

### Z-Score Normalization (Welford's Algorithm)

```
Online mean update:    Î¼â‚™ = Î¼â‚™â‚‹â‚ + (xâ‚™ - Î¼â‚™â‚‹â‚) / n
Online Mâ‚‚ update:      Mâ‚‚â‚™ = Mâ‚‚â‚™â‚‹â‚ + (xâ‚™ - Î¼â‚™â‚‹â‚)(xâ‚™ - Î¼â‚™)
Standard deviation:    Ïƒ = âˆš(Mâ‚‚ / (n-1))
Z-score:               z = (x - Î¼) / Ïƒ
```

### GELU Activation

```
GELU(x) = x Â· Î¦(x) â‰ˆ x Â· Ïƒ(1.702x)

where Î¦ is the CDF of standard normal
and Ïƒ is the sigmoid function
```

### Multi-Head Attention

```
MultiHead(Q, K, V) = Concat(headâ‚, ..., headâ‚•)Wá´¼

where headáµ¢ = Attention(QWáµ¢áµ , KWáµ¢á´·, VWáµ¢â±½)

Attention(Q, K, V) = softmax(QKáµ€/âˆšdâ‚– + TemporalBias)V
```

### Adam Optimizer

```
mâ‚œ = Î²â‚mâ‚œâ‚‹â‚ + (1 - Î²â‚)gâ‚œ           (First moment)
vâ‚œ = Î²â‚‚vâ‚œâ‚‹â‚ + (1 - Î²â‚‚)gâ‚œÂ²          (Second moment)
mÌ‚â‚œ = mâ‚œ / (1 - Î²â‚áµ—)                 (Bias correction)
vÌ‚â‚œ = vâ‚œ / (1 - Î²â‚‚áµ—)                 (Bias correction)
Î¸â‚œ = Î¸â‚œâ‚‹â‚ - Î· Â· mÌ‚â‚œ / (âˆšvÌ‚â‚œ + Îµ)     (Update)
```

### ADWIN Drift Detection

```
For window W split at cut point:
  |Î¼â‚€ - Î¼â‚| â‰¥ Îµcut âŸ¹ drift detected

where Îµcut = âˆš((1/2m)ln(4/Î´'))
      m = 1/(1/nâ‚€ + 1/nâ‚)
      Î´' = Î´/ln(n)
```

### Learning Rate Schedule

```
Warmup (t < T_warmup):
  Î·(t) = Î·_base Ã— (t + 1) / T_warmup

Cosine Decay (t â‰¥ T_warmup):
  progress = (t - T_warmup) / (T_total - T_warmup)
  Î·(t) = Î·_base Ã— 0.5 Ã— (1 + cos(Ï€ Ã— min(progress, 1)))
```

</details>

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues and pull requests
on [GitHub](https://github.com/hviana/multivariate-ft-transformer-regression).

---

## ğŸ“œ License

MIT License Â© 2025 [Henrique Emanoel Viana](https://github.com/hviana)

---

<div align="center">

**Made with â¤ï¸ for the time series community**

[â¬† Back to Top](#-multivariate-fusion-temporal-transformer-regression)

</div>
