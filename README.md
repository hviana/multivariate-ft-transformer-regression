Model: # ğŸ“š Multivariate FT transformer Regression

<div align="center">

# ğŸ§  FT-Transformer Regression

### _Feature Tokenizer Transformer for Multivariate Regression with Incremental Online Learning_

**Author:** Henrique Emanoel Viana

[ğŸ“¦ JSR Package](https://jsr.io/@hviana/multivariate-ft-transformer-regression)
â€¢ [ğŸ“‚ GitHub](https://github.com/hviana/multivariate-ft-transformer-regression)
â€¢ [ğŸ“„ License](./LICENSE)

</div>

---

## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [âš™ï¸ Configuration](#ï¸-configuration)
- [ğŸ“– API Reference](#-api-reference)
- [ğŸ¯ Optimization Guide](#-optimization-guide)
- [ğŸ’¡ Examples](#-examples)
- [ğŸ”§ Advanced Usage](#-advanced-usage)
- [ğŸ“Š Performance Tips](#-performance-tips)
- [ğŸ“œ License](#-license)

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ¯ Core Capabilities

| Feature                | Description                            |
| ---------------------- | -------------------------------------- |
| ğŸ”„ **Online Learning** | Incremental training on streaming data |
| ğŸ“Š **Multivariate**    | Multi-input, multi-output regression   |
| ğŸ§® **Auto-Detection**  | Automatic dimension detection          |
| ğŸ’¾ **Serialization**   | Full save/load model state             |

</td>
<td width="50%">

### ğŸ›¡ï¸ Advanced Features

| Feature                 | Description                         |
| ----------------------- | ----------------------------------- |
| ğŸ² **Drift Detection**  | ADWIN algorithm for concept drift   |
| ğŸ” **Outlier Handling** | Automatic outlier downweighting     |
| ğŸ“ˆ **Uncertainty**      | Confidence intervals on predictions |
| âš¡ **Memory Efficient** | Object pooling for GC optimization  |

</td>
</tr>
</table>

### ğŸ† Key Highlights

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚  âœ… Feature Tokenizer Transformer (FT-Transformer) Architecture         â”‚
â”‚  âœ… Multi-Head Self-Attention with Scaled Dot-Product                   â”‚
â”‚  âœ… GELU Activation in Feed-Forward Networks                            â”‚
â”‚  âœ… Layer Normalization with Learnable Parameters                       â”‚
â”‚  âœ… Adam Optimizer with Warmup & Cosine Decay Schedule                  â”‚
â”‚  âœ… Welford's Algorithm for Online Z-Score Normalization                â”‚
â”‚  âœ… ADWIN Algorithm for Concept Drift Detection                         â”‚
â”‚  âœ… L2 Regularization for Weight Decay                                  â”‚
â”‚  âœ… Xavier/Glorot Weight Initialization                                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### ğŸ“¦ Installation

```typescript
import { FTTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";
```

### ğŸ® Basic Usage

```typescript
// 1ï¸âƒ£ Create model instance
const model = new FTTransformerRegression();

// 2ï¸âƒ£ Train with online data
const result = model.fitOnline({
  xCoordinates: [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],
  yCoordinates: [[7.0, 8.0], [9.0, 10.0]],
});

console.log(`ğŸ“‰ Loss: ${result.loss.toFixed(6)}`);
console.log(`âœ… Converged: ${result.converged}`);

// 3ï¸âƒ£ Make predictions
const predictions = model.predict(3);
predictions.predictions.forEach((pred, i) => {
  console.log(`ğŸ¯ Step ${i + 1}: ${pred.predicted}`);
  console.log(`ğŸ“Š 95% CI: [${pred.lowerBound}, ${pred.upperBound}]`);
});

// 4ï¸âƒ£ Save model state
const state = model.save();
localStorage.setItem("my-model", state);

// 5ï¸âƒ£ Load model later
model.load(localStorage.getItem("my-model")!);
```

---

## ğŸ—ï¸ Architecture

### ğŸ“ FT-Transformer Overview

The FT-Transformer (Feature Tokenizer Transformer) adapts the Transformer
architecture for tabular/numerical regression tasks by tokenizing each input
feature individually.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FT-TRANSFORMER ARCHITECTURE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚    INPUT FEATURES              FEATURE TOKENIZER                            â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚    â”‚ xâ‚ xâ‚‚ ... xâ‚™â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚ eâ‚ eâ‚‚ ... eâ‚™    â”‚                          â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ (embeddings)     â”‚                          â”‚
â”‚                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â”‚                                        â”‚                                    â”‚
â”‚                                        â–¼                                    â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                          â”‚  [CLS] eâ‚ eâ‚‚ ... eâ‚™    â”‚                       â”‚
â”‚                          â”‚  (sequence with CLS)    â”‚                       â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                      â”‚                                      â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚              â”‚                       â–¼                       â”‚             â”‚
â”‚              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚             â”‚
â”‚              â”‚  â”‚          TRANSFORMER BLOCK Ã— N          â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”‚         Layer Norm 1              â”‚  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚                â”‚                         â”‚ â”‚             â”‚
â”‚              â”‚  â”‚                â–¼                         â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”‚   Multi-Head Self-Attention       â”‚  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”‚   Attention(Q,K,V) = softmax(     â”‚  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”‚     QK^T / âˆšd_k) V                â”‚  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚                â”‚                         â”‚ â”‚             â”‚
â”‚              â”‚  â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚ â”‚             â”‚
â”‚              â”‚  â”‚       â”‚  + Residual     â”‚               â”‚ â”‚             â”‚
â”‚              â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚ â”‚             â”‚
â”‚              â”‚  â”‚                â”‚                         â”‚ â”‚             â”‚
â”‚              â”‚  â”‚                â–¼                         â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”‚         Layer Norm 2              â”‚  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚                â”‚                         â”‚ â”‚             â”‚
â”‚              â”‚  â”‚                â–¼                         â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”‚     Feed-Forward Network          â”‚  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â”‚     FFN(x) = GELU(xWâ‚+bâ‚)Wâ‚‚+bâ‚‚   â”‚  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚             â”‚
â”‚              â”‚  â”‚                â”‚                         â”‚ â”‚             â”‚
â”‚              â”‚  â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚ â”‚             â”‚
â”‚              â”‚  â”‚       â”‚  + Residual     â”‚               â”‚ â”‚             â”‚
â”‚              â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚ â”‚             â”‚
â”‚              â”‚  â”‚                â”‚                         â”‚ â”‚             â”‚
â”‚              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚             â”‚
â”‚              â”‚                   â”‚                           â”‚             â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                  â”‚                                          â”‚
â”‚                                  â–¼                                          â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                     â”‚   Extract [CLS] Token  â”‚                             â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                 â”‚                                           â”‚
â”‚                                 â–¼                                           â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚                     â”‚    Output Projection   â”‚                             â”‚
â”‚                     â”‚    Å· = CLSÂ·W_out + b   â”‚                             â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚
â”‚                                 â”‚                                           â”‚
â”‚                                 â–¼                                           â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                        â”‚  PREDICTIONS â”‚                                    â”‚
â”‚                        â”‚  Å·â‚ Å·â‚‚ ... Å·â‚˜â”‚                                    â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”¤ Feature Tokenization

Unlike NLP Transformers that embed discrete tokens, the FT-Transformer creates
embeddings for continuous numerical features:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEATURE TOKENIZATION                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚   For each input feature xáµ¢:                                  â”‚
â”‚                                                               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  xáµ¢    â”‚ â”€â”€â”€â–¶ â”‚ xáµ¢ Â· Wáµ¢ â”‚ â”€â”€â”€â–¶ â”‚ eáµ¢ = xáµ¢ Â· Wáµ¢ + báµ¢ â”‚  â”‚
â”‚   â”‚ (scalar)â”‚      â”‚         â”‚      â”‚ (embedding vector)   â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                               â”‚
â”‚   Where:                                                      â”‚
â”‚   â€¢ Wáµ¢ âˆˆ â„^embeddingDim (learnable weights)                  â”‚
â”‚   â€¢ báµ¢ âˆˆ â„^embeddingDim (learnable biases)                   â”‚
â”‚   â€¢ eáµ¢ âˆˆ â„^embeddingDim (feature embedding)                  â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ·ï¸ CLS Token

The [CLS] (Classification/Aggregation) token is a learnable vector that:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLS TOKEN                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚   INPUT SEQUENCE:                                                 â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”                                â”‚
â”‚   â”‚[CLS]â”‚ eâ‚  â”‚ eâ‚‚  â”‚ ... â”‚ eâ‚™  â”‚                                â”‚
â”‚   â””â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚      â”‚                                                            â”‚
â”‚      â–¼                                                            â”‚
â”‚   After Transformer processing, [CLS] aggregates                  â”‚
â”‚   information from ALL features through attention                 â”‚
â”‚      â”‚                                                            â”‚
â”‚      â–¼                                                            â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚
â”‚   â”‚  [CLS]_final â†’ Predictions  â”‚                                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                â”‚
â”‚                                                                   â”‚
â”‚   Purpose:                                                        â”‚
â”‚   â€¢ Acts as a "global summary" of all features                   â”‚
â”‚   â€¢ Enables permutation-equivariant feature processing           â”‚
â”‚   â€¢ Final representation used for output prediction              â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¯ Multi-Head Self-Attention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTI-HEAD SELF-ATTENTION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   Input X âˆˆ â„^(seqLen Ã— embeddingDim)                              â”‚
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Q = X Â· Wq    K = X Â· Wk    V = X Â· Wv                     â”‚  â”‚
â”‚   â”‚  (Query)       (Key)         (Value)                        â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                      â”‚
â”‚                              â–¼                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  Split into h heads:                                        â”‚  â”‚
â”‚   â”‚                                                             â”‚  â”‚
â”‚   â”‚  head_i = Attention(Q_i, K_i, V_i)                         â”‚  â”‚
â”‚   â”‚                                                             â”‚  â”‚
â”‚   â”‚         = softmax(Q_i Â· K_i^T / âˆšd_k) Â· V_i                â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â”‚                                      â”‚
â”‚                              â–¼                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  MultiHead = Concat(head_1, ..., head_h) Â· W_o + b_o       â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚   Where: d_k = embeddingDim / numHeads (head dimension)            â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš¡ GELU Activation

The Gaussian Error Linear Unit (GELU) provides smooth, non-monotonic activation:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       GELU ACTIVATION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   GELU(x) = x Â· Î¦(x)                                            â”‚
â”‚                                                                  â”‚
â”‚   Approximation:                                                 â”‚
â”‚   GELU(x) â‰ˆ 0.5x(1 + tanh(âˆš(2/Ï€)(x + 0.044715xÂ³)))             â”‚
â”‚                                                                  â”‚
â”‚   Graph:                                                         â”‚
â”‚                                                                  â”‚
â”‚        â–² GELU(x)                                                â”‚
â”‚        â”‚                    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                         â”‚
â”‚        â”‚                 â•­â”€â”€â•¯                                   â”‚
â”‚        â”‚               â•­â”€â•¯                                      â”‚
â”‚        â”‚             â•­â”€â•¯                                        â”‚
â”‚   â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ x                        â”‚
â”‚        â”‚          â•­â”€â•¯                                           â”‚
â”‚        â”‚        â”€â”€â•¯                                             â”‚
â”‚        â”‚                                                         â”‚
â”‚                                                                  â”‚
â”‚   Properties:                                                    â”‚
â”‚   â€¢ Smooth: Differentiable everywhere                           â”‚
â”‚   â€¢ Non-monotonic: Small negative inputs â†’ small negative       â”‚
â”‚   â€¢ State-of-the-art: Used in BERT, GPT, Transformers           â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration

### ğŸ“ Configuration Parameters

```typescript
const model = new FTTransformerRegression({
  // Architecture
  numBlocks: 3, // Number of transformer blocks
  embeddingDim: 64, // Embedding dimension
  numHeads: 8, // Number of attention heads
  ffnMultiplier: 4, // FFN hidden dim = embeddingDim Ã— ffnMultiplier
  attentionDropout: 0.0, // Attention dropout rate

  // Optimizer
  learningRate: 0.001, // Base learning rate
  warmupSteps: 100, // Warmup period
  totalSteps: 10000, // Total steps for LR schedule
  beta1: 0.9, // Adam Î²â‚
  beta2: 0.999, // Adam Î²â‚‚
  epsilon: 1e-8, // Numerical stability

  // Regularization
  regularizationStrength: 1e-4, // L2 weight decay
  convergenceThreshold: 1e-6, // Convergence criterion
  outlierThreshold: 3.0, // Z-score for outliers
  adwinDelta: 0.002, // ADWIN confidence
});
```

### ğŸ“Š Parameter Reference Table

| Parameter                | Type     | Default | Range         | Description            |
| ------------------------ | -------- | ------- | ------------- | ---------------------- |
| `numBlocks`              | `number` | `3`     | 1-12          | Transformer depth      |
| `embeddingDim`           | `number` | `64`    | 16-512        | Feature embedding size |
| `numHeads`               | `number` | `8`     | 1-16          | Attention heads        |
| `ffnMultiplier`          | `number` | `4`     | 2-8           | FFN expansion ratio    |
| `attentionDropout`       | `number` | `0.0`   | 0.0-0.5       | Attention dropout      |
| `learningRate`           | `number` | `0.001` | 1e-5 to 0.1   | Base learning rate     |
| `warmupSteps`            | `number` | `100`   | 0-1000        | LR warmup period       |
| `totalSteps`             | `number` | `10000` | 1000-100000   | LR schedule length     |
| `beta1`                  | `number` | `0.9`   | 0.8-0.99      | Adam momentum          |
| `beta2`                  | `number` | `0.999` | 0.99-0.9999   | Adam RMSprop           |
| `epsilon`                | `number` | `1e-8`  | 1e-10 to 1e-6 | Numerical stability    |
| `regularizationStrength` | `number` | `1e-4`  | 0-0.01        | L2 regularization      |
| `convergenceThreshold`   | `number` | `1e-6`  | 1e-8 to 1e-4  | Convergence check      |
| `outlierThreshold`       | `number` | `3.0`   | 2.0-5.0       | Outlier z-score        |
| `adwinDelta`             | `number` | `0.002` | 0.0001-0.1    | Drift confidence       |

---

## ğŸ¯ Optimization Guide

### ğŸ”§ Parameter Optimization Strategies

#### 1ï¸âƒ£ Model Capacity (`numBlocks`, `embeddingDim`, `numHeads`)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MODEL CAPACITY TUNING                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   PROBLEM SIZE vs CAPACITY                                          â”‚
â”‚                                                                     â”‚
â”‚   Small datasets (< 1,000 samples):                                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚ numBlocks: 1-2                          â”‚                      â”‚
â”‚   â”‚ embeddingDim: 16-32                     â”‚                      â”‚
â”‚   â”‚ numHeads: 2-4                           â”‚                      â”‚
â”‚   â”‚ ffnMultiplier: 2                        â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                     â”‚
â”‚   Medium datasets (1,000 - 100,000 samples):                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚ numBlocks: 2-4                          â”‚                      â”‚
â”‚   â”‚ embeddingDim: 32-64                     â”‚                      â”‚
â”‚   â”‚ numHeads: 4-8                           â”‚                      â”‚
â”‚   â”‚ ffnMultiplier: 4                        â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                     â”‚
â”‚   Large datasets (> 100,000 samples):                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚   â”‚ numBlocks: 4-8                          â”‚                      â”‚
â”‚   â”‚ embeddingDim: 64-256                    â”‚                      â”‚
â”‚   â”‚ numHeads: 8-16                          â”‚                      â”‚
â”‚   â”‚ ffnMultiplier: 4-8                      â”‚                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                                                                     â”‚
â”‚   âš ï¸  CONSTRAINT: embeddingDim % numHeads === 0                    â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example Configuration:**

```typescript
// Small dataset / quick prototyping
const lightModel = new FTTransformerRegression({
  numBlocks: 2,
  embeddingDim: 32,
  numHeads: 4,
  ffnMultiplier: 2,
});

// Production model for complex patterns
const productionModel = new FTTransformerRegression({
  numBlocks: 4,
  embeddingDim: 128,
  numHeads: 8,
  ffnMultiplier: 4,
});
```

#### 2ï¸âƒ£ Learning Rate Optimization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEARNING RATE SCHEDULE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   Learning Rate = f(step)                                           â”‚
â”‚                                                                     â”‚
â”‚   â–² LR                                                              â”‚
â”‚   â”‚        warmup          cosine decay                            â”‚
â”‚   â”‚         â•±â•²                                                      â”‚
â”‚   â”‚        â•±  â•²                                                     â”‚
â”‚   â”‚       â•±    â•²â”€â”€â”€â”€â”€â”€â”€â”€â•®                                          â”‚
â”‚   â”‚      â•±              â•²                                          â”‚
â”‚   â”‚     â•±                â•²                                         â”‚
â”‚   â”‚    â•±                  â•²                                        â”‚
â”‚   â”‚   â•±                    â•²                                       â”‚
â”‚   â”‚â”€â”€â•±                      â•²â”€â”€â”€â”€â”€â”€                                â”‚
â”‚   â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ steps                   â”‚
â”‚      0    warmupSteps    totalSteps                                â”‚
â”‚                                                                     â”‚
â”‚   FORMULAS:                                                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                         â”‚
â”‚   Warmup phase (step < warmupSteps):                               â”‚
â”‚     lr = learningRate Ã— (step / warmupSteps)                       â”‚
â”‚                                                                     â”‚
â”‚   Decay phase (step â‰¥ warmupSteps):                                â”‚
â”‚     progress = (step - warmupSteps) / (totalSteps - warmupSteps)   â”‚
â”‚     lr = learningRate Ã— 0.5 Ã— (1 + cos(Ï€ Ã— progress))              â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Learning Rate Guidelines:**

| Scenario                 | `learningRate` | `warmupSteps` | `totalSteps` |
| ------------------------ | -------------- | ------------- | ------------ |
| Rapid convergence needed | 0.01           | 50            | 5000         |
| Standard training        | 0.001          | 100           | 10000        |
| Stable long training     | 0.0001         | 500           | 50000        |
| Fine-tuning              | 0.00001        | 200           | 20000        |

```typescript
// Fast training for simple patterns
const fastModel = new FTTransformerRegression({
  learningRate: 0.01,
  warmupSteps: 50,
  totalSteps: 5000,
});

// Stable training for noisy data
const stableModel = new FTTransformerRegression({
  learningRate: 0.0001,
  warmupSteps: 500,
  totalSteps: 50000,
});
```

#### 3ï¸âƒ£ Regularization Optimization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REGULARIZATION STRATEGY                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   L2 REGULARIZATION (Weight Decay)                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚                                                                     â”‚
â”‚   Loss = MSE_loss + (Î»/2) Ã— Î£||W||Â²                                â”‚
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚ regularization      â”‚ Use Case                     â”‚           â”‚
â”‚   â”‚ Strength            â”‚                              â”‚           â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”‚
â”‚   â”‚ 0                   â”‚ No regularization            â”‚           â”‚
â”‚   â”‚ 1e-5                â”‚ Very light (large datasets)  â”‚           â”‚
â”‚   â”‚ 1e-4 (default)      â”‚ Standard regularization      â”‚           â”‚
â”‚   â”‚ 1e-3                â”‚ Strong (small datasets)      â”‚           â”‚
â”‚   â”‚ 1e-2                â”‚ Very strong (overfitting)    â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                     â”‚
â”‚   OUTLIER HANDLING                                                  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚                                                                     â”‚
â”‚   Standardized residual: r = |y - Å·| / Ïƒ                           â”‚
â”‚   Outlier if: r > outlierThreshold                                 â”‚
â”‚   Outlier weight: 0.1 (downweighted contribution)                  â”‚
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚ outlierThreshold    â”‚ Interpretation               â”‚           â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”‚
â”‚   â”‚ 2.0                 â”‚ Aggressive (Â±2Ïƒ = 95%)       â”‚           â”‚
â”‚   â”‚ 3.0 (default)       â”‚ Standard (Â±3Ïƒ = 99.7%)       â”‚           â”‚
â”‚   â”‚ 4.0                 â”‚ Conservative (Â±4Ïƒ = 99.99%)  â”‚           â”‚
â”‚   â”‚ 5.0                 â”‚ Only extreme outliers        â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```typescript
// Noisy data with outliers
const robustModel = new FTTransformerRegression({
  regularizationStrength: 1e-3,
  outlierThreshold: 2.5,
});

// Clean data, maximize fit
const preciseModel = new FTTransformerRegression({
  regularizationStrength: 1e-5,
  outlierThreshold: 4.0,
});
```

#### 4ï¸âƒ£ Drift Detection (ADWIN)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ADWIN DRIFT DETECTION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   ADWIN (ADaptive WINdowing) detects concept drift by comparing     â”‚
â”‚   sub-windows of the loss history.                                  â”‚
â”‚                                                                     â”‚
â”‚   Window: [lossâ‚, lossâ‚‚, ..., lossâ‚™]                               â”‚
â”‚                                                                     â”‚
â”‚   For each split point i:                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚   â”‚    Window 0    â”‚    Window 1    â”‚                              â”‚
â”‚   â”‚  [lossâ‚...lossáµ¢]â”‚[lossáµ¢â‚Šâ‚...lossâ‚™]â”‚                            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                                     â”‚
â”‚   Drift detected if: |Î¼â‚€ - Î¼â‚| â‰¥ Îµ_cut                             â”‚
â”‚                                                                     â”‚
â”‚   Where: Îµ_cut = âˆš((1/2m) Ã— ln(4/Î´))                               â”‚
â”‚          m = harmonic mean of window sizes                         â”‚
â”‚          Î´ = adwinDelta (confidence parameter)                     â”‚
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚ adwinDelta          â”‚ Sensitivity                  â”‚           â”‚
â”‚   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤           â”‚
â”‚   â”‚ 0.1                 â”‚ Low (few false alarms)       â”‚           â”‚
â”‚   â”‚ 0.01                â”‚ Medium                       â”‚           â”‚
â”‚   â”‚ 0.002 (default)     â”‚ Standard sensitivity         â”‚           â”‚
â”‚   â”‚ 0.001               â”‚ High sensitivity             â”‚           â”‚
â”‚   â”‚ 0.0001              â”‚ Very high (many detections)  â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```typescript
// Streaming data with frequent distribution changes
const adaptiveModel = new FTTransformerRegression({
  adwinDelta: 0.001, // High sensitivity to changes
});

// Stable data, avoid false drift alarms
const stableDriftModel = new FTTransformerRegression({
  adwinDelta: 0.01, // Low sensitivity
});
```

---

## ğŸ“– API Reference

### ğŸ­ Constructor

```typescript
new FTTransformerRegression(options?: Partial<Config>)
```

Creates a new FT-Transformer regression model.

### ğŸ“š Methods

#### `fitOnline(data: FitInput): FitResult`

Performs incremental online learning with a batch of samples.

```typescript
interface FitInput {
  xCoordinates: number[][]; // [numSamples][inputDim]
  yCoordinates: number[][]; // [numSamples][outputDim]
}

interface FitResult {
  loss: number; // MSE + L2 regularization loss
  gradientNorm: number; // L2 norm of gradients
  effectiveLearningRate: number; // Current LR after schedule
  isOutlier: boolean; // Outlier detection flag
  converged: boolean; // Convergence status
  sampleIndex: number; // Total samples seen
  driftDetected: boolean; // ADWIN drift flag
}
```

**Example:**

```typescript
const result = model.fitOnline({
  xCoordinates: [[1, 2, 3], [4, 5, 6], [7, 8, 9]],
  yCoordinates: [[10, 11], [12, 13], [14, 15]],
});

if (result.driftDetected) {
  console.log("âš ï¸ Concept drift detected!");
}

if (result.converged) {
  console.log("âœ… Model has converged");
}
```

---

#### `predict(futureSteps: number): PredictionResult`

Generates predictions with uncertainty bounds.

```typescript
interface PredictionResult {
  predictions: SinglePrediction[];
  accuracy: number; // 1/(1 + avgLoss)
  sampleCount: number; // Training samples seen
  isModelReady: boolean; // Has been trained
}

interface SinglePrediction {
  predicted: number[]; // Point predictions
  lowerBound: number[]; // Lower 95% CI
  upperBound: number[]; // Upper 95% CI
  standardError: number[]; // Standard errors
}
```

**Example:**

```typescript
const result = model.predict(5);

console.log(`ğŸ“Š Model Accuracy: ${(result.accuracy * 100).toFixed(2)}%`);

result.predictions.forEach((pred, step) => {
  console.log(`\nğŸ¯ Step ${step + 1}:`);
  pred.predicted.forEach((val, dim) => {
    console.log(
      `   Dim ${dim}: ${val.toFixed(4)} Â± ${
        pred.standardError[dim].toFixed(4)
      }`,
    );
    console.log(
      `   95% CI: [${pred.lowerBound[dim].toFixed(4)}, ${
        pred.upperBound[dim].toFixed(4)
      }]`,
    );
  });
});
```

---

#### `getModelSummary(): ModelSummary`

Returns comprehensive model information.

```typescript
interface ModelSummary {
  isInitialized: boolean;
  inputDimension: number;
  outputDimension: number;
  numBlocks: number;
  embeddingDim: number;
  numHeads: number;
  totalParameters: number;
  sampleCount: number;
  accuracy: number;
  converged: boolean;
  effectiveLearningRate: number;
  driftCount: number;
}
```

**Example:**

```typescript
const summary = model.getModelSummary();

console.log(`
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MODEL SUMMARY               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Initialized: ${summary.isInitialized}
â”‚ Input Dim:   ${summary.inputDimension}
â”‚ Output Dim:  ${summary.outputDimension}
â”‚ Parameters:  ${summary.totalParameters.toLocaleString()}
â”‚ Samples:     ${summary.sampleCount.toLocaleString()}
â”‚ Accuracy:    ${(summary.accuracy * 100).toFixed(2)}%
â”‚ Converged:   ${summary.converged}
â”‚ Drift Count: ${summary.driftCount}
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
`);
```

---

#### `getWeights(): WeightInfo`

Returns all model weights for inspection.

```typescript
interface WeightInfo {
  featureEmbeddings: number[][][]; // [inputDim][2][embeddingDim]
  clsToken: number[]; // [embeddingDim]
  attentionWeights: number[][][]; // [numBlocks][4][...]
  ffnWeights: number[][][]; // [numBlocks][4][...]
  layerNormParams: number[][][]; // [numBlocks][4][...]
  outputWeights: number[][]; // [2][...]
  firstMoment: number[][][]; // Adam M
  secondMoment: number[][][]; // Adam V
  updateCount: number; // Adam step count
}
```

---

#### `getNormalizationStats(): NormalizationStats`

Returns running statistics for normalization.

```typescript
interface NormalizationStats {
  inputMean: number[]; // Running mean of inputs
  inputStd: number[]; // Running std of inputs
  outputMean: number[]; // Running mean of outputs
  outputStd: number[]; // Running std of outputs
  count: number; // Sample count
}
```

**Example:**

```typescript
const stats = model.getNormalizationStats();

console.log("ğŸ“ˆ Input Statistics:");
stats.inputMean.forEach((mean, i) => {
  console.log(
    `   Feature ${i}: Î¼=${mean.toFixed(4)}, Ïƒ=${stats.inputStd[i].toFixed(4)}`,
  );
});
```

---

#### `reset(): void`

Resets model to uninitialized state.

```typescript
model.reset();
// Model is now ready for fresh training
```

---

#### `save(): string`

Serializes complete model state to JSON.

```typescript
const state = model.save();

// Save to storage
localStorage.setItem("ft-transformer-model", state);
await Deno.writeTextFile("model.json", state);
```

---

#### `load(jsonString: string): void`

Restores model from serialized state.

```typescript
// Load from storage
const state = localStorage.getItem("ft-transformer-model");
if (state) {
  model.load(state);
}

// Load from file
const fileState = await Deno.readTextFile("model.json");
model.load(fileState);
```

---

## ğŸ’¡ Examples

### ğŸ“ˆ Time Series Forecasting

```typescript
import { FTTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

// Create model optimized for time series
const model = new FTTransformerRegression({
  numBlocks: 3,
  embeddingDim: 64,
  numHeads: 8,
  learningRate: 0.001,
  warmupSteps: 100,
});

// Simulate time series data
const windowSize = 10;
const data: number[] = [];

// Generate synthetic data: sine wave with noise
for (let i = 0; i < 1000; i++) {
  data.push(Math.sin(i * 0.1) + Math.random() * 0.1);
}

// Create training windows
for (let i = 0; i < data.length - windowSize - 1; i++) {
  const x = data.slice(i, i + windowSize);
  const y = [data[i + windowSize]];

  const result = model.fitOnline({
    xCoordinates: [x],
    yCoordinates: [y],
  });

  if (i % 100 === 0) {
    console.log(
      `Step ${i}: Loss=${result.loss.toFixed(6)}, LR=${
        result.effectiveLearningRate.toFixed(6)
      }`,
    );
  }
}

// Forecast next 5 values
const forecast = model.predict(5);
console.log("\nğŸ”® Forecast:");
forecast.predictions.forEach((pred, i) => {
  console.log(
    `  t+${i + 1}: ${pred.predicted[0].toFixed(4)} [${
      pred.lowerBound[0].toFixed(4)
    }, ${pred.upperBound[0].toFixed(4)}]`,
  );
});
```

### ğŸ  Multi-Output Regression

```typescript
import { FTTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

// House price prediction with multiple outputs
const model = new FTTransformerRegression({
  numBlocks: 4,
  embeddingDim: 128,
  numHeads: 8,
  regularizationStrength: 1e-4,
});

// Features: [sqft, bedrooms, bathrooms, age, distance_to_city]
// Outputs: [price, days_on_market]

const trainingData = [
  { x: [1500, 3, 2, 10, 5], y: [300000, 30] },
  { x: [2000, 4, 2.5, 5, 3], y: [450000, 20] },
  { x: [1200, 2, 1, 30, 10], y: [200000, 45] },
  // ... more data
];

// Train incrementally
for (const sample of trainingData) {
  const result = model.fitOnline({
    xCoordinates: [sample.x],
    yCoordinates: [sample.y],
  });

  if (result.isOutlier) {
    console.log("âš ï¸ Outlier detected in training data");
  }
}

// Predict for new house
const prediction = model.predict(1);
const [price, daysOnMarket] = prediction.predictions[0].predicted;

console.log(`\nğŸ  Prediction:`);
console.log(`   Price: $${price.toLocaleString()}`);
console.log(`   Days on Market: ${Math.round(daysOnMarket)}`);
```

### ğŸ”„ Online Learning with Drift Detection

```typescript
import { FTTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

const model = new FTTransformerRegression({
  numBlocks: 3,
  embeddingDim: 64,
  adwinDelta: 0.002, // Standard drift sensitivity
  learningRate: 0.001,
});

// Simulate streaming data with concept drift
async function processStream(
  dataStream: AsyncIterable<{ x: number[]; y: number[] }>,
) {
  let totalDrifts = 0;

  for await (const sample of dataStream) {
    const result = model.fitOnline({
      xCoordinates: [sample.x],
      yCoordinates: [sample.y],
    });

    if (result.driftDetected) {
      totalDrifts++;
      console.log(
        `ğŸ”„ Drift #${totalDrifts} detected at sample ${result.sampleIndex}`,
      );

      // Optional: Adjust learning rate after drift
      // The model automatically adapts via ADWIN window shrinking
    }

    // Periodic prediction
    if (result.sampleIndex % 100 === 0) {
      const pred = model.predict(1);
      console.log(
        `Sample ${result.sampleIndex}: Accuracy=${
          (pred.accuracy * 100).toFixed(2)
        }%`,
      );
    }
  }

  const summary = model.getModelSummary();
  console.log(`\nğŸ“Š Final Statistics:`);
  console.log(`   Total samples: ${summary.sampleCount}`);
  console.log(`   Total drifts: ${summary.driftCount}`);
  console.log(`   Final accuracy: ${(summary.accuracy * 100).toFixed(2)}%`);
}
```

### ğŸ’¾ Model Persistence

```typescript
import { FTTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

// Training phase
const model = new FTTransformerRegression();

// ... train model ...

// Save model
const savedState = model.save();

// Option 1: Save to file (Deno)
await Deno.writeTextFile("./model_checkpoint.json", savedState);

// Option 2: Save to localStorage (Browser)
localStorage.setItem("ft_transformer_model", savedState);

// Option 3: Save to database
await db.saveModel("my_model", savedState);

// Later: Load model
const newModel = new FTTransformerRegression();

// Load from file
const loadedState = await Deno.readTextFile("./model_checkpoint.json");
newModel.load(loadedState);

// Continue training or predict
const predictions = newModel.predict(5);
console.log("Model loaded successfully, making predictions...");
```

---

## ğŸ”§ Advanced Usage

### ğŸ›ï¸ Custom Training Loop

```typescript
import { FTTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

class ModelTrainer {
  private model: FTTransformerRegression;
  private bestLoss: number = Infinity;
  private bestState: string = "";
  private patience: number;
  private patienceCounter: number = 0;

  constructor(config: Partial<Config>, patience: number = 100) {
    this.model = new FTTransformerRegression(config);
    this.patience = patience;
  }

  train(data: { x: number[][]; y: number[][] }, epochs: number = 1) {
    for (let epoch = 0; epoch < epochs; epoch++) {
      let epochLoss = 0;

      for (let i = 0; i < data.x.length; i++) {
        const result = this.model.fitOnline({
          xCoordinates: [data.x[i]],
          yCoordinates: [data.y[i]],
        });

        epochLoss += result.loss;

        // Early stopping check
        if (result.loss < this.bestLoss) {
          this.bestLoss = result.loss;
          this.bestState = this.model.save();
          this.patienceCounter = 0;
        } else {
          this.patienceCounter++;
        }

        if (this.patienceCounter >= this.patience) {
          console.log(`â¹ï¸ Early stopping at epoch ${epoch}, sample ${i}`);
          this.model.load(this.bestState); // Restore best model
          return;
        }
      }

      const avgLoss = epochLoss / data.x.length;
      console.log(`Epoch ${epoch + 1}: Avg Loss = ${avgLoss.toFixed(6)}`);
    }
  }

  predict(steps: number) {
    return this.model.predict(steps);
  }

  getSummary() {
    return this.model.getModelSummary();
  }
}
```

### ğŸ“Š Model Monitoring Dashboard

```typescript
import { FTTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

function printDashboard(model: FTTransformerRegression) {
  const summary = model.getModelSummary();
  const stats = model.getNormalizationStats();

  console.clear();
  console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FT-TRANSFORMER DASHBOARD                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  STATUS                                                           â•‘
â•‘  â”œâ”€ Initialized: ${
    summary.isInitialized ? "âœ…" : "âŒ"
  }                                        â•‘
â•‘  â”œâ”€ Converged:   ${
    summary.converged ? "âœ…" : "ğŸ”„"
  }                                        â•‘
â•‘  â””â”€ Drifts:      ${summary.driftCount.toString().padEnd(40)}â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ARCHITECTURE                                                     â•‘
â•‘  â”œâ”€ Input Dim:   ${summary.inputDimension.toString().padEnd(40)}â•‘
â•‘  â”œâ”€ Output Dim:  ${summary.outputDimension.toString().padEnd(40)}â•‘
â•‘  â”œâ”€ Blocks:      ${summary.numBlocks.toString().padEnd(40)}â•‘
â•‘  â”œâ”€ Embed Dim:   ${summary.embeddingDim.toString().padEnd(40)}â•‘
â•‘  â”œâ”€ Heads:       ${summary.numHeads.toString().padEnd(40)}â•‘
â•‘  â””â”€ Parameters:  ${summary.totalParameters.toLocaleString().padEnd(40)}â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  TRAINING                                                         â•‘
â•‘  â”œâ”€ Samples:     ${summary.sampleCount.toLocaleString().padEnd(40)}â•‘
â•‘  â”œâ”€ Accuracy:    ${(summary.accuracy * 100).toFixed(2)}%${" ".repeat(36)}â•‘
â•‘  â””â”€ Current LR:  ${summary.effectiveLearningRate.toExponential(4).padEnd(40)}â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  NORMALIZATION (Sample Count: ${stats.count.toLocaleString().padEnd(26)}â•‘
â•‘  Input Means:  [${
    stats.inputMean.slice(0, 3).map((m) => m.toFixed(2)).join(", ")
  }...]${" ".repeat(30)}â•‘
â•‘  Input Stds:   [${
    stats.inputStd.slice(0, 3).map((s) => s.toFixed(2)).join(", ")
  }...]${" ".repeat(30)}â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  `);
}
```

---

## ğŸ“Š Performance Tips

### âš¡ Memory Optimization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MEMORY OPTIMIZATION TIPS                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  1. BATCH SIZE                                                      â”‚
â”‚     â€¢ Process data in small batches (1-32 samples)                 â”‚
â”‚     â€¢ Larger batches use more memory but train faster              â”‚
â”‚                                                                     â”‚
â”‚  2. MODEL SIZE                                                      â”‚
â”‚     â€¢ Reduce embeddingDim for memory-constrained environments      â”‚
â”‚     â€¢ Fewer numBlocks = less memory                                â”‚
â”‚                                                                     â”‚
â”‚  3. BUILT-IN OPTIMIZATION                                          â”‚
â”‚     â€¢ ArrayPool automatically reuses Float64Arrays                 â”‚
â”‚     â€¢ Minimizes garbage collection pressure                        â”‚
â”‚                                                                     â”‚
â”‚  4. SAVE/LOAD STRATEGY                                             â”‚
â”‚     â€¢ Save checkpoints periodically                                â”‚
â”‚     â€¢ Load only when needed to avoid memory duplication            â”‚
â”‚                                                                     â”‚
â”‚  Parameter Count Formula:                                           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  params â‰ˆ inputDim Ã— ed Ã— 2           (feature embeddings)         â”‚
â”‚         + ed                          (CLS token)                   â”‚
â”‚         + numBlocks Ã— (4 Ã— edÂ² + 4 Ã— ed Ã— ffnDim + ...)           â”‚
â”‚         + ed Ã— outputDim + outputDim  (output layer)               â”‚
â”‚                                                                     â”‚
â”‚  Where: ed = embeddingDim, ffnDim = ed Ã— ffnMultiplier             â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸš€ Speed Optimization

| Tip                             | Impact    | Description                          |
| ------------------------------- | --------- | ------------------------------------ |
| Reduce `numBlocks`              | ğŸŸ¢ High   | Linear reduction in compute          |
| Reduce `embeddingDim`           | ğŸŸ¢ High   | Quadratic reduction in attention     |
| Reduce `numHeads`               | ğŸŸ¡ Medium | Less parallel attention              |
| Increase `convergenceThreshold` | ğŸŸ¡ Medium | Earlier stopping                     |
| Pre-normalize data              | ğŸŸ¡ Medium | Skip internal normalization overhead |

### ğŸ“ˆ Accuracy Optimization

| Tip                          | Impact    | Description             |
| ---------------------------- | --------- | ----------------------- |
| Increase `numBlocks`         | ğŸŸ¢ High   | Deeper feature learning |
| Increase `embeddingDim`      | ğŸŸ¢ High   | Richer representations  |
| Lower `learningRate`         | ğŸŸ¡ Medium | More stable convergence |
| Lower `convergenceThreshold` | ğŸŸ¡ Medium | Tighter convergence     |
| Tune `outlierThreshold`      | ğŸŸ¡ Medium | Better outlier handling |
| Lower `adwinDelta`           | ğŸŸ¢ High   | Better drift adaptation |

---

## ğŸ§ª Algorithm Details

### ğŸ“ Welford's Online Statistics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WELFORD'S ALGORITHM                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Computes running mean and variance in a single pass:               â”‚
â”‚                                                                     â”‚
â”‚  Initialize:                                                        â”‚
â”‚    Î¼ = 0, Mâ‚‚ = 0, n = 0                                            â”‚
â”‚                                                                     â”‚
â”‚  For each new value x:                                              â”‚
â”‚    n  â† n + 1                                                       â”‚
â”‚    Î´  â† x - Î¼                                                       â”‚
â”‚    Î¼  â† Î¼ + Î´/n                                                     â”‚
â”‚    Î´â‚‚ â† x - Î¼                                                       â”‚
â”‚    Mâ‚‚ â† Mâ‚‚ + Î´ Ã— Î´â‚‚                                                â”‚
â”‚                                                                     â”‚
â”‚  Variance: ÏƒÂ² = Mâ‚‚/(n-1)                                           â”‚
â”‚  Std Dev:  Ïƒ  = âˆš(ÏƒÂ² + Îµ)                                          â”‚
â”‚                                                                     â”‚
â”‚  Benefits:                                                          â”‚
â”‚  â€¢ Numerically stable                                               â”‚
â”‚  â€¢ Single-pass (O(1) per update)                                   â”‚
â”‚  â€¢ No need to store all data                                        â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”„ Adam Optimizer

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ADAM OPTIMIZER                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  Adam: Adaptive Moment Estimation                                   â”‚
â”‚                                                                     â”‚
â”‚  For each parameter Î¸ with gradient g:                              â”‚
â”‚                                                                     â”‚
â”‚  m â† Î²â‚ Ã— m + (1 - Î²â‚) Ã— g        (1st moment: momentum)           â”‚
â”‚  v â† Î²â‚‚ Ã— v + (1 - Î²â‚‚) Ã— gÂ²       (2nd moment: RMSprop)            â”‚
â”‚                                                                     â”‚
â”‚  mÌ‚ â† m / (1 - Î²â‚áµ—)                (bias correction)                â”‚
â”‚  vÌ‚ â† v / (1 - Î²â‚‚áµ—)                (bias correction)                â”‚
â”‚                                                                     â”‚
â”‚  Î¸ â† Î¸ - Î· Ã— mÌ‚ / (âˆšvÌ‚ + Îµ)         (parameter update)               â”‚
â”‚                                                                     â”‚
â”‚  Default hyperparameters:                                           â”‚
â”‚  â€¢ Î²â‚ = 0.9    (momentum decay)                                    â”‚
â”‚  â€¢ Î²â‚‚ = 0.999  (RMSprop decay)                                     â”‚
â”‚  â€¢ Îµ  = 1e-8   (numerical stability)                               â”‚
â”‚  â€¢ Î·  = 0.001  (learning rate)                                     â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“œ License

```
MIT License

Copyright (c) 2025 Henrique Emanoel Viana

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

<div align="center">

### ğŸŒŸ Star this repository if you find it useful!

**[ğŸ“¦ JSR Package](https://jsr.io/@hviana/multivariate-ft-transformer-regression)**
â€¢
**[ğŸ“‚ GitHub Repository](https://github.com/hviana/multivariate-ft-transformer-regression)**

Made with â¤ï¸ by [Henrique Emanoel Viana](https://github.com/hviana)

</div>
