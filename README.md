ğŸ§  Fusion Temporal Transformer Regression

<div align="center">

**A powerful multivariate regression library with incremental online learning
capabilities**

[ğŸ“¦ JSR Package](https://jsr.io/@hviana/multivariate-ft-transformer-regression)
â€¢ [ğŸ™ GitHub](https://github.com/hviana/multivariate-ft-transformer-regression)
â€¢ [ğŸ‘¤ Author: Henrique Emanoel Viana](https://github.com/hviana)

</div>

---

## ğŸ“‘ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture Overview](#ï¸-architecture-overview)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“Š Core Concepts](#-core-concepts)
  - [Multi-Scale Temporal Convolution](#-multi-scale-temporal-convolution)
  - [Gated Cross-Scale Fusion](#-gated-cross-scale-fusion)
  - [Transformer Blocks](#-transformer-blocks)
  - [Attention-Weighted Pooling](#-attention-weighted-pooling)
  - [Online Learning & Normalization](#-online-learning--normalization)
  - [Drift Detection](#-drift-detection)
- [âš™ï¸ Configuration Parameters](#ï¸-configuration-parameters)
- [ğŸ¯ Parameter Optimization Guide](#-parameter-optimization-guide)
- [ğŸ“˜ API Reference](#-api-reference)
- [ğŸ’¡ Use Case Examples](#-use-case-examples)
- [ğŸ”§ Advanced Usage](#-advanced-usage)
- [ğŸ“ˆ Performance Notes](#-performance-notes)

---

## âœ¨ Features

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸŒŸ FUSION TEMPORAL TRANSFORMER ğŸŒŸ                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  ğŸ”„ ONLINE LEARNING          â”‚  ğŸ“Š MULTI-SCALE ANALYSIS                     â”‚
â”‚  â”œâ”€ Incremental updates      â”‚  â”œâ”€ Multiple temporal resolutions           â”‚
â”‚  â”œâ”€ No batch requirement     â”‚  â”œâ”€ Gated cross-scale fusion                â”‚
â”‚  â””â”€ Real-time adaptation     â”‚  â””â”€ Adaptive scale importance               â”‚
â”‚                              â”‚                                              â”‚
â”‚  ğŸ§® TRANSFORMER ARCHITECTURE â”‚  ğŸ“‰ ROBUST TRAINING                          â”‚
â”‚  â”œâ”€ Multi-head attention     â”‚  â”œâ”€ Welford z-score normalization           â”‚
â”‚  â”œâ”€ Pre-LN design            â”‚  â”œâ”€ Outlier downweighting                   â”‚
â”‚  â””â”€ GELU activation          â”‚  â””â”€ ADWIN drift detection                   â”‚
â”‚                              â”‚                                              â”‚
â”‚  âš¡ OPTIMIZED PERFORMANCE    â”‚  ğŸ›ï¸ FLEXIBLE CONFIGURATION                  â”‚
â”‚  â”œâ”€ Float64Array buffers     â”‚  â”œâ”€ Builder pattern                         â”‚
â”‚  â”œâ”€ Preallocated memory      â”‚  â”œâ”€ Extensive customization                 â”‚
â”‚  â””â”€ In-place operations      â”‚  â””â”€ Save/Load functionality                 â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Highlights

| Feature                              | Description                                            |
| ------------------------------------ | ------------------------------------------------------ |
| ğŸ”„ **Incremental Online Learning**   | Train with single samples - perfect for streaming data |
| ğŸŒŠ **Multi-Scale Temporal Analysis** | Captures patterns at different time resolutions        |
| ğŸšª **Gated Fusion Mechanism**        | Intelligently combines information across scales       |
| ğŸ¤– **Full Transformer Stack**        | LayerNorm â†’ MHA â†’ Residual â†’ FFN architecture          |
| ğŸ“Š **Attention-Weighted Pooling**    | Learns which timesteps matter most                     |
| ğŸ“ˆ **Adam with Scheduling**          | Warmup + cosine decay learning rate                    |
| ğŸ›¡ï¸ **Robust Normalization**          | Welford online z-score for stability                   |
| âš ï¸ **Outlier Handling**              | Automatic downweighting of anomalous samples           |
| ğŸ” **Drift Detection**               | ADWIN-like algorithm for concept drift                 |
| ğŸ’¾ **Serialization**                 | Complete save/load with Base64 encoding                |

---

## ğŸ—ï¸ Architecture Overview

```
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚         INPUT SEQUENCE               â”‚
                              â”‚    X = [xâ‚, xâ‚‚, ..., xâ‚œ] âˆˆ â„áµ€Ë£á´°â±â¿   â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚      WELFORD Z-SCORE NORMALIZATION   â”‚
                              â”‚         (Online Mean/Std)            â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                      â”‚                                      â”‚
          â–¼                                      â–¼                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   TEMPORAL CONV     â”‚              â”‚   TEMPORAL CONV     â”‚              â”‚   TEMPORAL CONV     â”‚
â”‚   Scale = 1         â”‚              â”‚   Scale = 2         â”‚              â”‚   Scale = 4         â”‚
â”‚   (stride=1)        â”‚              â”‚   (stride=2)        â”‚              â”‚   (stride=4)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   + Positional Enc  â”‚              â”‚   + Positional Enc  â”‚              â”‚   + Positional Enc  â”‚
â”‚   + Scale Embedding â”‚              â”‚   + Scale Embedding â”‚              â”‚   + Scale Embedding â”‚
â”‚   + GELU            â”‚              â”‚   + GELU            â”‚              â”‚   + GELU            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                    â”‚                                    â”‚
          â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
          â”‚         â”‚                          â”‚                            â”‚       â”‚
          â–¼         â–¼                          â–¼                            â–¼       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                               NEAREST NEIGHBOR RESAMPLING                                   â”‚
â”‚                            (Align all scales to finest resolution)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚        GATED CROSS-SCALE FUSION      â”‚
                              â”‚                                      â”‚
                              â”‚  concat = [Eâ‚, Eâ‚‚, ..., Eâ‚›]          â”‚
                              â”‚  gates = Ïƒ(concat Â· Wg + bg)         â”‚
                              â”‚  fused = Î£â‚› gateâ‚› Â· Eâ‚›               â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚   FUSION DROPOUT        â”‚
                                    â”‚   (optional)            â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
                    â•‘              TRANSFORMER BLOCK Ã— N                      â•‘
                    â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
                    â•‘  â”‚                                                    â”‚ â•‘
                    â•‘  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚ â•‘
                    â•‘  â”‚   â”‚  LayerNorm 1 â”‚                                 â”‚ â•‘
                    â•‘  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚ â•‘
                    â•‘  â”‚          â”‚                                         â”‚ â•‘
                    â•‘  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚ â•‘
                    â•‘  â”‚   â”‚  Multi-Head  â”‚  â† Causal/Sliding Window Mask  â”‚ â•‘
                    â•‘  â”‚   â”‚  Attention   â”‚  â† Attention Dropout           â”‚ â•‘
                    â•‘  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚ â•‘
                    â•‘  â”‚          â”‚                                         â”‚ â•‘
                    â•‘  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚ â•‘
                    â•‘  â”‚   â”‚  + Residual  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â•‘
                    â•‘  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚   â”‚ â•‘
                    â•‘  â”‚          â”‚                                     â”‚   â”‚ â•‘
                    â•‘  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                             â”‚   â”‚ â•‘
                    â•‘  â”‚   â”‚  LayerNorm 2 â”‚                             â”‚   â”‚ â•‘
                    â•‘  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚   â”‚ â•‘
                    â•‘  â”‚          â”‚                                     â”‚   â”‚ â•‘
                    â•‘  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                             â”‚   â”‚ â•‘
                    â•‘  â”‚   â”‚     FFN      â”‚  Linear â†’ GELU â†’ Linear     â”‚   â”‚ â•‘
                    â•‘  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚   â”‚ â•‘
                    â•‘  â”‚          â”‚                                     â”‚   â”‚ â•‘
                    â•‘  â”‚   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                             â”‚   â”‚ â•‘
                    â•‘  â”‚   â”‚  + Residual  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â•‘
                    â•‘  â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚ â•‘
                    â•‘  â”‚          â”‚                                         â”‚ â•‘
                    â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
                    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                  â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  ATTENTION-WEIGHTED POOLING â”‚
                   â”‚                             â”‚
                   â”‚  logits = H Â· Wpool + bpool â”‚
                   â”‚  Î± = softmax(logits)        â”‚
                   â”‚  pooled = Î£â‚œ Î±â‚œ Â· Hâ‚œ        â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚       OUTPUT LAYER          â”‚
                   â”‚   Å· = pooled Â· Wout + bout  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   DENORMALIZE PREDICTION    â”‚
                   â”‚   (Reverse z-score)         â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                                  â–¼
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   OUTPUT Å·    â”‚
                          â”‚   âˆˆ â„á´°áµ’áµ˜áµ—     â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Installation

```typescript
import { FusionTemporalTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";
```

### Basic Usage

```typescript
// 1ï¸âƒ£ Create a model instance
const model = new FusionTemporalTransformerRegression();

// 2ï¸âƒ£ Train incrementally with streaming data
const result = model.fitOnline({
  xCoordinates: [
    [1.0, 2.0, 3.0], // timestep 1: 3 features
    [2.0, 3.0, 4.0], // timestep 2: 3 features
    [3.0, 4.0, 5.0], // timestep 3: 3 features
  ],
  yCoordinates: [[0.5, 0.8]], // target: 2 output dimensions
});

console.log(`Loss: ${result.loss.toFixed(4)}`);
console.log(`Converged: ${result.converged}`);

// 3ï¸âƒ£ Make predictions
const prediction = model.predict(3); // predict 3 future steps
console.log(prediction.predictions[0].predicted);

// 4ï¸âƒ£ Save and load
const saved = model.save();
const newModel = new FusionTemporalTransformerRegression();
newModel.load(saved);
```

---

## ğŸ“Š Core Concepts

### ğŸŒŠ Multi-Scale Temporal Convolution

The library captures temporal patterns at multiple resolutions simultaneously
using strided 1D convolutions.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTI-SCALE CONVOLUTION                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  Input Sequence: [tâ‚, tâ‚‚, tâ‚ƒ, tâ‚„, tâ‚…, tâ‚†, tâ‚‡, tâ‚ˆ]             â”‚
â”‚                                                                â”‚
â”‚  Scale 1 (stride=1): [â—, â—, â—, â—, â—, â—, â—, â—]  â†’ 8 outputs    â”‚
â”‚                       Fine-grained patterns                    â”‚
â”‚                                                                â”‚
â”‚  Scale 2 (stride=2): [â—,    â—,    â—,    â—   ]  â†’ 4 outputs    â”‚
â”‚                       Medium patterns                          â”‚
â”‚                                                                â”‚
â”‚  Scale 4 (stride=4): [â—,          â—         ]  â†’ 2 outputs    â”‚
â”‚                       Coarse patterns                          â”‚
â”‚                                                                â”‚
â”‚  Each scale receives:                                          â”‚
â”‚  â€¢ Positional encoding (sinusoidal)                            â”‚
â”‚  â€¢ Scale-specific learned embedding                            â”‚
â”‚  â€¢ GELU activation                                             â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Parameters:**

- `temporalScales`: Array of stride values `[1, 2, 4]`
- `temporalKernelSize`: Convolution kernel size (default: 3)
- `embeddingDim`: Output dimension per scale

### ğŸšª Gated Cross-Scale Fusion

Intelligently combines information from all temporal scales using learned gating
mechanisms.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GATED FUSION MECHANISM                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  For each position t:                                                â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚  â”‚ Scale 1 â”‚   â”‚ Scale 2 â”‚   â”‚ Scale 3 â”‚                            â”‚
â”‚  â”‚   Eâ‚    â”‚   â”‚   Eâ‚‚    â”‚   â”‚   Eâ‚ƒ    â”‚                            â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                            â”‚
â”‚       â”‚             â”‚             â”‚                                  â”‚
â”‚       â–¼             â–¼             â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚        Concatenate: [Eâ‚, Eâ‚‚, Eâ‚ƒ]     â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                     â”‚                                                â”‚
â”‚                     â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚   Gate Logits = concat Â· Wg + bg     â”‚                           â”‚
â”‚  â”‚   Gates = Ïƒ(logits)  â† per scale     â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                     â”‚                                                â”‚
â”‚                     â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚  â”‚   Fused = gâ‚Â·Eâ‚ + gâ‚‚Â·Eâ‚‚ + gâ‚ƒÂ·Eâ‚ƒ     â”‚                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚                                                                      â”‚
â”‚  Benefits:                                                           â”‚
â”‚  âœ“ Learns which scales are important for each position              â”‚
â”‚  âœ“ Adapts to data characteristics automatically                     â”‚
â”‚  âœ“ Soft attention over temporal resolutions                         â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ¤– Transformer Blocks

Each transformer block follows the Pre-LayerNorm architecture for improved
training stability.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFORMER BLOCK DETAIL                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚                    Input: H âˆˆ â„á´¸Ë£á´°                                   â”‚
â”‚                           â”‚                                          â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚                    â”‚  LayerNorm  â”‚  Î³ âŠ™ (x-Î¼)/Ïƒ + Î²                  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                           â”‚                                          â”‚
â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚      â”‚                    â”‚                    â”‚                     â”‚
â”‚      â–¼                    â–¼                    â–¼                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚ Q=XWq â”‚           â”‚ K=XWk â”‚           â”‚ V=XWv â”‚                   â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜           â””â”€â”€â”€â”¬â”€â”€â”€â”˜           â””â”€â”€â”€â”¬â”€â”€â”€â”˜                   â”‚
â”‚      â”‚                   â”‚                   â”‚                       â”‚
â”‚      â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                       â”‚
â”‚      â”‚    â”‚  Split into H heads          â”‚   â”‚                       â”‚
â”‚      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                       â”‚
â”‚      â”‚                   â”‚                   â”‚                       â”‚
â”‚      â–¼                   â–¼                   â–¼                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚            Scaled Dot-Product Attention       â”‚                   â”‚
â”‚  â”‚                                               â”‚                   â”‚
â”‚  â”‚  Attention(Q,K,V) = softmax(QKáµ€/âˆšdâ‚– + M) Â· V â”‚                   â”‚
â”‚  â”‚                                               â”‚                   â”‚
â”‚  â”‚  M = Causal Mask + Sliding Window Mask        â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                          â”‚                                           â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚                   â”‚  Concat +   â”‚                                    â”‚
â”‚                   â”‚  Project Wo â”‚                                    â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                          â”‚                                           â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚           â”‚              â”‚            â”‚                              â”‚
â”‚           â”‚       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”     â”‚                              â”‚
â”‚           â”‚       â”‚  + Residual â”‚â—„â”€â”€â”€â”€â”˜                              â”‚
â”‚           â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚           â”‚              â”‚                                           â”‚
â”‚           â”‚       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚           â”‚       â”‚  LayerNorm  â”‚                                    â”‚
â”‚           â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚           â”‚              â”‚                                           â”‚
â”‚           â”‚       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚           â”‚       â”‚     FFN     â”‚                                    â”‚
â”‚           â”‚       â”‚ W2Â·GELU(W1x)â”‚                                    â”‚
â”‚           â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚           â”‚              â”‚                                           â”‚
â”‚           â”‚       â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â–ºâ”‚  + Residual â”‚                                    â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                          â”‚                                           â”‚
â”‚                    Output: H' âˆˆ â„á´¸Ë£á´°                                 â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**GELU Activation:**

```
GELU(x) â‰ˆ 0.5x[1 + tanh(âˆš(2/Ï€)(x + 0.044715xÂ³))]
```

### ğŸ¯ Attention-Weighted Pooling

Learns which timesteps are most relevant for the final prediction.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ATTENTION-WEIGHTED POOLING                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Transformer Output: H = [hâ‚, hâ‚‚, hâ‚ƒ, ..., hâ‚œ]                       â”‚
â”‚                                                                      â”‚
â”‚  Step 1: Compute attention logits                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚  logits[t] = hâ‚œ Â· Wpool + bpool         â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                      â”‚
â”‚  Step 2: Softmax to get attention weights                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚  Î± = softmax(logits)                    â”‚                         â”‚
â”‚  â”‚                                         â”‚                         â”‚
â”‚  â”‚  Î± = [0.05, 0.10, 0.15, 0.25, 0.45]    â”‚  â† Example weights       â”‚
â”‚  â”‚       â†‘     â†‘     â†‘     â†‘     â†‘        â”‚                         â”‚
â”‚  â”‚      tâ‚    tâ‚‚    tâ‚ƒ    tâ‚„    tâ‚…        â”‚  Recent = more importantâ”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                      â”‚
â”‚  Step 3: Weighted sum                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚  pooled = Î£â‚œ Î±â‚œ Â· hâ‚œ                    â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                      â”‚
â”‚  Result: Single vector representing the entire sequence              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“ˆ Online Learning & Normalization

Uses Welford's algorithm for numerically stable online statistics computation.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WELFORD ONLINE NORMALIZATION                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  For each new sample x:                                              â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚  n = n + 1                              â”‚                         â”‚
â”‚  â”‚  Î´ = x - Î¼                              â”‚                         â”‚
â”‚  â”‚  Î¼ = Î¼ + Î´/n           â† Running mean   â”‚                         â”‚
â”‚  â”‚  Î´â‚‚ = x - Î¼                             â”‚                         â”‚
â”‚  â”‚  Mâ‚‚ = Mâ‚‚ + Î´ Â· Î´â‚‚      â† Running var    â”‚                         â”‚
â”‚  â”‚  Ïƒ = âˆš(Mâ‚‚/(n-1))                        â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                                      â”‚
â”‚  Benefits:                                                           â”‚
â”‚  âœ“ Single-pass algorithm                                             â”‚
â”‚  âœ“ Numerically stable                                                â”‚
â”‚  âœ“ O(1) memory per feature                                           â”‚
â”‚  âœ“ No need to store all historical data                              â”‚
â”‚                                                                      â”‚
â”‚  Applied to:                                                         â”‚
â”‚  â€¢ Input features (X normalization)                                  â”‚
â”‚  â€¢ Output targets (Y normalization)                                  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ” Drift Detection

ADWIN-like algorithm detects when the underlying data distribution changes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ADWIN DRIFT DETECTION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Error Stream: [eâ‚, eâ‚‚, eâ‚ƒ, ..., eâ‚™]                                 â”‚
â”‚                                                                      â”‚
â”‚  Algorithm:                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  For each split point k in window:                          â”‚     â”‚
â”‚  â”‚                                                             â”‚     â”‚
â”‚  â”‚    Wâ‚€ = [eâ‚, ..., eâ‚–]      Wâ‚ = [eâ‚–â‚Šâ‚, ..., eâ‚™]           â”‚     â”‚
â”‚  â”‚    Î¼â‚€ = mean(Wâ‚€)           Î¼â‚ = mean(Wâ‚)                   â”‚     â”‚
â”‚  â”‚                                                             â”‚     â”‚
â”‚  â”‚    If |Î¼â‚€ - Î¼â‚| â‰¥ Îµ_cut:  â† Hoeffding bound                â”‚     â”‚
â”‚  â”‚       DRIFT DETECTED!                                       â”‚     â”‚
â”‚  â”‚       Drop older window Wâ‚€                                  â”‚     â”‚
â”‚  â”‚       Reset optimizer moments                               â”‚     â”‚
â”‚  â”‚                                                             â”‚     â”‚
â”‚  â”‚    Îµ_cut = âˆš(0.5 Â· ln(4/Î´) Â· (1/nâ‚€ + 1/nâ‚))               â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                      â”‚
â”‚  Visualization:                                                      â”‚
â”‚                                                                      â”‚
â”‚  Error                                                               â”‚
â”‚    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚    â”‚     â”‚    Stable      â”‚  â•±â•²  Drift!                              â”‚
â”‚    â”‚  â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â•±â”€â”€â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚    â”‚                       â•±    â•²  New distribution                  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Time                   â”‚
â”‚                                                                      â”‚
â”‚  Actions on drift:                                                   â”‚
â”‚  â€¢ Reset Adam optimizer moments                                      â”‚
â”‚  â€¢ Clear error window                                                â”‚
â”‚  â€¢ Maintain model weights (warm restart)                             â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration Parameters

### Complete Parameter Reference

```typescript
interface FusionTemporalTransformerRegressionConfig {
  // ğŸ—ï¸ Architecture
  numBlocks: number; // Number of transformer blocks
  embeddingDim: number; // Embedding/hidden dimension
  numHeads: number; // Number of attention heads
  ffnMultiplier: number; // FFN hidden = embeddingDim Ã— ffnMultiplier

  // ğŸ­ Dropout
  attentionDropout: number; // Dropout after attention
  fusionDropout: number; // Dropout after fusion

  // ğŸ“‰ Optimizer
  learningRate: number; // Base learning rate
  warmupSteps: number; // LR warmup steps
  totalSteps: number; // Total steps for cosine decay
  beta1: number; // Adam Î²â‚
  beta2: number; // Adam Î²â‚‚
  epsilon: number; // Numerical stability

  // ğŸ›¡ï¸ Regularization
  regularizationStrength: number; // L2 weight decay
  convergenceThreshold: number; // Gradient norm threshold

  // âš ï¸ Robustness
  outlierThreshold: number; // Standardized residual threshold
  adwinDelta: number; // Drift detection sensitivity

  // ğŸŒŠ Temporal
  temporalScales: number[]; // Multi-scale strides
  temporalKernelSize: number; // Conv kernel size
  maxSequenceLength: number; // Max input length

  // ğŸ­ Attention Masking
  causalMask: boolean; // Use causal attention
  slidingWindow: number; // Window size (0 = full)
}
```

### Default Configuration

```typescript
const DEFAULT_CONFIG = {
  // Architecture
  numBlocks: 3,
  embeddingDim: 64,
  numHeads: 8,
  ffnMultiplier: 4,

  // Dropout
  attentionDropout: 0.0,
  fusionDropout: 0.0,

  // Optimizer
  learningRate: 0.001,
  warmupSteps: 100,
  totalSteps: 10000,
  beta1: 0.9,
  beta2: 0.999,
  epsilon: 1e-8,

  // Regularization
  regularizationStrength: 1e-4,
  convergenceThreshold: 1e-6,

  // Robustness
  outlierThreshold: 3.0,
  adwinDelta: 0.002,

  // Temporal
  temporalScales: [1, 2, 4],
  temporalKernelSize: 3,
  maxSequenceLength: 512,

  // Attention
  causalMask: true,
  slidingWindow: 0,
};
```

---

## ğŸ¯ Parameter Optimization Guide

### ğŸ“Š By Data Characteristics

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PARAMETER OPTIMIZATION GUIDE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  ğŸ“ˆ SMALL DATASET (< 1,000 samples)                                         â”‚
â”‚  â”œâ”€ embeddingDim: 32-64                                                     â”‚
â”‚  â”œâ”€ numBlocks: 1-2                                                          â”‚
â”‚  â”œâ”€ regularizationStrength: 1e-3 to 1e-2                                    â”‚
â”‚  â”œâ”€ attentionDropout: 0.1-0.2                                               â”‚
â”‚  â””â”€ warmupSteps: 20-50                                                      â”‚
â”‚                                                                             â”‚
â”‚  ğŸ“Š MEDIUM DATASET (1,000 - 100,000 samples)                                â”‚
â”‚  â”œâ”€ embeddingDim: 64-128                                                    â”‚
â”‚  â”œâ”€ numBlocks: 2-4                                                          â”‚
â”‚  â”œâ”€ regularizationStrength: 1e-4                                            â”‚
â”‚  â”œâ”€ attentionDropout: 0.0-0.1                                               â”‚
â”‚  â””â”€ warmupSteps: 100-500                                                    â”‚
â”‚                                                                             â”‚
â”‚  ğŸ“ˆ LARGE DATASET (> 100,000 samples)                                       â”‚
â”‚  â”œâ”€ embeddingDim: 128-256                                                   â”‚
â”‚  â”œâ”€ numBlocks: 4-6                                                          â”‚
â”‚  â”œâ”€ regularizationStrength: 1e-5 to 1e-4                                    â”‚
â”‚  â”œâ”€ attentionDropout: 0.0                                                   â”‚
â”‚  â””â”€ warmupSteps: 500-1000                                                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸŒŠ By Temporal Complexity

| Pattern Type        | `temporalScales` | `temporalKernelSize` | `numBlocks` |
| ------------------- | ---------------- | -------------------- | ----------- |
| **Short-term only** | `[1]`            | 3                    | 1-2         |
| **Short + Medium**  | `[1, 2]`         | 3-5                  | 2-3         |
| **Multi-scale**     | `[1, 2, 4]`      | 3-5                  | 3-4         |
| **Long-range**      | `[1, 2, 4, 8]`   | 5-7                  | 4-6         |
| **Very long-range** | `[1, 4, 16, 64]` | 7-9                  | 4-6         |

### âš¡ By Latency Requirements

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LATENCY OPTIMIZATION                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  ğŸƒ ULTRA-LOW LATENCY (< 1ms)                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  embeddingDim: 16-32                                            â”‚ â”‚
â”‚  â”‚  numBlocks: 1                                                   â”‚ â”‚
â”‚  â”‚  numHeads: 2-4                                                  â”‚ â”‚
â”‚  â”‚  temporalScales: [1]                                            â”‚ â”‚
â”‚  â”‚  maxSequenceLength: 32-64                                       â”‚ â”‚
â”‚  â”‚  ffnMultiplier: 2                                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                      â”‚
â”‚  âš–ï¸ BALANCED (1-10ms)                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  embeddingDim: 64                                               â”‚ â”‚
â”‚  â”‚  numBlocks: 2-3                                                 â”‚ â”‚
â”‚  â”‚  numHeads: 8                                                    â”‚ â”‚
â”‚  â”‚  temporalScales: [1, 2, 4]                                      â”‚ â”‚
â”‚  â”‚  maxSequenceLength: 128-256                                     â”‚ â”‚
â”‚  â”‚  ffnMultiplier: 4                                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                      â”‚
â”‚  ğŸ¯ HIGH ACCURACY (> 10ms acceptable)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  embeddingDim: 128-256                                          â”‚ â”‚
â”‚  â”‚  numBlocks: 4-6                                                 â”‚ â”‚
â”‚  â”‚  numHeads: 8-16                                                 â”‚ â”‚
â”‚  â”‚  temporalScales: [1, 2, 4, 8]                                   â”‚ â”‚
â”‚  â”‚  maxSequenceLength: 512                                         â”‚ â”‚
â”‚  â”‚  ffnMultiplier: 4                                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“ Learning Rate Schedule Visualization

```
Learning Rate
      â”‚
  lr  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ â•±              â•²
      â”‚â•±   Warmup       â•² Cosine Decay
      â”‚                  â•²
      â”‚                   â•²
      â”‚                    â•²
   0  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Steps
      0   warmup    total/2         total

Formula:
- Warmup: lr Ã— (t / warmupSteps)
- Decay:  lr Ã— 0.5 Ã— (1 + cos(Ï€ Ã— (t - warmup) / (total - warmup)))
```

---

## ğŸ“˜ API Reference

### Constructor

```typescript
const model = new FusionTemporalTransformerRegression(config?: Partial<Config>);
```

### Methods

#### `fitOnline(data)`

Train the model with a single sample.

```typescript
interface FitInput {
  xCoordinates: number[][]; // [timesteps, features]
  yCoordinates: number[][]; // [1, outputs] or [timesteps, outputs]
}

interface FitResult {
  loss: number; // Current sample loss
  gradientNorm: number; // L2 norm of gradients
  effectiveLearningRate: number; // Current LR after scheduling
  isOutlier: boolean; // Was this sample downweighted?
  converged: boolean; // Has training converged?
  sampleIndex: number; // Total samples seen
  driftDetected: boolean; // Was drift detected?
}

const result: FitResult = model.fitOnline(data);
```

#### `predict(futureSteps)`

Generate predictions for future timesteps.

```typescript
interface SinglePrediction {
  predicted: number[]; // Point prediction
  lowerBound: number[]; // Lower confidence bound
  upperBound: number[]; // Upper confidence bound
  standardError: number[]; // Standard error per output
}

interface PredictionResult {
  predictions: SinglePrediction[]; // One per future step
  accuracy: number; // 1 / (1 + runningLoss)
  sampleCount: number; // Training samples seen
  isModelReady: boolean; // Has warmup completed?
}

const result: PredictionResult = model.predict(3);
```

#### `getModelSummary()`

```typescript
interface ModelSummary {
  isInitialized: boolean;
  inputDimension: number;
  outputDimension: number;
  numBlocks: number;
  embeddingDim: number;
  numHeads: number;
  temporalScales: number[];
  totalParameters: number;
  sampleCount: number;
  accuracy: number;
  converged: boolean;
  effectiveLearningRate: number;
  driftCount: number;
}

const summary: ModelSummary = model.getModelSummary();
```

#### `getNormalizationStats()`

```typescript
interface NormalizationStats {
  inputMean: number[];
  inputStd: number[];
  outputMean: number[];
  outputStd: number[];
  count: number;
}

const stats: NormalizationStats = model.getNormalizationStats();
```

#### `getWeights()`

```typescript
interface WeightInfo {
  temporalConvWeights: number[][][];
  scaleEmbeddings: number[][];
  positionalEncoding: number[][];
  fusionWeights: number[][][];
  attentionWeights: number[][][];
  ffnWeights: number[][][];
  layerNormParams: number[][][];
  outputWeights: number[][];
  firstMoment: number[][][]; // Adam m
  secondMoment: number[][][]; // Adam v
  updateCount: number;
}

const weights: WeightInfo = model.getWeights();
```

#### `save()` / `load()`

```typescript
// Save model state
const json: string = model.save();

// Load into new model
const newModel = new FusionTemporalTransformerRegression();
newModel.load(json);
```

#### `reset()`

```typescript
model.reset(); // Clear all state, return to uninitialized
```

---

## ğŸ’¡ Use Case Examples

### ğŸ“ˆ Stock Price Prediction

```typescript
import { 
  FusionTemporalTransformerRegression,
  FusionTemporalTransformerRegressionBuilder 
} from "jsr:@hviana/multivariate-ft-transformer-regression";

// Configure for financial time series
const config = new FusionTemporalTransformerRegressionBuilder()
  .setEmbeddingDim(128)
  .setNumBlocks(4)
  .setNumHeads(8)
  .setTemporalScales([1, 2, 4, 8])     // Multiple time horizons
  .setTemporalKernelSize(5)
  .setMaxSequenceLength(252)           // ~1 year of trading days
  .setLearningRate(0.0005)
  .setWarmupSteps(500)
  .setTotalSteps(50000)
  .setRegularizationStrength(1e-4)
  .setOutlierThreshold(2.5)            // More aggressive outlier detection
  .setAdwinDelta(0.001)                // Sensitive to market regime changes
  .setMasking(true, 0)                 // Causal attention
  .build();

const model = new FusionTemporalTransformerRegression(config);

// Training loop with streaming data
async function trainOnStream(dataStream: AsyncIterable<{prices: number[], volume: number}>) {
  const window: number[][] = [];
  const windowSize = 60;  // 60 days lookback
  
  for await (const tick of dataStream) {
    // Build feature vector: [open, high, low, close, volume, returns]
    const features = [
      tick.prices[0], tick.prices[1], tick.prices[2], tick.prices[3],
      tick.volume,
      (tick.prices[3] - tick.prices[0]) / tick.prices[0]  // Daily return
    ];
    
    window.push(features);
    if (window.length > windowSize) window.shift();
    
    if (window.length === windowSize) {
      // Target: next day's close price
      const nextClose = /* get next day's close */;
      
      const result = model.fitOnline({
        xCoordinates: window,
        yCoordinates: [[nextClose]]
      });
      
      if (result.driftDetected) {
        console.log("âš ï¸ Market regime change detected!");
      }
      
      // Make prediction
      const pred = model.predict(5);  // 5-day forecast
      console.log(`Next 5 days: ${pred.predictions.map(p => p.predicted[0].toFixed(2))}`);
    }
  }
}
```

### ğŸŒ¡ï¸ Sensor Anomaly Detection with Multi-Output

```typescript
const config = new FusionTemporalTransformerRegressionBuilder()
  .setEmbeddingDim(64)
  .setNumBlocks(2)
  .setNumHeads(4)
  .setTemporalScales([1, 4, 16]) // Multi-scale for periodic patterns
  .setMaxSequenceLength(128)
  .setLearningRate(0.001)
  .setOutlierThreshold(4.0) // Allow more variance in sensor data
  .setAdwinDelta(0.005)
  .build();

const model = new FusionTemporalTransformerRegression(config);

// Multi-sensor input, multi-output prediction
const sensorData = {
  xCoordinates: [
    [23.5, 45.2, 1013.2, 2.3], // temp, humidity, pressure, wind
    [23.8, 44.8, 1013.5, 2.1],
    [24.1, 44.5, 1013.8, 1.9],
    // ... more timesteps
  ],
  yCoordinates: [[24.5, 44.0, 1014.0, 1.8]], // Predict all sensors
};

const result = model.fitOnline(sensorData);

// Check if current reading is anomalous
if (result.isOutlier) {
  console.log("ğŸš¨ Anomalous sensor reading detected!");
}
```

### âš¡ Real-Time Energy Demand Forecasting

```typescript
const config = new FusionTemporalTransformerRegressionBuilder()
  .setEmbeddingDim(96)
  .setNumBlocks(3)
  .setNumHeads(6)
  .setTemporalScales([1, 2, 4, 8, 24]) // Hourly patterns + daily
  .setTemporalKernelSize(7)
  .setMaxSequenceLength(168) // 1 week hourly
  .setLearningRate(0.0008)
  .setWarmupSteps(200)
  .setAttentionDropout(0.05)
  .setFusionDropout(0.05)
  .setMasking(true, 48) // Sliding window: 2 days
  .build();

const model = new FusionTemporalTransformerRegression(config);

// Features: [demand, temp, day_sin, day_cos, week_sin, week_cos]
function processHour(data: HourlyData): number[] {
  const hourAngle = (data.hour / 24) * 2 * Math.PI;
  const dayAngle = (data.dayOfWeek / 7) * 2 * Math.PI;

  return [
    data.demandMW,
    data.temperature,
    Math.sin(hourAngle),
    Math.cos(hourAngle),
    Math.sin(dayAngle),
    Math.cos(dayAngle),
  ];
}

// Predict next 24 hours
const forecast = model.predict(24);
forecast.predictions.forEach((pred, hour) => {
  console.log(
    `Hour +${hour + 1}: ${pred.predicted[0].toFixed(1)} MW ` +
      `[${pred.lowerBound[0].toFixed(1)}, ${pred.upperBound[0].toFixed(1)}]`,
  );
});
```

### ğŸ­ Manufacturing Quality Control

```typescript
// High-frequency sensor data from production line
const config = new FusionTemporalTransformerRegressionBuilder()
  .setEmbeddingDim(48)
  .setNumBlocks(2)
  .setNumHeads(4)
  .setTemporalScales([1, 2, 4])
  .setTemporalKernelSize(3)
  .setMaxSequenceLength(64)
  .setLearningRate(0.002) // Faster adaptation
  .setWarmupSteps(50)
  .setTotalSteps(5000)
  .setOutlierThreshold(2.5)
  .setAdwinDelta(0.01) // Quick drift response
  .setMasking(false, 0) // Full attention for quality correlation
  .build();

const model = new FusionTemporalTransformerRegression(config);

// Continuous monitoring loop
function monitorProduction(sensors: Float64Array[]) {
  const result = model.fitOnline({
    xCoordinates: sensors.map((s) => Array.from(s)),
    yCoordinates: [[targetQualityMetric]],
  });

  const summary = model.getModelSummary();

  return {
    qualityPrediction: model.predict(1).predictions[0].predicted[0],
    isAnomaly: result.isOutlier,
    confidence: summary.accuracy,
    calibrationSamples: summary.sampleCount,
    processChanged: result.driftDetected,
  };
}
```

---

## ğŸ”§ Advanced Usage

### Builder Pattern

```typescript
import { FusionTemporalTransformerRegressionBuilder } from "jsr:@hviana/multivariate-ft-transformer-regression";

const config = new FusionTemporalTransformerRegressionBuilder()
  // Architecture
  .setNumBlocks(4)
  .setEmbeddingDim(128)
  .setNumHeads(8)
  .setFfnMultiplier(4)
  // Temporal settings
  .setTemporalScales([1, 2, 4, 8])
  .setTemporalKernelSize(5)
  .setMaxSequenceLength(256)
  // Training
  .setLearningRate(0.0005)
  .setWarmupSteps(500)
  .setTotalSteps(50000)
  .setBetas(0.9, 0.999)
  .setEpsilon(1e-8)
  // Regularization
  .setRegularizationStrength(1e-4)
  .setAttentionDropout(0.1)
  .setFusionDropout(0.05)
  // Robustness
  .setOutlierThreshold(3.0)
  .setAdwinDelta(0.002)
  .setConvergenceThreshold(1e-6)
  // Attention masking
  .setMasking(true, 64) // Causal + sliding window
  .build();

const model = new FusionTemporalTransformerRegression(config);
```

### Model Checkpointing

```typescript
class ModelCheckpointer {
  private checkpoints: Map<number, string> = new Map();
  private bestLoss = Infinity;
  private bestCheckpoint: string | null = null;

  checkpoint(model: FusionTemporalTransformerRegression, step: number) {
    const state = model.save();
    const summary = model.getModelSummary();

    // Keep periodic checkpoints
    if (step % 1000 === 0) {
      this.checkpoints.set(step, state);
    }

    // Track best model
    const loss = 1 / summary.accuracy - 1;
    if (loss < this.bestLoss) {
      this.bestLoss = loss;
      this.bestCheckpoint = state;
    }
  }

  loadBest(model: FusionTemporalTransformerRegression) {
    if (this.bestCheckpoint) {
      model.load(this.bestCheckpoint);
    }
  }

  rollback(model: FusionTemporalTransformerRegression, toStep: number) {
    const state = this.checkpoints.get(toStep);
    if (state) {
      model.load(state);
    }
  }
}
```

### Ensemble of Models

```typescript
class EnsembleRegressor {
  private models: FusionTemporalTransformerRegression[];

  constructor(configs: Partial<FusionTemporalTransformerRegressionConfig>[]) {
    this.models = configs.map((c) =>
      new FusionTemporalTransformerRegression(c)
    );
  }

  fitOnline(data: { xCoordinates: number[][]; yCoordinates: number[][] }) {
    return this.models.map((m) => m.fitOnline(data));
  }

  predict(steps: number): PredictionResult {
    const predictions = this.models.map((m) => m.predict(steps));

    // Aggregate predictions
    const numOutputs = predictions[0].predictions[0].predicted.length;
    const aggregated: SinglePrediction[] = [];

    for (let s = 0; s < steps; s++) {
      const predicted: number[] = new Array(numOutputs).fill(0);
      const variance: number[] = new Array(numOutputs).fill(0);

      // Mean prediction
      for (const pred of predictions) {
        for (let j = 0; j < numOutputs; j++) {
          predicted[j] += pred.predictions[s].predicted[j] / this.models.length;
        }
      }

      // Ensemble variance
      for (const pred of predictions) {
        for (let j = 0; j < numOutputs; j++) {
          const diff = pred.predictions[s].predicted[j] - predicted[j];
          variance[j] += diff * diff / this.models.length;
        }
      }

      const stdErr = variance.map((v) => Math.sqrt(v));
      aggregated.push({
        predicted,
        standardError: stdErr,
        lowerBound: predicted.map((p, j) => p - 1.96 * stdErr[j]),
        upperBound: predicted.map((p, j) => p + 1.96 * stdErr[j]),
      });
    }

    return {
      predictions: aggregated,
      accuracy: predictions.reduce((s, p) => s + p.accuracy, 0) /
        this.models.length,
      sampleCount: predictions[0].sampleCount,
      isModelReady: predictions.every((p) => p.isModelReady),
    };
  }
}

// Usage: Diverse ensemble
const ensemble = new EnsembleRegressor([
  { numBlocks: 2, temporalScales: [1, 2] },
  { numBlocks: 3, temporalScales: [1, 2, 4] },
  { numBlocks: 4, temporalScales: [1, 4, 8] },
]);
```

### Custom Learning Rate Schedules

```typescript
// Implement custom LR scheduling by wrapping fitOnline
class AdaptiveLRModel {
  private model: FusionTemporalTransformerRegression;
  private baseLR: number;
  private patience: number;
  private factor: number;
  private minLR: number;

  private bestLoss = Infinity;
  private stepsWithoutImprovement = 0;
  private currentLR: number;

  constructor(baseLR = 0.001, patience = 1000, factor = 0.5, minLR = 1e-6) {
    this.baseLR = baseLR;
    this.currentLR = baseLR;
    this.patience = patience;
    this.factor = factor;
    this.minLR = minLR;

    this.model = new FusionTemporalTransformerRegression({
      learningRate: baseLR,
      warmupSteps: 0,
      totalSteps: 1, // Disable built-in scheduling
    });
  }

  fitOnline(data: { xCoordinates: number[][]; yCoordinates: number[][] }) {
    const result = this.model.fitOnline(data);

    // Reduce on plateau
    if (result.loss < this.bestLoss) {
      this.bestLoss = result.loss;
      this.stepsWithoutImprovement = 0;
    } else {
      this.stepsWithoutImprovement++;

      if (this.stepsWithoutImprovement >= this.patience) {
        this.currentLR = Math.max(this.currentLR * this.factor, this.minLR);
        this.stepsWithoutImprovement = 0;
        console.log(`ğŸ“‰ Reducing LR to ${this.currentLR}`);
      }
    }

    return result;
  }
}
```

---

## ğŸ“ˆ Performance Notes

### Memory Optimization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MEMORY EFFICIENCY                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  âœ“ Float64Array for all numerical operations                        â”‚
â”‚    â€¢ Direct memory access                                            â”‚
â”‚    â€¢ No JavaScript object overhead                                   â”‚
â”‚    â€¢ Better cache locality                                           â”‚
â”‚                                                                      â”‚
â”‚  âœ“ Preallocated buffers                                              â”‚
â”‚    â€¢ All attention matrices allocated once                           â”‚
â”‚    â€¢ Gradient buffers reused across iterations                       â”‚
â”‚    â€¢ No GC pressure during training                                  â”‚
â”‚                                                                      â”‚
â”‚  âœ“ In-place operations                                               â”‚
â”‚    â€¢ Softmax computed in-place                                       â”‚
â”‚    â€¢ Activation gradients overwrite forward buffers                  â”‚
â”‚    â€¢ Minimal temporary allocations                                   â”‚
â”‚                                                                      â”‚
â”‚  Memory Footprint Estimation:                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Parameters: ~numBlocks Ã— (8Ã—DÂ² + 8Ã—DÃ—H + 6Ã—D)                  â”‚ â”‚
â”‚  â”‚  Activations: ~maxSeqLen Ã— D Ã— (numBlocks Ã— 6 + S Ã— 2)          â”‚ â”‚
â”‚  â”‚  Attention: ~numHeads Ã— maxSeqLenÂ² per block                    â”‚ â”‚
â”‚  â”‚                                                                 â”‚ â”‚
â”‚  â”‚  Example (D=64, blocks=3, maxSeq=512, heads=8):                 â”‚ â”‚
â”‚  â”‚  â€¢ Parameters: ~200KB                                           â”‚ â”‚
â”‚  â”‚  â€¢ Activations: ~1.5MB                                          â”‚ â”‚
â”‚  â”‚  â€¢ Attention: ~24MB (dominates for long sequences)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Computational Complexity

| Operation      | Time Complexity  | Notes                              |
| -------------- | ---------------- | ---------------------------------- |
| Temporal Conv  | O(L Ã— K Ã— D Ã— F) | L=seq_len, K=kernel, F=input_dim   |
| Self-Attention | O(LÂ² Ã— D)        | Dominated by LÂ² for long sequences |
| FFN            | O(L Ã— D Ã— H)     | H = ffnMultiplier Ã— D              |
| Pooling        | O(L Ã— D)         | Linear in sequence length          |
| Adam Update    | O(P)             | P = total parameters               |

### Tips for Speed

```typescript
// 1. Use smaller sequences when possible
const fastConfig = {
  maxSequenceLength: 64, // vs 512
  embeddingDim: 32, // vs 64
  numBlocks: 2, // vs 3
  numHeads: 4, // vs 8
};

// 2. Disable dropout for inference speed
const inferenceConfig = {
  attentionDropout: 0.0,
  fusionDropout: 0.0,
};

// 3. Use sliding window attention for long sequences
const longSeqConfig = {
  slidingWindow: 128, // Only attend to last 128 positions
  causalMask: true,
};

// 4. Reduce temporal scales for simpler patterns
const simpleConfig = {
  temporalScales: [1], // Single scale
};
```

---

## ğŸ“„ License

MIT License Â© 2025 Henrique Emanoel Viana

---

<div align="center">

**Made with â¤ï¸ for the time series community**

[Report Bug](https://github.com/hviana/multivariate-ft-transformer-regression/issues)
â€¢
[Request Feature](https://github.com/hviana/multivariate-ft-transformer-regression/issues)
â€¢
[Contribute](https://github.com/hviana/multivariate-ft-transformer-regression/pulls)

</div>
