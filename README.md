ğŸ“Š Fusion Temporal Transformer Regression

<div align="center">

**A powerful multivariate time-series regression library with incremental online
learning capabilities**

[ğŸ“¦ JSR Package](https://jsr.io/@hviana/multivariate-ft-transformer-regression)
â€¢ [ğŸ™ GitHub](https://github.com/hviana/multivariate-ft-transformer-regression)
â€¢ [ğŸ“š Documentation](#-api-reference)

</div>

---

## ğŸ“‹ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸ“¦ Installation](#-installation)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“– API Reference](#-api-reference)
- [âš™ï¸ Configuration Guide](#ï¸-configuration-guide)
- [ğŸ’¡ Examples](#-examples)
- [ğŸ¯ Best Practices](#-best-practices)
- [ğŸ”§ Troubleshooting](#-troubleshooting)
- [ğŸ“„ License](#-license)

---

## âœ¨ Features

<table>
<tr>
<td width="50%">

### ğŸ§  Advanced Architecture

- **Multi-scale temporal convolution** for capturing patterns at different time
  scales
- **Gated cross-scale fusion** for intelligent feature combination
- **Transformer blocks** with self-attention mechanism
- **Attention-weighted temporal pooling** for sequence aggregation

</td>
<td width="50%">

### ğŸ“ˆ Online Learning

- **Incremental training** - learn from streaming data
- **Adam optimizer** with warmup & cosine decay
- **ADWIN-lite drift detection** for concept drift
- **Outlier downweighting** for robust training

</td>
</tr>
<tr>
<td width="50%">

### ğŸ”’ Numerical Stability

- **Stable softmax** with max-subtraction
- **LayerNorm** with epsilon protection
- **Welford algorithm** for streaming statistics
- **Causal masking** preventing future leakage

</td>
<td width="50%">

### ğŸ“Š Predictions & Monitoring

- **Confidence intervals** with uncertainty estimation
- **Multi-step forecasting** with widening uncertainty
- **Real-time model metrics** and convergence tracking
- **Complete state serialization** for model persistence

</td>
</tr>
</table>

---

## ğŸ—ï¸ Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FUSION TEMPORAL TRANSFORMER (FTT)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   INPUT     â”‚    â”‚  MULTI-SCALE    â”‚    â”‚     TRANSFORMER STACK       â”‚â”‚
â”‚  â”‚  SEQUENCE   â”‚â”€â”€â”€â–¶â”‚  CONVOLUTION    â”‚â”€â”€â”€â–¶â”‚                              â”‚â”‚
â”‚  â”‚ [T Ã— D_in]  â”‚    â”‚  + FUSION       â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ Block 1: LNâ†’MHAâ†’LNâ†’FFN â”‚ â”‚â”‚
â”‚                                             â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚â”‚
â”‚                                             â”‚  â”‚ Block 2: LNâ†’MHAâ†’LNâ†’FFN â”‚ â”‚â”‚
â”‚                                             â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚â”‚
â”‚                                             â”‚  â”‚ Block N: LNâ†’MHAâ†’LNâ†’FFN â”‚ â”‚â”‚
â”‚                                             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚
â”‚                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                            â”‚                â”‚
â”‚                                                            â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   OUTPUT    â”‚    â”‚    OUTPUT       â”‚    â”‚   ATTENTION POOLING          â”‚â”‚
â”‚  â”‚ PREDICTION  â”‚â—€â”€â”€â”€â”‚    LAYER        â”‚â—€â”€â”€â”€â”‚   [T Ã— E] â†’ [E]              â”‚â”‚
â”‚  â”‚  [D_out]    â”‚    â”‚  [E â†’ D_out]    â”‚    â”‚                              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Multi-Scale Temporal Processing

```
INPUT SEQUENCE                    MULTI-SCALE CONVOLUTION
     â”‚                                    â”‚
     â”‚                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”
     â–¼                           â–¼        â–¼        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ xâ‚ xâ‚‚ xâ‚ƒ...â”‚            â”‚Scale=1  â”‚â”‚Scale=2  â”‚â”‚Scale=4  â”‚
â”‚ [T Ã— D_in] â”‚            â”‚(Fine)   â”‚â”‚(Medium) â”‚â”‚(Coarse) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚stride=1 â”‚â”‚stride=2 â”‚â”‚stride=4 â”‚
                           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                â”‚          â”‚          â”‚
                                â–¼          â–¼          â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚    GATED CROSS-SCALE FUSION  â”‚
                           â”‚                              â”‚
                           â”‚  g_s = Ïƒ(W_g Â· concat(E) + b)â”‚
                           â”‚  fused = Î£ g_s Â· E_s         â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â”‚
                                         â–¼
                                   [T Ã— Embed]
```

### Transformer Block Detail

```
          INPUT [T Ã— E]
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚              â”‚
â”‚         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”‚
â”‚         â”‚LayerNormâ”‚         â”‚
â”‚         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚
â”‚              â”‚              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚    â”‚  Multi-Head Self  â”‚    â”‚
â”‚    â”‚    Attention      â”‚    â”‚
â”‚    â”‚   (Causal Mask)   â”‚    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚              â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          ADD (Residual)
               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚              â”‚
â”‚         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”         â”‚
â”‚         â”‚LayerNormâ”‚         â”‚
â”‚         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â”‚
â”‚              â”‚              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚    â”‚   Feed-Forward    â”‚    â”‚
â”‚    â”‚  (GELU Activation)â”‚    â”‚
â”‚    â”‚  [E â†’ 4E â†’ E]     â”‚    â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚              â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          ADD (Residual)
               â”‚
               â–¼
         OUTPUT [T Ã— E]
```

### Data Flow Pipeline

```mermaid
graph LR
    A[Raw Input] --> B[Welford Normalization]
    B --> C[Multi-Scale Conv]
    C --> D[Positional Encoding]
    D --> E[Scale Embeddings]
    E --> F[Gated Fusion]
    F --> G[Transformer Blocks]
    G --> H[Attention Pooling]
    H --> I[Output Layer]
    I --> J[Denormalization]
    J --> K[Prediction]
    
    style A fill:#e1f5fe
    style K fill:#c8e6c9
    style F fill:#fff3e0
    style G fill:#fce4ec
```

---

## ğŸ“¦ Installation

### Using JSR (Recommended)

```typescript
import { FusionTemporalTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";
```

### Using Deno

```typescript
import { FusionTemporalTransformerRegression } from "https://jsr.io/@hviana/multivariate-ft-transformer-regression/mod.ts";
```

---

## ğŸš€ Quick Start

### Basic Example

```typescript
import { FusionTemporalTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

// Create model with default configuration
const model = new FusionTemporalTransformerRegression();

// Training data: sequence of [feature1, feature2] pairs
const xCoordinates = [
  [1.0, 2.0], // t=0
  [2.0, 3.0], // t=1
  [3.0, 4.0], // t=2
  [4.0, 5.0], // t=3
];

// Target: what we want to predict (uses last row)
const yCoordinates = [
  [0.1], // t=0
  [0.2], // t=1
  [0.25], // t=2
  [0.3], // t=3 â† This is the actual target
];

// Train incrementally
const fitResult = model.fitOnline({ xCoordinates, yCoordinates });

console.log(`ğŸ“‰ Loss: ${fitResult.loss.toFixed(6)}`);
console.log(`ğŸ“Š Gradient Norm: ${fitResult.gradientNorm.toFixed(6)}`);
console.log(`âœ… Converged: ${fitResult.converged}`);

// Make predictions for next 3 steps
const predictions = model.predict(3);

predictions.predictions.forEach((pred, i) => {
  console.log(`Step ${i + 1}:`, {
    predicted: pred.predicted[0].toFixed(4),
    confidence: `[${pred.lowerBound[0].toFixed(4)}, ${
      pred.upperBound[0].toFixed(4)
    }]`,
  });
});
```

### Output

```
ğŸ“‰ Loss: 0.125432
ğŸ“Š Gradient Norm: 0.034521
âœ… Converged: false
Step 1: { predicted: '0.3245', confidence: '[-0.1234, 0.7724]' }
Step 2: { predicted: '0.3512', confidence: '[-0.2156, 0.9180]' }
Step 3: { predicted: '0.3801', confidence: '[-0.3012, 1.0614]' }
```

---

## ğŸ“– API Reference

### Constructor

```typescript
const model = new FusionTemporalTransformerRegression(config?: Partial<FusionTemporalTransformerRegressionConfig>);
```

### Methods

| Method                    | Description                    | Returns              |
| ------------------------- | ------------------------------ | -------------------- |
| `fitOnline(data)`         | Train model on a single sample | `FitResult`          |
| `predict(steps)`          | Generate predictions           | `PredictionResult`   |
| `getModelSummary()`       | Get model information          | `ModelSummary`       |
| `getWeights()`            | Export all weights             | `WeightInfo`         |
| `getNormalizationStats()` | Get normalization statistics   | `NormalizationStats` |
| `reset()`                 | Reset model to initial state   | `void`               |
| `save()`                  | Serialize model to JSON string | `string`             |
| `load(json)`              | Load model from JSON string    | `void`               |

### Type Definitions

#### FitResult

```typescript
interface FitResult {
  loss: number; // Combined MSE + L2 loss
  gradientNorm: number; // L2 norm of all gradients
  effectiveLearningRate: number; // Current LR after scheduling
  isOutlier: boolean; // Whether sample was detected as outlier
  converged: boolean; // Whether gradient norm < threshold
  sampleIndex: number; // Total samples seen
  driftDetected: boolean; // Whether ADWIN detected drift
}
```

#### PredictionResult

```typescript
interface PredictionResult {
  predictions: SinglePrediction[]; // Array of predictions per step
  accuracy: number; // Model accuracy metric (0-1)
  sampleCount: number; // Total training samples
  isModelReady: boolean; // Whether model can predict
}

interface SinglePrediction {
  predicted: number[]; // Predicted values
  lowerBound: number[]; // 95% CI lower bound
  upperBound: number[]; // 95% CI upper bound
  standardError: number[]; // Standard error per output
}
```

#### ModelSummary

```typescript
interface ModelSummary {
  isInitialized: boolean;
  inputDimension: number;
  outputDimension: number;
  numBlocks: number;
  embeddingDim: number;
  numHeads: number;
  temporalScales: number[];
  totalParameters: number;
  sampleCount: number;
  accuracy: number;
  converged: boolean;
  effectiveLearningRate: number;
  driftCount: number;
}
```

---

## âš™ï¸ Configuration Guide

### Complete Configuration Reference

```typescript
interface FusionTemporalTransformerRegressionConfig {
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // ğŸ—ï¸ ARCHITECTURE PARAMETERS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  numBlocks: number; // Number of transformer blocks
  embeddingDim: number; // Embedding dimension (must be divisible by numHeads)
  numHeads: number; // Number of attention heads
  ffnMultiplier: number; // FFN hidden size = embeddingDim Ã— ffnMultiplier
  temporalScales: number[]; // Convolution stride scales
  temporalKernelSize: number; // Convolution kernel size
  maxSequenceLength: number; // Maximum input sequence length

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // ğŸ“‰ OPTIMIZER PARAMETERS
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  learningRate: number; // Base learning rate
  warmupSteps: number; // Steps for linear warmup
  totalSteps: number; // Total steps for cosine decay
  beta1: number; // Adam first moment decay
  beta2: number; // Adam second moment decay
  epsilon: number; // Numerical stability constant

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // ğŸ›ï¸ REGULARIZATION & STABILITY
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  regularizationStrength: number; // L2 regularization coefficient
  convergenceThreshold: number; // Gradient norm for convergence
  outlierThreshold: number; // Z-score threshold for outliers
  adwinDelta: number; // ADWIN confidence parameter

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // ğŸ’§ DROPOUT (Currently implemented as 0)
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  attentionDropout: number; // Dropout in attention (reserved)
  fusionDropout: number; // Dropout in fusion (reserved)
}
```

### Parameter Optimization Guide

#### ğŸ—ï¸ Architecture Parameters

<details>
<summary><b>numBlocks</b> (default: 3)</summary>

**What it controls:** The depth of the transformer stack.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  numBlocks = 1        â”‚  numBlocks = 3      â”‚  numBlocks = 6    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Block 1 â”‚          â”‚  â”‚ Block 1 â”‚        â”‚  â”‚ Block 1 â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚  â”‚ Block 2 â”‚        â”‚  â”‚ Block 2 â”‚      â”‚
â”‚                       â”‚  â”‚ Block 3 â”‚        â”‚  â”‚ Block 3 â”‚      â”‚
â”‚  Fast, simple         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚ Block 4 â”‚      â”‚
â”‚  patterns             â”‚  Balanced           â”‚  â”‚ Block 5 â”‚      â”‚
â”‚                       â”‚                     â”‚  â”‚ Block 6 â”‚      â”‚
â”‚                       â”‚                     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                       â”‚                     â”‚  Complex patterns â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Optimization Guide:**

| Scenario                       | Recommended | Reason                    |
| ------------------------------ | ----------- | ------------------------- |
| Simple linear trends           | 1-2         | Less overfitting          |
| Moderate complexity            | 3           | Good balance              |
| Complex multi-scale patterns   | 4-6         | More representation power |
| Limited data (<1000 samples)   | 1-2         | Prevent overfitting       |
| Abundant data (>10000 samples) | 4-6         | Utilize capacity          |

**Example:**

```typescript
// For stock price prediction with complex patterns
const model = new FusionTemporalTransformerRegression({
  numBlocks: 4,
});

// For simple sensor data
const simpleModel = new FusionTemporalTransformerRegression({
  numBlocks: 2,
});
```

</details>

<details>
<summary><b>embeddingDim</b> (default: 64)</summary>

**What it controls:** The internal representation size throughout the model.

**âš ï¸ Constraint:** Must be divisible by `numHeads`

```
embeddingDim = 32  â†’  Lightweight, fast
embeddingDim = 64  â†’  Balanced (default)
embeddingDim = 128 â†’  High capacity
embeddingDim = 256 â†’  Maximum expressiveness
```

**Memory & Compute Impact:**

| embeddingDim | Parameters* | Relative Speed |
| ------------ | ----------- | -------------- |
| 32           | ~50K        | 4Ã— faster      |
| 64           | ~200K       | 1Ã— (baseline)  |
| 128          | ~800K       | 4Ã— slower      |
| 256          | ~3.2M       | 16Ã— slower     |

*Approximate, varies with other settings

**Optimization Guide:**

```typescript
// Real-time applications - prioritize speed
const fastModel = new FusionTemporalTransformerRegression({
  embeddingDim: 32,
  numHeads: 4, // 32/4 = 8 dim per head
});

// High-dimensional input data
const richModel = new FusionTemporalTransformerRegression({
  embeddingDim: 128,
  numHeads: 8, // 128/8 = 16 dim per head
});
```

</details>

<details>
<summary><b>numHeads</b> (default: 8)</summary>

**What it controls:** Number of parallel attention mechanisms.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MULTI-HEAD ATTENTION                      â”‚
â”‚                                                              â”‚
â”‚  numHeads = 4                    numHeads = 8                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”  â”‚
â”‚  â”‚Head1â”‚Head2â”‚Head3â”‚Head4â”‚      â”‚H1â”‚H2â”‚H3â”‚H4â”‚H5â”‚H6â”‚H7â”‚H8â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  Fewer, broader attention        More, specialized attention â”‚
â”‚  patterns                        patterns                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Dimension per head:** `d_k = embeddingDim / numHeads`

| embeddingDim | numHeads | d_k | Recommendation |
| ------------ | -------- | --- | -------------- |
| 64           | 4        | 16  | âœ… Good        |
| 64           | 8        | 8   | âœ… Default     |
| 64           | 16       | 4   | âš ï¸ Too small   |
| 128          | 8        | 16  | âœ… Good        |
| 128          | 16       | 8   | âœ… Good        |

**Example:**

```typescript
// Recommended: d_k between 8-32
const model = new FusionTemporalTransformerRegression({
  embeddingDim: 64,
  numHeads: 8, // d_k = 8 âœ…
});
```

</details>

<details>
<summary><b>temporalScales</b> (default: [1, 2, 4])</summary>

**What it controls:** Multi-resolution temporal analysis.

```
Input Sequence: [xâ‚, xâ‚‚, xâ‚ƒ, xâ‚„, xâ‚…, xâ‚†, xâ‚‡, xâ‚ˆ]

Scale = 1 (stride=1): Captures every timestep
  â”‚ xâ‚ â”‚ xâ‚‚ â”‚ xâ‚ƒ â”‚ xâ‚„ â”‚ xâ‚… â”‚ xâ‚† â”‚ xâ‚‡ â”‚ xâ‚ˆ â”‚
  
Scale = 2 (stride=2): Captures pairs
  â”‚ xâ‚,xâ‚‚ â”‚ xâ‚ƒ,xâ‚„ â”‚ xâ‚…,xâ‚† â”‚ xâ‚‡,xâ‚ˆ â”‚
  
Scale = 4 (stride=4): Captures quadruplets
  â”‚ xâ‚,xâ‚‚,xâ‚ƒ,xâ‚„ â”‚ xâ‚…,xâ‚†,xâ‚‡,xâ‚ˆ â”‚

â†’ Gated Fusion combines all scales
```

**Use Case Examples:**

```typescript
// High-frequency trading (microsecond patterns)
const hftModel = new FusionTemporalTransformerRegression({
  temporalScales: [1, 2, 4, 8], // Fine-grained
  maxSequenceLength: 256,
});

// Daily weather forecasting
const weatherModel = new FusionTemporalTransformerRegression({
  temporalScales: [1, 7, 30], // Daily, weekly, monthly
  maxSequenceLength: 365,
});

// IoT sensor with varying patterns
const iotModel = new FusionTemporalTransformerRegression({
  temporalScales: [1, 3, 6, 12], // Multiple granularities
  maxSequenceLength: 128,
});
```

</details>

<details>
<summary><b>temporalKernelSize</b> (default: 3)</summary>

**What it controls:** The receptive field of temporal convolutions.

```
Kernel Size = 3:          Kernel Size = 5:          Kernel Size = 7:
  â”Œâ”€â”¬â”€â”¬â”€â”                   â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”              â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”
  â”‚â—€â”€â”€â”€â–¶â”‚                   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚              â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
  â””â”€â”´â”€â”´â”€â”˜                   â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜              â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜
  Local patterns            Medium patterns          Wide patterns
```

**Recommendations:**

| Data Type              | Kernel Size | Reason                |
| ---------------------- | ----------- | --------------------- |
| High-frequency signals | 3           | Preserve local detail |
| Medium-frequency       | 5           | Balanced              |
| Low-frequency trends   | 7-9         | Capture wider context |

</details>

<details>
<summary><b>maxSequenceLength</b> (default: 512)</summary>

**What it controls:** Maximum temporal window for processing.

**âš ï¸ Memory Impact:** Attention is O(TÂ²) in memory!

| maxSequenceLength | Memory (approx.) | Use Case               |
| ----------------- | ---------------- | ---------------------- |
| 64                | ~16 MB           | Real-time, low latency |
| 128               | ~64 MB           | Standard applications  |
| 256               | ~256 MB          | Historical analysis    |
| 512               | ~1 GB            | Long-term patterns     |

```typescript
// Constrained environment
const lightModel = new FusionTemporalTransformerRegression({
  maxSequenceLength: 64,
  embeddingDim: 32,
});

// Server with ample memory
const heavyModel = new FusionTemporalTransformerRegression({
  maxSequenceLength: 512,
  embeddingDim: 128,
});
```

</details>

---

#### ğŸ“‰ Optimizer Parameters

<details>
<summary><b>learningRate</b> (default: 0.001)</summary>

**What it controls:** Step size for parameter updates.

```
Learning Rate Schedule:
                                
Rate â”‚    â•±â”€â”€â”€â”€â”€â”€â•²
     â”‚   â•±        â•²
     â”‚  â•±          â•²
     â”‚ â•±            â•²
     â”‚â•±              â•²___
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
       Warmup   Cosine Decay
       Steps    Phase
```

**Optimization Guide:**

| Scenario                | Learning Rate | Reason                  |
| ----------------------- | ------------- | ----------------------- |
| Fast convergence needed | 0.01          | Quick but may overshoot |
| Standard training       | 0.001         | Balanced (default)      |
| Fine-tuning             | 0.0001        | Gentle updates          |
| Very noisy data         | 0.0005        | More stable             |

```typescript
// Aggressive learning for quick results
const fastLearner = new FusionTemporalTransformerRegression({
  learningRate: 0.005,
  warmupSteps: 50,
});

// Conservative learning for stability
const stableLearner = new FusionTemporalTransformerRegression({
  learningRate: 0.0005,
  warmupSteps: 200,
});
```

</details>

<details>
<summary><b>warmupSteps & totalSteps</b> (defaults: 100, 10000)</summary>

**What they control:** Learning rate scheduling.

```
                    Learning Rate Over Time
                    
    LR â”‚        warmupSteps=100    totalSteps=10000
       â”‚              â”‚                    â”‚
  0.001â”‚    â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²       â”‚
       â”‚   â•±                        â•²      â”‚
       â”‚  â•±                          â•²     â”‚
       â”‚ â•±                            â•²    â”‚
       â”‚â•±                              â•²___â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
         0    100              10000   Steps
         
Formula:
  - Warmup:   lr = base_lr Ã— (step / warmup_steps)
  - Decay:    lr = base_lr Ã— 0.5 Ã— (1 + cos(Ï€ Ã— progress))
```

**Configuration Examples:**

```typescript
// Short training session
const quickTrain = new FusionTemporalTransformerRegression({
  warmupSteps: 20,
  totalSteps: 500,
});

// Extended training
const longTrain = new FusionTemporalTransformerRegression({
  warmupSteps: 500,
  totalSteps: 50000,
});
```

</details>

<details>
<summary><b>beta1 & beta2</b> (defaults: 0.9, 0.999)</summary>

**What they control:** Adam optimizer momentum parameters.

```
Adam Update Rule:
  m = Î²â‚Â·m + (1-Î²â‚)Â·g          (First moment)
  v = Î²â‚‚Â·v + (1-Î²â‚‚)Â·gÂ²         (Second moment)
  
  Î²â‚ = 0.9:  90% old momentum, 10% new gradient
  Î²â‚‚ = 0.999: 99.9% old variance, 0.1% new
```

| Parameter | Effect of Higher Value                 | Effect of Lower Value                |
| --------- | -------------------------------------- | ------------------------------------ |
| beta1     | Smoother updates, slower adaptation    | More responsive, possibly noisy      |
| beta2     | More stable scaling, slower adaptation | Faster adaptation, possibly unstable |

```typescript
// For very noisy gradients
const stableAdam = new FusionTemporalTransformerRegression({
  beta1: 0.95, // More momentum
  beta2: 0.9999, // More stable scaling
});

// For quick adaptation
const adaptiveAdam = new FusionTemporalTransformerRegression({
  beta1: 0.85,
  beta2: 0.99,
});
```

</details>

---

#### ğŸ›ï¸ Regularization Parameters

<details>
<summary><b>regularizationStrength</b> (default: 1e-4)</summary>

**What it controls:** L2 weight decay penalty.

```
Total Loss = MSE Loss + (Î»/2) Ã— Î£||W||Â²

Î» = 0:      No regularization (may overfit)
Î» = 1e-4:   Light regularization (default)
Î» = 1e-3:   Moderate regularization
Î» = 1e-2:   Strong regularization (may underfit)
```

**Visual Effect:**

```
           Low Regularization              High Regularization
           
Weights â”‚  â–“â–“â–“â–‘â–‘â–‘â–“â–“â–‘â–‘â–‘â–“â–“â–“â–‘â–‘           â”‚  â–“â–‘â–‘â–‘â–‘â–“â–‘â–‘â–‘â–‘â–‘â–“â–‘â–‘â–‘â–‘
Distribution â”‚  Many large weights        â”‚  Mostly small weights
           â”‚  Complex model              â”‚  Simpler model
```

```typescript
// Complex patterns, lots of data
const complexModel = new FusionTemporalTransformerRegression({
  regularizationStrength: 1e-5, // Less regularization
});

// Limited data, prevent overfitting
const simpleModel = new FusionTemporalTransformerRegression({
  regularizationStrength: 1e-3, // More regularization
});
```

</details>

<details>
<summary><b>outlierThreshold</b> (default: 3.0)</summary>

**What it controls:** Z-score threshold for outlier detection.

```
                Normal Distribution
                
         â”‚      â—â—â—â—â—â—â—â—â—â—â—
         â”‚   â—â—â—            â—â—â—
         â”‚ â—â—                  â—â—
Density  â”‚â—                      â—
         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”
         â”‚      -3Ïƒâ”‚ 99.7%â”‚+3Ïƒ
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â–¶
                   â–¼      â–¼
              outlierThreshold = 3.0
              
Points beyond Â±3Ïƒ are flagged as outliers
and receive reduced weight (0.1Ã—) during training
```

| Threshold | Coverage | Outlier Sensitivity |
| --------- | -------- | ------------------- |
| 2.0       | 95.4%    | Very sensitive      |
| 2.5       | 98.8%    | Sensitive           |
| 3.0       | 99.7%    | Balanced (default)  |
| 3.5       | 99.95%   | Conservative        |
| 4.0       | 99.99%   | Very conservative   |

```typescript
// Sensor data with occasional spikes
const robustModel = new FusionTemporalTransformerRegression({
  outlierThreshold: 2.5, // More aggressive outlier detection
});

// Clean data, trust all samples
const trustingModel = new FusionTemporalTransformerRegression({
  outlierThreshold: 4.0, // Very conservative
});
```

</details>

<details>
<summary><b>adwinDelta</b> (default: 0.002)</summary>

**What it controls:** Sensitivity of concept drift detection.

```
ADWIN Drift Detection
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

     Error Rate
         â”‚
       â•±â”€â”‚â”€â•²    Drift Detected!
      â•±  â”‚  â•²        â”‚
  â”€â”€â”€â•±   â”‚   â•²â”€â”€â—â”€â”€â”€â”€â”¼â”€â”€â”€â”€â–¶
     â”‚   â”‚   â”‚  â†‘    â”‚
     â”‚   â”‚   â”‚  Statistical
     â”‚   â”‚   â”‚  significance
     â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€test (Î´)â”€â”€â”€â”€
     
Î´ = 0.002: Sensitive (detects subtle drift)
Î´ = 0.01:  Moderate
Î´ = 0.05:  Conservative (only major shifts)
```

```typescript
// Streaming data with frequent distribution shifts
const adaptiveModel = new FusionTemporalTransformerRegression({
  adwinDelta: 0.01, // Quick drift detection
});

// Stable environment
const stableModel = new FusionTemporalTransformerRegression({
  adwinDelta: 0.001, // Very sensitive
});
```

</details>

---

### ğŸ“Š Configuration Presets

#### ğŸš€ High-Performance Preset

```typescript
const highPerformanceConfig = {
  numBlocks: 4,
  embeddingDim: 128,
  numHeads: 8,
  ffnMultiplier: 4,
  temporalScales: [1, 2, 4, 8],
  temporalKernelSize: 5,
  maxSequenceLength: 256,
  learningRate: 0.001,
  warmupSteps: 200,
  totalSteps: 20000,
  regularizationStrength: 1e-4,
};
```

#### âš¡ Real-Time Preset

```typescript
const realTimeConfig = {
  numBlocks: 2,
  embeddingDim: 32,
  numHeads: 4,
  ffnMultiplier: 2,
  temporalScales: [1, 2],
  temporalKernelSize: 3,
  maxSequenceLength: 64,
  learningRate: 0.002,
  warmupSteps: 50,
  totalSteps: 5000,
};
```

#### ğŸ”¬ Research Preset

```typescript
const researchConfig = {
  numBlocks: 6,
  embeddingDim: 256,
  numHeads: 16,
  ffnMultiplier: 4,
  temporalScales: [1, 2, 4, 8, 16],
  temporalKernelSize: 7,
  maxSequenceLength: 512,
  learningRate: 0.0005,
  warmupSteps: 500,
  totalSteps: 100000,
  regularizationStrength: 1e-5,
};
```

---

## ğŸ’¡ Examples

### ğŸ“ˆ Stock Price Prediction

```typescript
import { FusionTemporalTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

// Model optimized for financial data
const stockModel = new FusionTemporalTransformerRegression({
  numBlocks: 3,
  embeddingDim: 64,
  numHeads: 8,
  temporalScales: [1, 5, 20], // Daily, weekly, monthly patterns
  maxSequenceLength: 252, // One trading year
  outlierThreshold: 2.5, // Financial data has outliers
  adwinDelta: 0.005, // Detect market regime changes
});

// Historical data: [open, high, low, close, volume]
const historicalData = [
  [150.0, 152.5, 149.0, 151.5, 1000000],
  [151.5, 153.0, 150.5, 152.0, 1100000],
  // ... more historical data
];

// Target: next day's closing price
const targets = [
  [151.5],
  [152.0],
  [153.5], // ...
];

// Train on streaming data
for (let i = 20; i < historicalData.length; i++) {
  const window = historicalData.slice(i - 20, i);
  const target = [targets[i]];

  const result = model.fitOnline({
    xCoordinates: window,
    yCoordinates: target,
  });

  if (result.driftDetected) {
    console.log(`âš ï¸ Market regime change detected at index ${i}`);
  }
}

// Predict next 5 trading days
const forecast = model.predict(5);
forecast.predictions.forEach((pred, day) => {
  console.log(
    `Day ${day + 1}: $${pred.predicted[0].toFixed(2)} Â± $${
      pred.standardError[0].toFixed(2)
    }`,
  );
});
```

### ğŸŒ¡ï¸ Multi-Sensor IoT Monitoring

```typescript
import { FusionTemporalTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

// Model for IoT sensor fusion
const iotModel = new FusionTemporalTransformerRegression({
  numBlocks: 2, // Lightweight for edge deployment
  embeddingDim: 32,
  numHeads: 4,
  temporalScales: [1, 4, 16], // Different sensor update rates
  maxSequenceLength: 64,
  learningRate: 0.002,
  outlierThreshold: 3.0, // Handle sensor glitches
});

// Sensor readings: [temperature, humidity, pressure, light]
const sensorBuffer: number[][] = [];

// Simulated real-time data ingestion
async function processSensorData(reading: number[]) {
  sensorBuffer.push(reading);

  // Keep sliding window
  if (sensorBuffer.length > 64) {
    sensorBuffer.shift();
  }

  // Need minimum data to train
  if (sensorBuffer.length < 10) return;

  // Target: predict temperature
  const target = [[reading[0]]];

  const result = iotModel.fitOnline({
    xCoordinates: sensorBuffer,
    yCoordinates: target,
  });

  // Alert on anomalies
  if (result.isOutlier) {
    console.log(`ğŸš¨ Anomaly detected! Sensor reading: ${reading}`);
  }

  // Predict next reading
  const prediction = iotModel.predict(1);
  if (prediction.isModelReady) {
    console.log(
      `ğŸ“Š Predicted temperature: ${
        prediction.predictions[0].predicted[0].toFixed(1)
      }Â°C`,
    );
  }
}
```

### ğŸ“‰ Time Series Forecasting with Uncertainty

```typescript
import { FusionTemporalTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

const forecastModel = new FusionTemporalTransformerRegression({
  numBlocks: 3,
  embeddingDim: 64,
  numHeads: 8,
  temporalScales: [1, 7, 28], // Daily, weekly, monthly
  maxSequenceLength: 180, // 6 months of daily data
});

// Training loop
const trainingData = generateSyntheticData(1000);

for (const sample of trainingData) {
  const result = forecastModel.fitOnline(sample);

  if (result.sampleIndex % 100 === 0) {
    console.log(`
ğŸ“Š Training Progress
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Sample: ${result.sampleIndex}
Loss: ${result.loss.toFixed(6)}
Learning Rate: ${result.effectiveLearningRate.toFixed(6)}
Gradient Norm: ${result.gradientNorm.toFixed(6)}
Converged: ${result.converged}
    `);
  }
}

// Generate forecast with confidence intervals
const horizon = 14; // 2 weeks
const forecast = forecastModel.predict(horizon);

console.log("\nğŸ”® 14-Day Forecast with 95% Confidence Intervals\n");
console.log("Day  â”‚ Prediction â”‚    95% CI     â”‚ Std Error");
console.log("â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");

forecast.predictions.forEach((pred, i) => {
  const day = (i + 1).toString().padStart(2);
  const prediction = pred.predicted[0].toFixed(2).padStart(8);
  const lower = pred.lowerBound[0].toFixed(2);
  const upper = pred.upperBound[0].toFixed(2);
  const ci = `[${lower}, ${upper}]`.padStart(13);
  const se = pred.standardError[0].toFixed(3).padStart(7);

  console.log(`  ${day} â”‚  ${prediction} â”‚ ${ci} â”‚  ${se}`);
});
```

### ğŸ’¾ Model Persistence

```typescript
import { FusionTemporalTransformerRegression } from "jsr:@hviana/multivariate-ft-transformer-regression";

// Train model
const model = new FusionTemporalTransformerRegression();

// ... training code ...

// Save model
const modelJson = model.save();
await Deno.writeTextFile("model_checkpoint.json", modelJson);

console.log("âœ… Model saved successfully");

// Later: Load model
const loadedJson = await Deno.readTextFile("model_checkpoint.json");
const restoredModel = new FusionTemporalTransformerRegression();
restoredModel.load(loadedJson);

console.log("âœ… Model restored successfully");

// Verify restoration
const summary = restoredModel.getModelSummary();
console.log(`ğŸ“Š Restored model has ${summary.totalParameters} parameters`);
console.log(`ğŸ“ˆ Training samples: ${summary.sampleCount}`);
```

---

## ğŸ¯ Best Practices

### 1ï¸âƒ£ Data Preparation

```typescript
// âœ… DO: Provide sufficient sequence length
const goodData = {
  xCoordinates: generateSequence(50), // At least 10+ timesteps
  yCoordinates: generateTargets(50),
};

// âŒ DON'T: Use very short sequences
const badData = {
  xCoordinates: [[1, 2], [3, 4]], // Only 2 timesteps
  yCoordinates: [[5]],
};
```

### 2ï¸âƒ£ Incremental Training

```typescript
// âœ… DO: Train on streaming data incrementally
for (const sample of dataStream) {
  model.fitOnline(sample);
}

// âœ… DO: Monitor training metrics
const result = model.fitOnline(sample);
if (result.driftDetected) {
  logDriftEvent(result);
}
```

### 3ï¸âƒ£ Memory Management

```typescript
// âœ… DO: Use appropriate maxSequenceLength
const efficientModel = new FusionTemporalTransformerRegression({
  maxSequenceLength: Math.min(yourDataLength, 256),
});

// âŒ DON'T: Set unnecessarily large maxSequenceLength
const wastefulModel = new FusionTemporalTransformerRegression({
  maxSequenceLength: 10000, // Excessive for most use cases
});
```

### 4ï¸âƒ£ Hyperparameter Selection

```typescript
// Start with defaults, then tune based on metrics
let bestConfig = { ...defaultConfig };
let bestLoss = Infinity;

for (const config of configCandidates) {
  const model = new FusionTemporalTransformerRegression(config);
  const avgLoss = trainAndEvaluate(model, validationData);

  if (avgLoss < bestLoss) {
    bestLoss = avgLoss;
    bestConfig = config;
  }
}
```

---

## ğŸ”§ Troubleshooting

### Common Issues

<details>
<summary><b>âŒ Error: embeddingDim must be divisible by numHeads</b></summary>

**Problem:** Invalid configuration where `embeddingDim % numHeads !== 0`

**Solution:**

```typescript
// âŒ Wrong
const model = new FusionTemporalTransformerRegression({
  embeddingDim: 50,
  numHeads: 8, // 50 % 8 = 2 â‰  0
});

// âœ… Correct
const model = new FusionTemporalTransformerRegression({
  embeddingDim: 64, // 64 % 8 = 0 âœ“
  numHeads: 8,
});
```

</details>

<details>
<summary><b>âš ï¸ Model not converging</b></summary>

**Symptoms:** Loss remains high, gradientNorm doesn't decrease

**Solutions:**

1. **Adjust learning rate:**

```typescript
const model = new FusionTemporalTransformerRegression({
  learningRate: 0.0005, // Try lower
  warmupSteps: 200, // Longer warmup
});
```

2. **Add more regularization:**

```typescript
const model = new FusionTemporalTransformerRegression({
  regularizationStrength: 1e-3,
});
```

3. **Check data normalization** - the model normalizes internally, but extreme
   values may cause issues

</details>

<details>
<summary><b>âš ï¸ Predictions have high uncertainty</b></summary>

**Symptoms:** Wide confidence intervals

**Solutions:**

1. **Train longer:**

```typescript
// Check sample count
const summary = model.getModelSummary();
if (summary.sampleCount < 1000) {
  console.log("Continue training for better predictions");
}
```

2. **Reduce model complexity:**

```typescript
const simplerModel = new FusionTemporalTransformerRegression({
  numBlocks: 2,
  embeddingDim: 32,
});
```

</details>

<details>
<summary><b>âš ï¸ Frequent drift detection</b></summary>

**Symptoms:** `driftDetected: true` on many samples

**Solutions:**

1. **Adjust ADWIN sensitivity:**

```typescript
const model = new FusionTemporalTransformerRegression({
  adwinDelta: 0.05, // Less sensitive (default: 0.002)
});
```

2. **Check if data genuinely has distribution shifts** - this may be expected
   behavior

</details>

---

## ğŸ“„ License

MIT License Â© 2025 Henrique Emanoel Viana

---

<div align="center">

**[â¬† Back to Top](#-fusion-temporal-transformer-regression)**

Made with â¤ï¸ by [Henrique Emanoel Viana](https://github.com/hviana)

</div>
